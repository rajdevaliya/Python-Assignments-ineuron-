{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d29ff4",
   "metadata": {},
   "source": [
    "# 1. In what modes should the PdfFileReader() and PdfFileWriter() File objects will be opened?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150bc823",
   "metadata": {},
   "source": [
    "Ans: For PdfFileReader() file objects should be opened in rb -> read binary mode, Whereas for PdfFileWriter() file objects should be opened in wb -> write binary mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a67b22",
   "metadata": {},
   "source": [
    "# 2. From a PdfFileReader object, how do you get a Page object for page 5?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc969c",
   "metadata": {},
   "source": [
    "Ans: PdfFileReader class provides a method called getPage(page_no) to get a page object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a84a2c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame the Problem                                                                                                    39\n",
      "Select a Performance Measure                                                                                  42\n",
      "Check the Assumptions                                                                                             45\n",
      "Get the Data                                                                                                                    45\n",
      "Create the Workspace                                                                                                45\n",
      "Download the Data                                                                                                    49\n",
      "Take a Quick Look at the Data Structure                                                                50\n",
      "Create a Test Set                                                                                                          54\n",
      "Discover and Visualize the Data to Gain Insights                                                     58\n",
      "Visualizing Geographical Data                                                                                 59\n",
      "Looking for Correlations                                                                                           62\n",
      "Experimenting with Attribute Combinations                                                        65\n",
      "Prepare the Data for Machine Learning Algorithms                                                66\n",
      "Data Cleaning                                                                                                             67\n",
      "Handling Text and Categorical Attributes                                                              69\n",
      "Custom Transformers                                                                                                71\n",
      "Feature Scaling                                                                                                            72\n",
      "Transformation Pipelines                                                                                          73\n",
      "Select and Train a Model                                                                                               75\n",
      "Training and Evaluating on the Training Set                                                         75\n",
      "Better Evaluation Using Cross-Validation                                                              76\n",
      "Fine-Tune Y our Model                                                                                                  79\n",
      "Grid Search                                                                                                                 79\n",
      "Randomized Search                                                                                                   81\n",
      "Ensemble Methods                                                                                                     82\n",
      "Analyze the Best Models and Their Errors                                                             82\n",
      "Evaluate Y our System on the Test Set                                                                      83\n",
      "Launch, Monitor, and Maintain Y our System                                                            84\n",
      "Try It Out!                                                                                                                       85\n",
      "Exercises                                                                                                                          85\n",
      "3.Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  87\n",
      "MNIST                                                                                                                             87\n",
      "Training a Binary Classifier                                                                                          90\n",
      "Performance Measures                                                                                                  90\n",
      "Measuring Accuracy Using Cross-Validation                                                        91\n",
      "Confusion Matrix                                                                                                       92\n",
      "Precision and Recall                                                                                                   94\n",
      "Precision/Recall Tradeoff                                                                                          95\n",
      "The ROC Curve                                                                                                          99\n",
      "Multiclass Classification                                                                                             102\n",
      "Error Analysis                                                                                                              104\n",
      "iv | Table of Contents\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "pdf_reader = PdfReader(\"C:\\\\Users\\\\rajde\\\\ineuron_ai\\\\2-Aurélien-Géron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-O’Reilly-Media-2019.pdf\")\n",
    "print(pdf_reader.pages[5].extract_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d74b3",
   "metadata": {},
   "source": [
    "# 3. What PdfFileReader variable stores the number of pages in the PDF document?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944e90f4",
   "metadata": {},
   "source": [
    "Ans: getNumPages() method of PdfFileReader class stores the no pages in a PDF document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe7e5ca",
   "metadata": {},
   "source": [
    "# 4. If a PdfFileReader object’s PDF is encrypted with the password swordfish, what must you do before you can obtain Page objects from it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4016e30f",
   "metadata": {},
   "source": [
    "Ans: If a PdfFileReader object’s PDF is encrypted with the password swordfish and you're not aware of it. first read the Pdf using the PdfFileReader Class. PdfFileReader class provides a attribute called isEncrypted to check whether a pdf is encrypted or not. the method returns true if a pdf is encrypted and vice versa.\n",
    "if pdf is encrypted use the decrypt() method provided by PdfFileReader class first then try to read the contents/pages of the pdf, else PyPDF2 will raise the following error PyPDF2.utils.PdfReadError: file has not been decrypted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "991297c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Aurélien GéronHands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and\n",
      "TensorFlow\n",
      "Concepts, Tools, and Techniques to\n",
      "Build Intelligent SystemsSECOND EDITION\n",
      "Boston Farnham Sebastopol Tokyo Beijing Boston Farnham Sebastopol Tokyo Beijing\n",
      "978-1-492-03264-9\n",
      "[LSI]Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
      "by Aurélien Géron\n",
      "Copyright © 2019 Aurélien Géron. All rights reserved.\n",
      "Printed in the United States of America.\n",
      "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
      "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
      "also available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\n",
      "sales department: 800-998-9938 or corporate@oreilly.com .\n",
      "Editor:  Nicole Tache\n",
      "Interior Designer:  David FutatoCover Designer:  Karen Montgomery\n",
      "Illustrator:  Rebecca Demarest\n",
      "June 2019:  Second Edition\n",
      "Revision History for the Early Release\n",
      "2018-11-05: First Release\n",
      "2019-01-24: Second Release\n",
      "2019-03-07: Third Release\n",
      "2019-03-29: Fourth Release\n",
      "2019-04-22: Fifth Release\n",
      "See http://oreilly.com/catalog/errata.csp?isbn=9781492032649  for release details.\n",
      "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-on Machine Learning with\n",
      "Scikit-Learn, Keras, and TensorFlow , the cover image, and related trade dress are trademarks of O’Reilly\n",
      "Media, Inc.\n",
      "While the publisher and the author have used good faith efforts to ensure that the information and\n",
      "instructions contained in this work are accurate, the publisher and the author disclaim all responsibility\n",
      "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
      "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
      "risk. If any code samples or other technology this work contains or describes is subject to open source\n",
      "licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\n",
      "thereof complies with such licenses and/or rights.\n",
      "Table of Contents\n",
      "Preface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\n",
      "Part I. The Fundamentals of Machine Learning\n",
      "1.The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\n",
      "What Is Machine Learning?                                                                                           4\n",
      "Why Use Machine Learning?                                                                                         4\n",
      "Types of Machine Learning Systems                                                                             8\n",
      "Supervised/Unsupervised Learning                                                                           8\n",
      "Batch and Online Learning                                                                                       15\n",
      "Instance-Based Versus Model-Based Learning                                                      18\n",
      "Main Challenges of Machine Learning                                                                       24\n",
      "Insufficient Quantity of Training Data                                                                   24\n",
      "Nonrepresentative Training Data                                                                            26\n",
      "Poor-Quality Data                                                                                                      27\n",
      "Irrelevant Features                                                                                                     27\n",
      "Overfitting the Training Data                                                                                   28\n",
      "Underfitting the Training Data                                                                                30\n",
      "Stepping Back                                                                                                             30\n",
      "Testing and Validating                                                                                                   31\n",
      "Hyperparameter Tuning and Model Selection                                                       32\n",
      "Data Mismatch                                                                                                           33\n",
      "Exercises                                                                                                                          34\n",
      "2.End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\n",
      "Working with Real Data                                                                                                38\n",
      "Look at the Big Picture                                                                                                  39\n",
      "iii\n",
      "Frame the Problem                                                                                                    39\n",
      "Select a Performance Measure                                                                                  42\n",
      "Check the Assumptions                                                                                             45\n",
      "Get the Data                                                                                                                    45\n",
      "Create the Workspace                                                                                                45\n",
      "Download the Data                                                                                                    49\n",
      "Take a Quick Look at the Data Structure                                                                50\n",
      "Create a Test Set                                                                                                          54\n",
      "Discover and Visualize the Data to Gain Insights                                                     58\n",
      "Visualizing Geographical Data                                                                                 59\n",
      "Looking for Correlations                                                                                           62\n",
      "Experimenting with Attribute Combinations                                                        65\n",
      "Prepare the Data for Machine Learning Algorithms                                                66\n",
      "Data Cleaning                                                                                                             67\n",
      "Handling Text and Categorical Attributes                                                              69\n",
      "Custom Transformers                                                                                                71\n",
      "Feature Scaling                                                                                                            72\n",
      "Transformation Pipelines                                                                                          73\n",
      "Select and Train a Model                                                                                               75\n",
      "Training and Evaluating on the Training Set                                                         75\n",
      "Better Evaluation Using Cross-Validation                                                              76\n",
      "Fine-Tune Y our Model                                                                                                  79\n",
      "Grid Search                                                                                                                 79\n",
      "Randomized Search                                                                                                   81\n",
      "Ensemble Methods                                                                                                     82\n",
      "Analyze the Best Models and Their Errors                                                             82\n",
      "Evaluate Y our System on the Test Set                                                                      83\n",
      "Launch, Monitor, and Maintain Y our System                                                            84\n",
      "Try It Out!                                                                                                                       85\n",
      "Exercises                                                                                                                          85\n",
      "3.Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  87\n",
      "MNIST                                                                                                                             87\n",
      "Training a Binary Classifier                                                                                          90\n",
      "Performance Measures                                                                                                  90\n",
      "Measuring Accuracy Using Cross-Validation                                                        91\n",
      "Confusion Matrix                                                                                                       92\n",
      "Precision and Recall                                                                                                   94\n",
      "Precision/Recall Tradeoff                                                                                          95\n",
      "The ROC Curve                                                                                                          99\n",
      "Multiclass Classification                                                                                             102\n",
      "Error Analysis                                                                                                              104\n",
      "iv | Table of Contents\n",
      "Multilabel Classification                                                                                             108\n",
      "Multioutput Classification                                                                                          109\n",
      "Exercises                                                                                                                        110\n",
      "4.Training Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  113\n",
      "Linear Regression                                                                                                        114\n",
      "The Normal Equation                                                                                              116\n",
      "Computational Complexity                                                                                    119\n",
      "Gradient Descent                                                                                                         119\n",
      "Batch Gradient Descent                                                                                           123\n",
      "Stochastic Gradient Descent                                                                                   126\n",
      "Mini-batch Gradient Descent                                                                                 129\n",
      "Polynomial Regression                                                                                                130\n",
      "Learning Curves                                                                                                           132\n",
      "Regularized Linear Models                                                                                         136\n",
      "Ridge Regression                                                                                                      137\n",
      "Lasso Regression                                                                                                      139\n",
      "Elastic Net                                                                                                                 142\n",
      "Early Stopping                                                                                                          142\n",
      "Logistic Regression                                                                                                      144\n",
      "Estimating Probabilities                                                                                          144\n",
      "Training and Cost Function                                                                                   145\n",
      "Decision Boundaries                                                                                                146\n",
      "Softmax Regression                                                                                                  149\n",
      "Exercises                                                                                                                        153\n",
      "5.Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155\n",
      "Linear SVM Classification                                                                                          155\n",
      "Soft Margin Classification                                                                                       156\n",
      "Nonlinear SVM Classification                                                                                   159\n",
      "Polynomial Kernel                                                                                                   160\n",
      "Adding Similarity Features                                                                                     161\n",
      "Gaussian RBF Kernel                                                                                               162\n",
      "Computational Complexity                                                                                    163\n",
      "SVM Regression                                                                                                           164\n",
      "Under the Hood                                                                                                           166\n",
      "Decision Function and Predictions                                                                       166\n",
      "Training Objective                                                                                                   167\n",
      "Quadratic Programming                                                                                         169\n",
      "The Dual Problem                                                                                                    170\n",
      "Kernelized SVM                                                                                                       171\n",
      "Online SVMs                                                                                                            174\n",
      "Table of Contents | v\n",
      "Exercises                                                                                                                        175\n",
      "6.Decision Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  177\n",
      "Training and Visualizing a Decision Tree                                                                177\n",
      "Making Predictions                                                                                                     179\n",
      "Estimating Class Probabilities                                                                                   181\n",
      "The CART Training Algorithm                                                                                 182\n",
      "Computational Complexity                                                                                        183\n",
      "Gini Impurity or Entropy?                                                                                         183\n",
      "Regularization Hyperparameters                                                                              184\n",
      "Regression                                                                                                                     185\n",
      "Instability                                                                                                                      188\n",
      "Exercises                                                                                                                        189\n",
      "7.Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  191\n",
      "Voting Classifiers                                                                                                         192\n",
      "Bagging and Pasting                                                                                                    195\n",
      "Bagging and Pasting in Scikit-Learn                                                                     196\n",
      "Out-of-Bag Evaluation                                                                                            197\n",
      "Random Patches and Random Subspaces                                                                198\n",
      "Random Forests                                                                                                           199\n",
      "Extra-Trees                                                                                                                200\n",
      "Feature Importance                                                                                                  200\n",
      "Boosting                                                                                                                        201\n",
      "AdaBoost                                                                                                                   202\n",
      "Gradient Boosting                                                                                                    205\n",
      "Stacking                                                                                                                         210\n",
      "Exercises                                                                                                                        213\n",
      "8.Dimensionality Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  215\n",
      "The Curse of Dimensionality                                                                                     216\n",
      "Main Approaches for Dimensionality Reduction                                                   218\n",
      "Projection                                                                                                                  218\n",
      "Manifold Learning                                                                                                   220\n",
      "PCA                                                                                                                                222\n",
      "Preserving the Variance                                                                                          222\n",
      "Principal Components                                                                                            223\n",
      "Projecting Down to d Dimensions                                                                        224\n",
      "Using Scikit-Learn                                                                                                    224\n",
      "Explained Variance Ratio                                                                                        225\n",
      "Choosing the Right Number of Dimensions                                                       225\n",
      "PCA for Compression                                                                                             226\n",
      "vi | Table of Contents\n",
      "Randomized PCA                                                                                                    227\n",
      "Incremental PCA                                                                                                     227\n",
      "Kernel PCA                                                                                                                   228\n",
      "Selecting a Kernel and Tuning Hyperparameters                                                229\n",
      "LLE                                                                                                                                 232\n",
      "Other Dimensionality Reduction Techniques                                                         234\n",
      "Exercises                                                                                                                        235\n",
      "9.Unsupervised Learning Techniques. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  237\n",
      "Clustering                                                                                                                      238\n",
      "K-Means                                                                                                                    240\n",
      "Limits of K-Means                                                                                                   250\n",
      "Using clustering for image segmentation                                                             251\n",
      "Using Clustering for Preprocessing                                                                       252\n",
      "Using Clustering for Semi-Supervised Learning                                                 254\n",
      "DBSCAN                                                                                                                   256\n",
      "Other Clustering Algorithms                                                                                 259\n",
      "Gaussian Mixtures                                                                                                       260\n",
      "Anomaly Detection using Gaussian Mixtures                                                     266\n",
      "Selecting the Number of Clusters                                                                          267\n",
      "Bayesian Gaussian Mixture Models                                                                      270\n",
      "Other Anomaly Detection and Novelty Detection Algorithms                        274\n",
      "Part II. Neural Networks and Deep Learning\n",
      "10. Introduction to Artificial  Neural Networks with Keras. . . . . . . . . . . . . . . . . . . . . . . . . .  277\n",
      "From Biological to Artificial Neurons                                                                      278\n",
      "Biological Neurons                                                                                                   279\n",
      "Logical Computations with Neurons                                                                    281\n",
      "The Perceptron                                                                                                         281\n",
      "Multi-Layer Perceptron and Backpropagation                                                    286\n",
      "Regression MLPs                                                                                                      289\n",
      "Classification MLPs                                                                                                 290\n",
      "Implementing MLPs with Keras                                                                                292\n",
      "Installing TensorFlow 2                                                                                           293\n",
      "Building an Image Classifier Using the Sequential API                                     294\n",
      "Building a Regression MLP Using the Sequential API                                       303\n",
      "Building Complex Models Using the Functional API                                        304\n",
      "Building Dynamic Models Using the Subclassing API                                       309\n",
      "Saving and Restoring a Model                                                                                311\n",
      "Using Callbacks                                                                                                        311\n",
      "Table of Contents | vii\n",
      "Visualization Using TensorBoard                                                                          313\n",
      "Fine-Tuning Neural Network Hyperparameters                                                     315\n",
      "Number of Hidden Layers                                                                                      319\n",
      "Number of Neurons per Hidden Layer                                                                 320\n",
      "Learning Rate, Batch Size and Other Hyperparameters                                     320\n",
      "Exercises                                                                                                                        322\n",
      "11. Training Deep Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  325\n",
      "Vanishing/Exploding Gradients Problems                                                              326\n",
      "Glorot and He Initialization                                                                                   327\n",
      "Nonsaturating Activation Functions                                                                     329\n",
      "Batch Normalization                                                                                                333\n",
      "Gradient Clipping                                                                                                    338\n",
      "Reusing Pretrained Layers                                                                                          339\n",
      "Transfer Learning With Keras                                                                                341\n",
      "Unsupervised Pretraining                                                                                       343\n",
      "Pretraining on an Auxiliary Task                                                                           344\n",
      "Faster Optimizers                                                                                                         344\n",
      "Momentum Optimization                                                                                      345\n",
      "Nesterov Accelerated Gradient                                                                              346\n",
      "AdaGrad                                                                                                                    347\n",
      "RMSProp                                                                                                                   349\n",
      "Adam and Nadam Optimization                                                                           349\n",
      "Learning Rate Scheduling                                                                                       352\n",
      "Avoiding Overfitting Through Regularization                                                        356\n",
      "ℓ1 and ℓ2 Regularization                                                                                           356\n",
      "Dropout                                                                                                                     357\n",
      "Monte-Carlo (MC) Dropout                                                                                  360\n",
      "Max-Norm Regularization                                                                                      362\n",
      "Summary and Practical Guidelines                                                                           363\n",
      "Exercises                                                                                                                        364\n",
      "12. Custom Models and Training with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  367\n",
      "A Quick Tour of TensorFlow                                                                                     368\n",
      "Using TensorFlow like NumPy                                                                                  371\n",
      "Tensors and Operations                                                                                          371\n",
      "Tensors and NumPy                                                                                                373\n",
      "Type Conversions                                                                                                     374\n",
      "Variables                                                                                                                    374\n",
      "Other Data Structures                                                                                             375\n",
      "Customizing Models and Training Algorithms                                                      376\n",
      "Custom Loss Functions                                                                                           376\n",
      "viii | Table of Contents\n",
      "Saving and Loading Models That Contain Custom Components                    377\n",
      "Custom Activation Functions, Initializers, Regularizers, and Constraints     379\n",
      "Custom Metrics                                                                                                        380\n",
      "Custom Layers                                                                                                          383\n",
      "Custom Models                                                                                                        386\n",
      "Losses and Metrics Based on Model Internals                                                     388\n",
      "Computing Gradients Using Autodiff                                                                   389\n",
      "Custom Training Loops                                                                                          393\n",
      "TensorFlow Functions and Graphs                                                                           396\n",
      "Autograph and Tracing                                                                                           398\n",
      "TF Function Rules                                                                                                    400\n",
      "13. Loading and Preprocessing Data with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  403\n",
      "The Data API                                                                                                                404\n",
      "Chaining Transformations                                                                                      405\n",
      "Shuffling the Data                                                                                                    406\n",
      "Preprocessing the Data                                                                                            409\n",
      "Putting Everything Together                                                                                  410\n",
      "Prefetching                                                                                                                411\n",
      "Using the Dataset With tf.keras                                                                              413\n",
      "The TFRecord Format                                                                                                414\n",
      "Compressed TFRecord Files                                                                                   415\n",
      "A Brief Introduction to Protocol Buffers                                                              415\n",
      "TensorFlow Protobufs                                                                                             416\n",
      "Loading and Parsing Examples                                                                              418\n",
      "Handling Lists of Lists Using the SequenceExample  Protobuf                          419\n",
      "The Features API                                                                                                         420\n",
      "Categorical Features                                                                                                 421\n",
      "Crossed Categorical Features                                                                                 421\n",
      "Encoding Categorical Features Using One-Hot Vectors                                    422\n",
      "Encoding Categorical Features Using Embeddings                                            423\n",
      "Using Feature Columns for Parsing                                                                      426\n",
      "Using Feature Columns in Y our Models                                                               426\n",
      "TF Transform                                                                                                               428\n",
      "The TensorFlow Datasets (TFDS) Project                                                                429\n",
      "14. Deep Computer Vision Using Convolutional Neural Networks. . . . . . . . . . . . . . . . . . .  431\n",
      "The Architecture of the Visual Cortex                                                                     432\n",
      "Convolutional Layer                                                                                                    434\n",
      "Filters                                                                                                                         436\n",
      "Stacking Multiple Feature Maps                                                                             437\n",
      "TensorFlow Implementation                                                                                  439\n",
      "Table of Contents | ix\n",
      "Memory Requirements                                                                                           441\n",
      "Pooling Layer                                                                                                                442\n",
      "TensorFlow Implementation                                                                                  444\n",
      "CNN Architectures                                                                                                      446\n",
      "LeNet-5                                                                                                                      449\n",
      "AlexNet                                                                                                                      450\n",
      "GoogLeNet                                                                                                                452\n",
      "VGGNet                                                                                                                     456\n",
      "ResNet                                                                                                                        457\n",
      "Xception                                                                                                                    459\n",
      "SENet                                                                                                                         461\n",
      "Implementing a ResNet-34 CNN Using Keras                                                        464\n",
      "Using Pretrained Models From Keras                                                                      465\n",
      "Pretrained Models for Transfer Learning                                                                 467\n",
      "Classification and Localization                                                                                  469\n",
      "Object Detection                                                                                                          471\n",
      "Fully Convolutional Networks (FCNs)                                                                 473\n",
      "Y ou Only Look Once (YOLO)                                                                                475\n",
      "Semantic Segmentation                                                                                               478\n",
      "Exercises                                                                                                                        482\n",
      "x | Table of Contents\n",
      "1Available on Hinton’s home page at http://www.cs.toronto.edu/~hinton/ .\n",
      "2Despite the fact that Y ann Lecun’s deep convolutional neural networks had worked well for image recognition\n",
      "since the 1990s, although they were not as general purpose.Preface\n",
      "The Machine Learning Tsunami\n",
      "In 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\n",
      "network capable of recognizing handwritten digits with state-of-the-art precision\n",
      "(>98%). They branded this technique “Deep Learning. ” Training a deep neural net\n",
      "was widely considered impossible at the time,2 and most researchers had abandoned\n",
      "the idea since the 1990s. This paper revived the interest of the scientific community\n",
      "and before long many new papers demonstrated that Deep Learning was not only\n",
      "possible, but capable of mind-blowing achievements that no other Machine Learning\n",
      "(ML) technique could hope to match (with the help of tremendous computing power\n",
      "and great amounts of data). This enthusiasm soon extended to many other areas of\n",
      "Machine Learning.\n",
      "Fast-forward 10 years and Machine Learning has conquered the industry: it is now at\n",
      "the heart of much of the magic in today’s high-tech products, ranking your web\n",
      "search results, powering your smartphone’s speech recognition, recommending vid‐\n",
      "eos, and beating the world champion at the game of Go. Before you know it, it will be\n",
      "driving your car.\n",
      "Machine Learning in Your Projects\n",
      "So naturally you are excited about Machine Learning and you would love to join the\n",
      "party!\n",
      "Perhaps you would like to give your homemade robot a brain of its own? Make it rec‐\n",
      "ognize faces? Or learn to walk around?\n",
      "xi\n",
      "Or maybe your company has tons of data (user logs, financial data, production data,\n",
      "machine sensor data, hotline stats, HR reports, etc.), and more than likely you could\n",
      "unearth some hidden gems if you just knew where to look; for example:\n",
      "•Segment customers and find the best marketing strategy for each group\n",
      "•Recommend products for each client based on what similar clients bought\n",
      "•Detect which transactions are likely to be fraudulent\n",
      "•Forecast next year’s revenue\n",
      "•And more\n",
      "Whatever the reason, you have decided to learn Machine Learning and implement it\n",
      "in your projects. Great idea!\n",
      "Objective and Approach\n",
      "This book assumes that you know close to nothing about Machine Learning. Its goal\n",
      "is to give you the concepts, the intuitions, and the tools you need to actually imple‐\n",
      "ment programs capable of learning from data .\n",
      "We will cover a large number of techniques, from the simplest and most commonly\n",
      "used (such as linear regression) to some of the Deep Learning techniques that regu‐\n",
      "larly win competitions.\n",
      "Rather than implementing our own toy versions of each algorithm, we will be using\n",
      "actual production-ready Python frameworks:\n",
      "•Scikit-Learn  is very easy to use, yet it implements many Machine Learning algo‐\n",
      "rithms efficiently, so it makes for a great entry point to learn Machine Learning.\n",
      "•TensorFlow  is a more complex library for distributed numerical computation. It\n",
      "makes it possible to train and run very large neural networks efficiently by dis‐\n",
      "tributing the computations across potentially hundreds of multi-GPU servers.\n",
      "TensorFlow was created at Google and supports many of their large-scale\n",
      "Machine Learning applications. It was open sourced in November 2015.\n",
      "•Keras  is a high level Deep Learning API that makes it very simple to train and\n",
      "run neural networks. It can run on top of either TensorFlow, Theano or Micro‐\n",
      "soft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its\n",
      "own implementation of this API, called tf.keras , which provides support for some\n",
      "advanced TensorFlow features (e.g., to efficiently load data).\n",
      "The book favors a hands-on approach, growing an intuitive understanding of\n",
      "Machine Learning through concrete working examples and just a little bit of theory.\n",
      "While you can read this book without picking up your laptop, we highly recommend\n",
      "xii | Preface\n",
      "you experiment with the code examples available online as Jupyter notebooks at\n",
      "https://github.com/ageron/handson-ml2 .\n",
      "Prerequisites\n",
      "This book assumes that you have some Python programming experience and that you\n",
      "are familiar with Python’s main scientific libraries, in particular NumPy , Pandas , and\n",
      "Matplotlib .\n",
      "Also, if you care about what’s under the hood you should have a reasonable under‐\n",
      "standing of college-level math as well (calculus, linear algebra, probabilities, and sta‐\n",
      "tistics).\n",
      "If you don’t know Python yet, http://learnpython.org/  is a great place to start. The offi‐\n",
      "cial tutorial on python.org  is also quite good.\n",
      "If you have never used Jupyter, Chapter 2  will guide you through installation and the\n",
      "basics: it is a great tool to have in your toolbox.\n",
      "If you are not familiar with Python’s scientific libraries, the provided Jupyter note‐\n",
      "books include a few tutorials. There is also a quick math tutorial for linear algebra.\n",
      "Roadmap\n",
      "This book is organized in two parts. Part I, The Fundamentals of Machine Learning ,\n",
      "covers the following topics:\n",
      "•What is Machine Learning? What problems does it try to solve? What are the\n",
      "main categories and fundamental concepts of Machine Learning systems?\n",
      "•The main steps in a typical Machine Learning project.\n",
      "•Learning by fitting a model to data.\n",
      "•Optimizing a cost function.\n",
      "•Handling, cleaning, and preparing data.\n",
      "•Selecting and engineering features.\n",
      "•Selecting a model and tuning hyperparameters using cross-validation.\n",
      "•The main challenges of Machine Learning, in particular underfitting and overfit‐\n",
      "ting (the bias/variance tradeoff).\n",
      "•Reducing the dimensionality of the training data to fight the curse of dimension‐\n",
      "ality.\n",
      "•Other unsupervised learning techniques, including clustering, density estimation\n",
      "and anomaly detection.\n",
      "Preface | xiii\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "•The most common learning algorithms: Linear and Polynomial Regression,\n",
      "Logistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision\n",
      "Trees, Random Forests, and Ensemble methods.\n",
      "xiv | Preface\n",
      "Part II, Neural Networks and Deep Learning , covers the following topics:\n",
      "•What are neural nets? What are they good for?\n",
      "•Building and training neural nets using TensorFlow and Keras.\n",
      "•The most important neural net architectures: feedforward neural nets, convolu‐\n",
      "tional nets, recurrent nets, long short-term memory (LSTM) nets, autoencoders\n",
      "and generative adversarial networks (GANs).\n",
      "•Techniques for training deep neural nets.\n",
      "•Scaling neural networks for large datasets.\n",
      "•Learning strategies with Reinforcement Learning.\n",
      "•Handling uncertainty with Bayesian Deep Learning.\n",
      "The first part is based mostly on Scikit-Learn while the second part uses TensorFlow\n",
      "and Keras.\n",
      "Don’t jump into deep waters too hastily: while Deep Learning is no\n",
      "doubt one of the most exciting areas in Machine Learning, you\n",
      "should master the fundamentals first. Moreover, most problems\n",
      "can be solved quite well using simpler techniques such as Random\n",
      "Forests and Ensemble methods (discussed in Part I ). Deep Learn‐\n",
      "ing is best suited for complex problems such as image recognition,\n",
      "speech recognition, or natural language processing, provided you\n",
      "have enough data, computing power, and patience.\n",
      "Other Resources\n",
      "Many resources are available to learn about Machine Learning. Andrew Ng’s ML\n",
      "course on Coursera  and Geoffrey Hinton’s course on neural networks and Deep\n",
      "Learning  are amazing, although they both require a significant time investment\n",
      "(think months).\n",
      "There are also many interesting websites about Machine Learning, including of\n",
      "course Scikit-Learn’s exceptional User Guide . Y ou may also enjoy Dataquest , which\n",
      "provides very nice interactive tutorials, and ML blogs such as those listed on Quora .\n",
      "Finally, the Deep Learning website  has a good list of resources to learn more.\n",
      "Of course there are also many other introductory books about Machine Learning, in\n",
      "particular:\n",
      "•Joel Grus, Data Science from Scratch  (O’Reilly). This book presents the funda‐\n",
      "mentals of Machine Learning, and implements some of the main algorithms in\n",
      "pure Python (from scratch, as the name suggests).\n",
      "Preface | xv\n",
      "•Stephen Marsland, Machine Learning: An Algorithmic Perspective  (Chapman and\n",
      "Hall). This book is a great introduction to Machine Learning, covering a wide\n",
      "range of topics in depth, with code examples in Python (also from scratch, but\n",
      "using NumPy).\n",
      "•Sebastian Raschka, Python Machine Learning  (Packt Publishing). Also a great\n",
      "introduction to Machine Learning, this book leverages Python open source libra‐\n",
      "ries (Pylearn 2 and Theano).\n",
      "•François Chollet, Deep Learning with Python  (Manning). A very practical book\n",
      "that covers a large range of topics in a clear and concise way, as you might expect\n",
      "from the author of the excellent Keras library. It favors code examples over math‐\n",
      "ematical theory.\n",
      "•Y aser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from\n",
      "Data  (AMLBook). A rather theoretical approach to ML, this book provides deep\n",
      "insights, in particular on the bias/variance tradeoff (see Chapter 4 ).\n",
      "•Stuart Russell and Peter Norvig, Artificial  Intelligence: A Modern Approach, 3rd\n",
      "Edition  (Pearson). This is a great (and huge) book covering an incredible amount\n",
      "of topics, including Machine Learning. It helps put ML into perspective.\n",
      "Finally, a great way to learn is to join ML competition websites such as Kaggle.com\n",
      "this will allow you to practice your skills on real-world problems, with help and\n",
      "insights from some of the best ML professionals out there.\n",
      "Conventions Used in This Book\n",
      "The following typographical conventions are used in this book:\n",
      "Italic\n",
      "Indicates new terms, URLs, email addresses, filenames, and file extensions.\n",
      "Constant width\n",
      "Used for program listings, as well as within paragraphs to refer to program ele‐\n",
      "ments such as variable or function names, databases, data types, environment\n",
      "variables, statements and keywords.\n",
      "Constant width bold\n",
      "Shows commands or other text that should be typed literally by the user.\n",
      "Constant width italic\n",
      "Shows text that should be replaced with user-supplied values or by values deter‐\n",
      "mined by context.\n",
      "xvi | Preface\n",
      "This element signifies a tip or suggestion.\n",
      "This element signifies a general note.\n",
      "This element indicates a warning or caution.\n",
      "Code Examples\n",
      "Supplemental material (code examples, exercises, etc.) is available for download at\n",
      "https://github.com/ageron/handson-ml2 . It is mostly composed of Jupyter notebooks.\n",
      "Some of the code examples in the book leave out some repetitive sections, or details\n",
      "that are obvious or unrelated to Machine Learning. This keeps the focus on the\n",
      "important parts of the code, and it saves space to cover more topics. However, if you\n",
      "want the full code examples, they are all available in the Jupyter notebooks.\n",
      "Note that when the code examples display some outputs, then these code examples\n",
      "are shown with Python prompts ( >>> and ...), as in a Python shell, to clearly distin‐\n",
      "guish the code from the outputs. For example, this code defines the square()  func‐\n",
      "tion then it computes and displays the square of 3:\n",
      ">>> def square(x):\n",
      "...     return x ** 2\n",
      "...\n",
      ">>> result = square(3)\n",
      ">>> result\n",
      "9\n",
      "When code does not display anything, prompts are not used. However, the result may\n",
      "sometimes be shown as a comment like this:\n",
      "def square(x):\n",
      "    return x ** 2\n",
      "result = square(3)  # result is 9\n",
      "Preface | xvii\n",
      "Using Code Examples\n",
      "This book is here to help you get your job done. In general, if example code is offered\n",
      "with this book, you may use it in your programs and documentation. Y ou do not\n",
      "need to contact us for permission unless you’re reproducing a significant portion of\n",
      "the code. For example, writing a program that uses several chunks of code from this\n",
      "book does not require permission. Selling or distributing a CD-ROM of examples\n",
      "from O’Reilly books does require permission. Answering a question by citing this\n",
      "book and quoting example code does not require permission. Incorporating a signifi‐\n",
      "cant amount of example code from this book into your product’s documentation does\n",
      "require permission.\n",
      "We appreciate, but do not require, attribution. An attribution usually includes the\n",
      "title, author, publisher, and ISBN. For example: “ Hands-On Machine Learning with\n",
      "Scikit-Learn, Keras and TensorFlow  by Aurélien Géron (O’Reilly). Copyright 2019\n",
      "Aurélien Géron, 978-1-492-03264-9. ” If you feel your use of code examples falls out‐\n",
      "side fair use or the permission given above, feel free to contact us at permis‐\n",
      "sions@oreilly.com .\n",
      "O’Reilly Safari\n",
      "Safari  (formerly Safari Books Online) is a membership-based\n",
      "training and reference platform for enterprise, government,\n",
      "educators, and individuals.\n",
      "Members have access to thousands of books, training videos, Learning Paths, interac‐\n",
      "tive tutorials, and curated playlists from over 250 publishers, including O’Reilly\n",
      "Media, Harvard Business Review, Prentice Hall Professional, Addison-Wesley Profes‐\n",
      "sional, Microsoft Press, Sams, Que, Peachpit Press, Adobe, Focal Press, Cisco Press,\n",
      "John Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe\n",
      "Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and\n",
      "Course Technology, among others.\n",
      "For more information, please visit http://oreilly.com/safari .\n",
      "How to Contact Us\n",
      "Please address comments and questions concerning this book to the publisher:\n",
      "O’Reilly Media, Inc.\n",
      "1005 Gravenstein Highway North\n",
      "Sebastopol, CA 95472\n",
      "800-998-9938 (in the United States or Canada)\n",
      "xviii | Preface\n",
      "707-829-0515 (international or local)\n",
      "707-829-0104 (fax)\n",
      "We have a web page for this book, where we list errata, examples, and any additional\n",
      "information. Y ou can access this page at http://bit.ly/hands-on-machine-learning-\n",
      "with-scikit-learn-and-tensorflow  or https://homl.info/oreilly .\n",
      "To comment or ask technical questions about this book, send email to bookques‐\n",
      "tions@oreilly.com .\n",
      "For more information about our books, courses, conferences, and news, see our web‐\n",
      "site at http://www.oreilly.com .\n",
      "Find us on Facebook: http://facebook.com/oreilly\n",
      "Follow us on Twitter: http://twitter.com/oreillymedia\n",
      "Watch us on Y ouTube: http://www.youtube.com/oreillymedia\n",
      "Changes in the Second Edition\n",
      "This second edition has five main objectives:\n",
      "1.Cover additional topics: additional unsupervised learning techniques (including\n",
      "clustering, anomaly detection, density estimation and mixture models), addi‐\n",
      "tional techniques for training deep nets (including self-normalized networks),\n",
      "additional computer vision techniques (including the Xception, SENet, object\n",
      "detection with YOLO, and semantic segmentation using R-CNN), handling\n",
      "sequences using CNNs (including WaveNet), natural language processing using\n",
      "RNNs, CNNs and Transformers, generative adversarial networks, deploying Ten‐\n",
      "sorFlow models, and more.\n",
      "2.Update the book to mention some of the latest results from Deep Learning\n",
      "research.\n",
      "3.Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlow’s imple‐\n",
      "mentation of the Keras API (called tf.keras) whenever possible, to simplify the\n",
      "code examples.\n",
      "4.Update the code examples to use the latest version of Scikit-Learn, NumPy, Pan‐\n",
      "das, Matplotlib and other libraries.\n",
      "5.Clarify some sections and fix some errors, thanks to plenty of great feedback\n",
      "from readers.\n",
      "Some chapters were added, others were rewritten and a few were reordered. Table P-1\n",
      "shows the mapping between the 1st edition chapters and the 2nd edition chapters:\n",
      "Preface | xix\n",
      "Table P-1. Chapter mapping between 1st and 2nd edition\n",
      "1st Ed. chapter 2nd Ed. Chapter % Changes 2nd Ed. Title\n",
      "1 1 <10% The Machine Learning Landscape\n",
      "2 2 <10% End-to-End Machine Learning Project\n",
      "3 3 <10% Classification\n",
      "4 4 <10% Training Models\n",
      "5 5 <10% Support Vector Machines\n",
      "6 6 <10% Decision Trees\n",
      "7 7 <10% Ensemble Learning and Random Forests\n",
      "8 8 <10% Dimensionality Reduction\n",
      "N/A 9 100% new Unsupervised Learning Techniques\n",
      "10 10 ~75% Introduction to Artificial  Neural Networks with Keras\n",
      "11 11 ~50% Training Deep Neural Networks\n",
      "9 12 100% rewritten Custom Models and Training with TensorFlow\n",
      "Part of 12 13 100% rewritten Loading and Preprocessing Data with TensorFlow\n",
      "13 14 ~50% Deep Computer Vision Using Convolutional Neural Networks\n",
      "Part of 14 15 ~75% Processing Sequences Using RNNs and CNNs\n",
      "Part of 14 16 ~90% Natural Language Processing with RNNs and Attention\n",
      "15 17 ~75% Autoencoders and GANs\n",
      "16 18 ~75% Reinforcement Learning\n",
      "Part of 12 19 100% rewritten Deploying your TensorFlow Models\n",
      "More specifically, here are the main changes for each 2nd edition chapter (other than\n",
      "clarifications, corrections and code updates):\n",
      "•Chapter 1\n",
      "—Added a section on handling mismatch between the training set and the vali‐\n",
      "dation & test sets.\n",
      "•Chapter 2\n",
      "—Added how to compute a confidence interval.\n",
      "—Improved the installation instructions (e.g., for Windows).\n",
      "—Introduced the upgraded OneHotEncoder  and the new ColumnTransformer .\n",
      "•Chapter 4\n",
      "—Explained the need for training instances to be Independent and Identically\n",
      "Distributed (IID).\n",
      "•Chapter 7\n",
      "—Added a short section about XGBoost.\n",
      "xx | Preface\n",
      "•Chapter 9 – new chapter including:\n",
      "—Clustering with K-Means, how to choose the number of clusters, how to use it\n",
      "for dimensionality reduction, semi-supervised learning, image segmentation,\n",
      "and more.\n",
      "—The DBSCAN clustering algorithm and an overview of other clustering algo‐\n",
      "rithms available in Scikit-Learn.\n",
      "—Gaussian mixture models, the Expectation-Maximization (EM) algorithm,\n",
      "Bayesian variational inference, and how mixture models can be used for clus‐\n",
      "tering, density estimation, anomaly detection and novelty detection.\n",
      "—Overview of other anomaly detection and novelty detection algorithms.\n",
      "•Chapter 10 (mostly new)\n",
      "—Added an introduction to the Keras API, including all its APIs (Sequential,\n",
      "Functional and Subclassing), persistence and callbacks (including the Tensor\n",
      "Board  callback).\n",
      "•Chapter 11 (many changes)\n",
      "—Introduced self-normalizing nets, the SELU activation function and Alpha\n",
      "Dropout.\n",
      "—Introduced self-supervised learning.\n",
      "—Added Nadam optimization.\n",
      "—Added Monte-Carlo Dropout.\n",
      "—Added a note about the risks of adaptive optimization methods.\n",
      "—Updated the practical guidelines.\n",
      "•Chapter 12 – completely rewritten chapter, including:\n",
      "—A tour of TensorFlow 2\n",
      "—TensorFlow’s lower-level Python API\n",
      "—Writing custom loss functions, metrics, layers, models\n",
      "—Using auto-differentiation and creating custom training algorithms.\n",
      "—TensorFlow Functions and graphs (including tracing and autograph).\n",
      "•Chapter 13 – new chapter, including:\n",
      "—The Data API\n",
      "—Loading/Storing data efficiently using TFRecords\n",
      "—The Features API (including an introduction to embeddings).\n",
      "—An overview of TF Transform and TF Datasets\n",
      "—Moved the low-level implementation of the neural network to the exercises.\n",
      "Preface | xxi\n",
      "—Removed details about queues and readers that are now superseded by the\n",
      "Data API.\n",
      "•Chapter 14\n",
      "—Added Xception and SENet architectures.\n",
      "—Added a Keras implementation of ResNet-34.\n",
      "—Showed how to use pretrained models using Keras.\n",
      "—Added an end-to-end transfer learning example.\n",
      "—Added classification and localization.\n",
      "—Introduced Fully Convolutional Networks (FCNs).\n",
      "—Introduced object detection using the YOLO architecture.\n",
      "—Introduced semantic segmentation using R-CNN.\n",
      "•Chapter 15\n",
      "—Added an introduction to Wavenet.\n",
      "—Moved the Encoder–Decoder architecture and Bidirectional RNNs to Chapter\n",
      "16.\n",
      "•Chapter 16\n",
      "—Explained how to use the Data API to handle sequential data.\n",
      "—Showed an end-to-end example of text generation using a Character RNN,\n",
      "using both a stateless and a stateful RNN.\n",
      "—Showed an end-to-end example of sentiment analysis using an LSTM.\n",
      "—Explained masking in Keras.\n",
      "—Showed how to reuse pretrained embeddings using TF Hub.\n",
      "—Showed how to build an Encoder–Decoder for Neural Machine Translation\n",
      "using TensorFlow Addons/seq2seq.\n",
      "—Introduced beam search.\n",
      "—Explained attention mechanisms.\n",
      "—Added a short overview of visual attention and a note on explainability.\n",
      "—Introduced the fully attention-based Transformer architecture, including posi‐\n",
      "tional embeddings and multi-head attention.\n",
      "—Added an overview of recent language models (2018).\n",
      "•Chapters 17, 18 and 19: coming soon.\n",
      "xxii | Preface\n",
      "3“Deep Learning with Python, ” François Chollet (2017).Acknowledgments\n",
      "Never in my wildest dreams did I imagine that the first edition of this book would get\n",
      "such a large audience. I received so many messages from readers, many asking ques‐\n",
      "tions, some kindly pointing out errata, and most sending me encouraging words. I\n",
      "cannot express how grateful I am to all these readers for their tremendous support.\n",
      "Thank you all so very much! Please do not hesitate to file issues on github  if you find\n",
      "errors in the code examples (or just to ask questions), or to submit errata  if you find\n",
      "errors in the text. Some readers also shared how this book helped them get their first\n",
      "job, or how it helped them solve a concrete problem they were working on: I find\n",
      "such feedback incredibly motivating. If you find this book helpful, I would love it if\n",
      "you could share your story with me, either privately (e.g., via LinkedIn ) or publicly\n",
      "(e.g., in an Amazon review ).\n",
      "I am also incredibly thankful to all the amazing people who took time out of their\n",
      "busy lives to review my book with such care. In particular, I would like to thank Fran‐\n",
      "çois Chollet for reviewing all the chapters based on Keras & TensorFlow, and giving\n",
      "me some great, in-depth feedback. Since Keras is one of the main additions to this 2nd\n",
      "edition, having its author review the book was invaluable. I highly recommend Fran‐\n",
      "çois’s excellent book Deep Learning with Python3: it has the conciseness, clarity and\n",
      "depth of the Keras library itself. Big thanks as well to Ankur Patel, who reviewed\n",
      "every chapter of this 2nd edition and gave me excellent feedback.\n",
      "This book also benefited from plenty of help from members of the TensorFlow team,\n",
      "in particular Martin Wicke, who tirelessly answered dozens of my questions and dis‐\n",
      "patched the rest to the right people, including Alexandre Passos, Allen Lavoie, André\n",
      "Susano Pinto, Anna Revinskaya, Anthony Platanios, Clemens Mewald, Dan Moldo‐\n",
      "van, Daniel Dobson, Dustin Tran, Edd Wilder-James, Goldie Gadde, Jiri Simsa, Kar‐\n",
      "mel Allison, Nick Felt, Paige Bailey, Pete Warden (who also reviewed the 1st edition),\n",
      "Ryan Sepassi, Sandeep Gupta, Sean Morgan, Todd Wang, Tom O’Malley, William\n",
      "Chargin, and Yuefeng Zhou, all of whom were tremendously helpful. A huge thank\n",
      "you to all of you, and to all other members of the TensorFlow team. Not just for your\n",
      "help, but also for making such a great library.\n",
      "Big thanks to Haesun Park, who gave me plenty of excellent feedback and caught sev‐\n",
      "eral errors while he was writing the Korean translation of the 1st edition of this book.\n",
      "He also translated the Jupyter notebooks to Korean, not to mention TensorFlow’s\n",
      "documentation. I do not speak Korean, but judging by the quality of his feedback, all\n",
      "his translations must be truly excellent! Moreover, he kindly contributed some of the\n",
      "solutions to the exercises in this book.\n",
      "Preface | xxiii\n",
      "Many thanks as well to O’Reilly’s fantastic staff, in particular Nicole Tache, who gave\n",
      "me insightful feedback, always cheerful, encouraging, and helpful: I could not dream\n",
      "of a better editor. Big thanks to Michele Cronin as well, who was very helpful (and\n",
      "patient) at the start of this 2nd edition. Thanks to Marie Beaugureau, Ben Lorica, Mike\n",
      "Loukides, and Laurel Ruma for believing in this project and helping me define its\n",
      "scope. Thanks to Matt Hacker and all of the Atlas team for answering all my technical\n",
      "questions regarding formatting, asciidoc, and LaTeX, and thanks to Rachel Mona‐\n",
      "ghan, Nick Adams, and all of the production team for their final review and their\n",
      "hundreds of corrections.\n",
      "I would also like to thank my former Google colleagues, in particular the Y ouTube\n",
      "video classification team, for teaching me so much about Machine Learning. I could\n",
      "never have started the first edition without them. Special thanks to my personal ML\n",
      "gurus: Clément Courbet, Julien Dubois, Mathias Kende, Daniel Kitachewsky, James\n",
      "Pack, Alexander Pak, Anosh Raj, Vitor Sessak, Wiktor Tomczak, Ingrid von Glehn,\n",
      "Rich Washington, and everyone I worked with at Y ouTube and in the amazing Goo‐\n",
      "gle research teams in Mountain View. All these people are just as nice and helpful as\n",
      "they are bright, and that’s saying a lot.\n",
      "I will never forget the kind people who reviewed the 1st edition of this book, including\n",
      "David Andrzejewski, Eddy Hung, Grégoire Mesnil, Iain Smears, Ingrid von Glehn,\n",
      "Justin Francis, Karim Matrah, Lukas Biewald, Michel Tessier, Salim Sémaoune, Vin‐\n",
      "cent Guilbeau and of course my dear brother Sylvain.\n",
      "Last but not least, I am infinitely grateful to my beloved wife, Emmanuelle, and to our\n",
      "three wonderful children, Alexandre, Rémi, and Gabrielle, for encouraging me to\n",
      "work hard on this book, as well as for their insatiable curiosity: explaining some of\n",
      "the most difficult concepts in this book to my wife and children helped me clarify my\n",
      "thoughts and directly improved many parts of this book. Plus, they keep bringing me\n",
      "cookies and coffee! What more can one dream of?\n",
      "xxiv | Preface\n",
      "PART I\n",
      "The Fundamentals of\n",
      "Machine Learning\n",
      "\n",
      "CHAPTER 1\n",
      "The Machine Learning Landscape\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 1 in the final\n",
      "release of the book.\n",
      "When most people hear “Machine Learning, ” they picture a robot: a dependable but‐\n",
      "ler or a deadly Terminator depending on who you ask. But Machine Learning is not\n",
      "just a futuristic fantasy, it’s already here. In fact, it has been around for decades in\n",
      "some specialized applications, such as Optical Character Recognition  (OCR). But the\n",
      "first ML application that really became mainstream, improving the lives of hundreds\n",
      "of millions of people, took over the world back in the 1990s: it was the spam filter .\n",
      "Not exactly a self-aware Skynet, but it does technically qualify as Machine Learning\n",
      "(it has actually learned so well that you seldom need to flag an email as spam any‐\n",
      "more). It was followed by hundreds of ML applications that now quietly power hun‐\n",
      "dreds of products and features that you use regularly, from better recommendations\n",
      "to voice search.\n",
      "Where does Machine Learning start and where does it end? What exactly does it\n",
      "mean for a machine to learn  something? If I download a copy of Wikipedia, has my\n",
      "computer really “learned” something? Is it suddenly smarter? In this chapter we will\n",
      "start by clarifying what Machine Learning is and why you may want to use it.\n",
      "Then, before we set out to explore the Machine Learning continent, we will take a\n",
      "look at the map and learn about the main regions and the most notable landmarks:\n",
      "supervised versus unsupervised learning, online versus batch learning, instance-\n",
      "based versus model-based learning. Then we will look at the workflow of a typical ML\n",
      "project, discuss the main challenges you may face, and cover how to evaluate and\n",
      "fine-tune a Machine Learning system.\n",
      "3\n",
      "This chapter introduces a lot of fundamental concepts (and jargon) that every data\n",
      "scientist should know by heart. It will be a high-level overview (the only chapter\n",
      "without much code), all rather simple, but you should make sure everything is\n",
      "crystal-clear to you before continuing to the rest of the book. So grab a coffee and let’s\n",
      "get started!\n",
      "If you already know all the Machine Learning basics, you may want\n",
      "to skip directly to Chapter 2 . If you are not sure, try to answer all\n",
      "the questions listed at the end of the chapter before moving on.\n",
      "What Is Machine Learning?\n",
      "Machine Learning is the science (and art) of programming computers so they can\n",
      "learn from data .\n",
      "Here is a slightly more general definition:\n",
      "[Machine Learning is the] field of study that gives computers the ability to learn\n",
      "without being explicitly programmed.\n",
      "—Arthur Samuel, 1959\n",
      "And a more engineering-oriented one:\n",
      "A computer program is said to learn from experience E with respect to some task T\n",
      "and some performance measure P , if its performance on T, as measured by P , improves\n",
      "with experience E.\n",
      "—Tom Mitchell, 1997\n",
      "For example, your spam filter is a Machine Learning program that can learn to flag\n",
      "spam given examples of spam emails (e.g., flagged by users) and examples of regular\n",
      "(nonspam, also called “ham”) emails. The examples that the system uses to learn are\n",
      "called the training set . Each training example is called a training instance  (or sample ).\n",
      "In this case, the task T is to flag spam for new emails, the experience E is the training\n",
      "data , and the performance measure P needs to be defined; for example, you can use\n",
      "the ratio of correctly classified emails. This particular performance measure is called\n",
      "accuracy  and it is often used in classification tasks.\n",
      "If you just download a copy of Wikipedia, your computer has a lot more data, but it is\n",
      "not suddenly better at any task. Thus, it is not Machine Learning.\n",
      "Why Use Machine Learning?\n",
      "Consider how you would write a spam filter using traditional programming techni‐\n",
      "ques ( Figure 1-1 ):\n",
      "4 | Chapter 1: The Machine Learning Landscape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.First you would look at what spam typically looks like. Y ou might notice that\n",
      "some words or phrases (such as “4U, ” “credit card, ” “free, ” and “amazing”) tend to\n",
      "come up a lot in the subject. Perhaps you would also notice a few other patterns\n",
      "in the sender’s name, the email’s body, and so on.\n",
      "2.Y ou would write a detection algorithm for each of the patterns that you noticed,\n",
      "and your program would flag emails as spam if a number of these patterns are\n",
      "detected.\n",
      "3.Y ou would test your program, and repeat steps 1 and 2 until it is good enough.\n",
      "Figure 1-1. The traditional approach\n",
      "Since the problem is not trivial, your program will likely become a long list of com‐\n",
      "plex rules—pretty hard to maintain.\n",
      "In contrast, a spam filter based on Machine Learning techniques automatically learns\n",
      "which words and phrases are good predictors of spam by detecting unusually fre‐\n",
      "quent patterns of words in the spam examples compared to the ham examples\n",
      "(Figure 1-2 ). The program is much shorter, easier to maintain, and most likely more\n",
      "accurate.\n",
      "Why Use Machine Learning? | 5\n",
      "Figure 1-2. Machine Learning approach\n",
      "Moreover, if spammers notice that all their emails containing “4U” are blocked, they\n",
      "might start writing “For U” instead. A spam filter using traditional programming\n",
      "techniques would need to be updated to flag “For U” emails. If spammers keep work‐\n",
      "ing around your spam filter, you will need to keep writing new rules forever.\n",
      "In contrast, a spam filter based on Machine Learning techniques automatically noti‐\n",
      "ces that “For U” has become unusually frequent in spam flagged by users, and it starts\n",
      "flagging them without your intervention ( Figure 1-3 ).\n",
      "Figure 1-3. Automatically adapting to change\n",
      "Another area where Machine Learning shines is for problems that either are too com‐\n",
      "plex for traditional approaches or have no known algorithm. For example, consider \n",
      "speech recognition: say you want to start simple and write a program capable of dis‐\n",
      "tinguishing the words “one” and “two. ” Y ou might notice that the word “two” starts\n",
      "with a high-pitch sound (“T”), so you could hardcode an algorithm that measures\n",
      "high-pitch sound intensity and use that to distinguish ones and twos. Obviously this\n",
      "technique will not scale to thousands of words spoken by millions of very different\n",
      "6 | Chapter 1: The Machine Learning Landscape\n",
      "people in noisy environments and in dozens of languages. The best solution (at least\n",
      "today) is to write an algorithm that learns by itself, given many example recordings\n",
      "for each word.\n",
      "Finally, Machine Learning can help humans learn ( Figure 1-4 ): ML algorithms can be\n",
      "inspected to see what they have learned (although for some algorithms this can be\n",
      "tricky). For instance, once the spam filter has been trained on enough spam, it can\n",
      "easily be inspected to reveal the list of words and combinations of words that it\n",
      "believes are the best predictors of spam. Sometimes this will reveal unsuspected cor‐\n",
      "relations or new trends, and thereby lead to a better understanding of the problem.\n",
      "Applying ML techniques to dig into large amounts of data can help discover patterns\n",
      "that were not immediately apparent. This is called data mining .\n",
      "Figure 1-4. Machine Learning can help humans learn\n",
      "To summarize, Machine Learning is great for:\n",
      "•Problems for which existing solutions require a lot of hand-tuning or long lists of\n",
      "rules: one Machine Learning algorithm can often simplify code and perform bet‐\n",
      "ter.\n",
      "•Complex problems for which there is no good solution at all using a traditional\n",
      "approach: the best Machine Learning techniques can find a solution.\n",
      "•Fluctuating environments: a Machine Learning system can adapt to new data.\n",
      "•Getting insights about complex problems and large amounts of data.\n",
      "Why Use Machine Learning? | 7\n",
      "Types of Machine Learning Systems\n",
      "There are so many different types of Machine Learning systems that it is useful to\n",
      "classify them in broad categories based on:\n",
      "•Whether or not they are trained with human supervision (supervised, unsuper‐\n",
      "vised, semisupervised, and Reinforcement Learning)\n",
      "•Whether or not they can learn incrementally on the fly (online versus batch\n",
      "learning)\n",
      "•Whether they work by simply comparing new data points to known data points,\n",
      "or instead detect patterns in the training data and build a predictive model, much\n",
      "like scientists do (instance-based versus model-based learning)\n",
      "These criteria are not exclusive; you can combine them in any way you like. For\n",
      "example, a state-of-the-art spam filter may learn on the fly using a deep neural net‐\n",
      "work model trained using examples of spam and ham; this makes it an online, model-\n",
      "based, supervised learning system.\n",
      "Let’s look at each of these criteria a bit more closely.\n",
      "Supervised/Unsupervised Learning\n",
      "Machine Learning systems can be classified according to the amount and type of\n",
      "supervision they get during training. There are four major categories: supervised\n",
      "learning, unsupervised learning, semisupervised learning, and Reinforcement Learn‐\n",
      "ing.\n",
      "Supervised learning\n",
      "In supervised learning , the training data you feed to the algorithm includes the desired\n",
      "solutions, called labels  (Figure 1-5 ).\n",
      "Figure 1-5. A labeled training set for supervised learning (e.g., spam classification)\n",
      "8 | Chapter 1: The Machine Learning Landscape\n",
      "1Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was studying the\n",
      "fact that the children of tall people tend to be shorter than their parents. Since children were shorter, he called\n",
      "this regression to the mean . This name was then applied to the methods he used to analyze correlations\n",
      "between variables.\n",
      "A typical supervised learning task is classification . The spam filter is a good example\n",
      "of this: it is trained with many example emails along with their class  (spam or ham),\n",
      "and it must learn how to classify new emails.\n",
      "Another typical task is to predict a target  numeric value, such as the price of a car,\n",
      "given a set of features  (mileage, age, brand, etc.) called predictors . This sort of task is \n",
      "called regression  (Figure 1-6 ).1 To train the system, you need to give it many examples\n",
      "of cars, including both their predictors and their labels (i.e., their prices).\n",
      "In Machine Learning an attribute  is a data type (e.g., “Mileage”),\n",
      "while a feature  has several meanings depending on the context, but\n",
      "generally means an attribute plus its value (e.g., “Mileage =\n",
      "15,000”). Many people use the words attribute  and feature  inter‐\n",
      "changeably, though.\n",
      "Figure 1-6. Regression\n",
      "Note that some regression algorithms can be used for classification as well, and vice\n",
      "versa. For example, Logistic Regression  is commonly used for classification, as it can\n",
      "output a value that corresponds to the probability of belonging to a given class (e.g.,\n",
      "20% chance of being spam).\n",
      "Types of Machine Learning Systems | 9\n",
      "2Some neural network architectures can be unsupervised, such as autoencoders and restricted Boltzmann\n",
      "machines. They can also be semisupervised, such as in deep belief networks and unsupervised pretraining.Here are some of the most important supervised learning algorithms (covered in this\n",
      "book):\n",
      "•k-Nearest Neighbors\n",
      "•Linear Regression\n",
      "•Logistic Regression\n",
      "•Support Vector Machines (SVMs)\n",
      "•Decision Trees and Random Forests\n",
      "•Neural networks2\n",
      "Unsupervised learning\n",
      "In unsupervised learning , as you might guess, the training data is unlabeled\n",
      "(Figure 1-7 ). The system tries to learn without a teacher.\n",
      "Figure 1-7. An unlabeled training set for unsupervised learning\n",
      "Here are some of the most important unsupervised learning algorithms (most of\n",
      "these are covered in Chapter 8  and Chapter 9 ):\n",
      "•Clustering\n",
      "—K-Means\n",
      "—DBSCAN\n",
      "—Hierarchical Cluster Analysis (HCA)\n",
      "•Anomaly detection and novelty detection\n",
      "—One-class SVM\n",
      "—Isolation Forest\n",
      "10 | Chapter 1: The Machine Learning Landscape\n",
      "•Visualization and dimensionality reduction\n",
      "—Principal Component Analysis (PCA)\n",
      "—Kernel PCA\n",
      "—Locally-Linear Embedding (LLE)\n",
      "—t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
      "•Association rule learning\n",
      "—Apriori\n",
      "—Eclat\n",
      "For example, say you have a lot of data about your blog’s visitors. Y ou may want to\n",
      "run a clustering  algorithm to try to detect groups of similar visitors ( Figure 1-8 ). At\n",
      "no point do you tell the algorithm which group a visitor belongs to: it finds those\n",
      "connections without your help. For example, it might notice that 40% of your visitors\n",
      "are males who love comic books and generally read your blog in the evening, while\n",
      "20% are young sci-fi lovers who visit during the weekends, and so on. If you use a\n",
      "hierarchical clustering  algorithm, it may also subdivide each group into smaller\n",
      "groups. This may help you target your posts for each group.\n",
      "Figure 1-8. Clustering\n",
      "Visualization  algorithms are also good examples of unsupervised learning algorithms:\n",
      "you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep‐\n",
      "resentation of your data that can easily be plotted ( Figure 1-9 ). These algorithms try\n",
      "to preserve as much structure as they can (e.g., trying to keep separate clusters in the\n",
      "input space from overlapping in the visualization), so you can understand how the\n",
      "data is organized and perhaps identify unsuspected patterns.\n",
      "Types of Machine Learning Systems | 11\n",
      "3Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds,\n",
      "and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), “T-SNE visual‐\n",
      "ization of the semantic word space. ”\n",
      "Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3\n",
      "A related task is dimensionality reduction , in which the goal is to simplify the data\n",
      "without losing too much information. One way to do this is to merge several correla‐\n",
      "ted features into one. For example, a car’s mileage may be very correlated with its age,\n",
      "so the dimensionality reduction algorithm will merge them into one feature that rep‐\n",
      "resents the car’s wear and tear. This is called feature extraction .\n",
      "It is often a good idea to try to reduce the dimension of your train‐\n",
      "ing data using a dimensionality reduction algorithm before you\n",
      "feed it to another Machine Learning algorithm (such as a super‐\n",
      "vised learning algorithm). It will run much faster, the data will take\n",
      "up less disk and memory space, and in some cases it may also per‐\n",
      "form better.\n",
      "Y et another important unsupervised task is anomaly detection —for example, detect‐\n",
      "ing unusual credit card transactions to prevent fraud, catching manufacturing defects,\n",
      "or automatically removing outliers from a dataset before feeding it to another learn‐\n",
      "ing algorithm. The system is shown mostly normal instances during training, so it\n",
      "learns to recognize them and when it sees a new instance it can tell whether it looks\n",
      "12 | Chapter 1: The Machine Learning Landscape\n",
      "4That’s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes\n",
      "mixes up two people who look alike, so you need to provide a few labels per person and manually clean up\n",
      "some clusters.like a normal one or whether it is likely an anomaly (see Figure 1-10 ). A very similar\n",
      "task is novelty detection : the difference is that novelty detection algorithms expect to\n",
      "see only normal data during training, while anomaly detection algorithms are usually\n",
      "more tolerant, they can often perform well even with a small percentage of outliers in\n",
      "the training set.\n",
      "Figure 1-10. Anomaly detection\n",
      "Finally, another common unsupervised task is association rule learning , in which the\n",
      "goal is to dig into large amounts of data and discover interesting relations between\n",
      "attributes. For example, suppose you own a supermarket. Running an association rule\n",
      "on your sales logs may reveal that people who purchase barbecue sauce and potato\n",
      "chips also tend to buy steak. Thus, you may want to place these items close to each \n",
      "other.\n",
      "Semisupervised learning\n",
      "Some algorithms can deal with partially labeled training data, usually a lot of unla‐\n",
      "beled data and a little bit of labeled data. This is called semisupervised learning\n",
      "(Figure 1-11 ).\n",
      "Some photo-hosting services, such as Google Photos, are good examples of this. Once\n",
      "you upload all your family photos to the service, it automatically recognizes that the\n",
      "same person A shows up in photos 1, 5, and 11, while another person B shows up in\n",
      "photos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all\n",
      "the system needs is for you to tell it who these people are. Just one label per person,4\n",
      "and it is able to name everyone in every photo, which is useful for searching photos.\n",
      "Types of Machine Learning Systems | 13\n",
      "Figure 1-11. Semisupervised learning\n",
      "Most semisupervised learning algorithms are combinations of unsupervised and\n",
      "supervised algorithms. For example, deep belief networks  (DBNs) are based on unsu‐\n",
      "pervised components called restricted Boltzmann machines  (RBMs) stacked on top of\n",
      "one another. RBMs are trained sequentially in an unsupervised manner, and then the\n",
      "whole system is fine-tuned using supervised learning techniques.\n",
      "Reinforcement Learning\n",
      "Reinforcement Learning  is a very different beast. The learning system, called an agent\n",
      "in this context, can observe the environment, select and perform actions, and get\n",
      "rewards  in return (or penalties  in the form of negative rewards, as in Figure 1-12 ). It\n",
      "must then learn by itself what is the best strategy, called a policy , to get the most\n",
      "reward over time. A policy defines what action the agent should choose when it is in a\n",
      "given situation.\n",
      "14 | Chapter 1: The Machine Learning Landscape\n",
      "Figure 1-12. Reinforcement Learning\n",
      "For example, many robots implement Reinforcement Learning algorithms to learn\n",
      "how to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement\n",
      "Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie\n",
      "at the game of Go. It learned its winning policy by analyzing millions of games, and\n",
      "then playing many games against itself. Note that learning was turned off during the\n",
      "games against the champion; AlphaGo was just applying the policy it had learned.\n",
      "Batch and Online Learning\n",
      "Another criterion used to classify Machine Learning systems is whether or not the\n",
      "system can learn incrementally from a stream of incoming data.\n",
      "Batch learning\n",
      "In batch learning , the system is incapable of learning incrementally: it must be trained\n",
      "using all the available data. This will generally take a lot of time and computing\n",
      "resources, so it is typically done offline. First the system is trained, and then it is\n",
      "launched into production and runs without learning anymore; it just applies what it\n",
      "has learned. This is called offline  learning .\n",
      "If you want a batch learning system to know about new data (such as a new type of\n",
      "spam), you need to train a new version of the system from scratch on the full dataset\n",
      "(not just the new data, but also the old data), then stop the old system and replace it\n",
      "with the new one.\n",
      "Fortunately, the whole process of training, evaluating, and launching a Machine\n",
      "Learning system can be automated fairly easily (as shown in Figure 1-3 ), so even a\n",
      "Types of Machine Learning Systems | 15\n",
      "batch learning system can adapt to change. Simply update the data and train a new\n",
      "version of the system from scratch as often as needed.\n",
      "This solution is simple and often works fine, but training using the full set of data can\n",
      "take many hours, so you would typically train a new system only every 24 hours or\n",
      "even just weekly. If your system needs to adapt to rapidly changing data (e.g., to pre‐\n",
      "dict stock prices), then you need a more reactive solution.\n",
      "Also, training on the full set of data requires a lot of computing resources (CPU,\n",
      "memory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and\n",
      "you automate your system to train from scratch every day, it will end up costing you a\n",
      "lot of money. If the amount of data is huge, it may even be impossible to use a batch\n",
      "learning algorithm.\n",
      "Finally, if your system needs to be able to learn autonomously and it has limited\n",
      "resources (e.g., a smartphone application or a rover on Mars), then carrying around\n",
      "large amounts of training data and taking up a lot of resources to train for hours\n",
      "every day is a showstopper.\n",
      "Fortunately, a better option in all these cases is to use algorithms that are capable of\n",
      "learning incrementally.\n",
      "Online learning\n",
      "In online learning , you train the system incrementally by feeding it data instances\n",
      "sequentially, either individually or by small groups called mini-batches . Each learning\n",
      "step is fast and cheap, so the system can learn about new data on the fly, as it arrives\n",
      "(see Figure 1-13 ).\n",
      "Figure 1-13. Online learning\n",
      "Online learning is great for systems that receive data as a continuous flow (e.g., stock\n",
      "prices) and need to adapt to change rapidly or autonomously. It is also a good option\n",
      "16 | Chapter 1: The Machine Learning Landscape\n",
      "if you have limited computing resources: once an online learning system has learned\n",
      "about new data instances, it does not need them anymore, so you can discard them\n",
      "(unless you want to be able to roll back to a previous state and “replay” the data). This\n",
      "can save a huge amount of space.\n",
      "Online learning algorithms can also be used to train systems on huge datasets that\n",
      "cannot fit in one machine’s main memory (this is called out-of-core  learning). The\n",
      "algorithm loads part of the data, runs a training step on that data, and repeats the\n",
      "process until it has run on all of the data (see Figure 1-14 ).\n",
      "Out-of-core learning is usually done offline (i.e., not on the live\n",
      "system), so online learning  can be a confusing name. Think of it as\n",
      "incremental learning .\n",
      "Figure 1-14. Using online learning to handle huge datasets\n",
      "One important parameter of online learning systems is how fast they should adapt to\n",
      "changing data: this is called the learning rate . If you set a high learning rate, then your\n",
      "system will rapidly adapt to new data, but it will also tend to quickly forget the old\n",
      "data (you don’t want a spam filter to flag only the latest kinds of spam it was shown).\n",
      "Conversely, if you set a low learning rate, the system will have more inertia; that is, it\n",
      "will learn more slowly, but it will also be less sensitive to noise in the new data or to\n",
      "sequences of nonrepresentative data points (outliers).\n",
      "A big challenge with online learning is that if bad data is fed to the system, the sys‐\n",
      "tem’s performance will gradually decline. If we are talking about a live system, your\n",
      "clients will notice. For example, bad data could come from a malfunctioning sensor\n",
      "on a robot, or from someone spamming a search engine to try to rank high in search\n",
      "Types of Machine Learning Systems | 17\n",
      "results. To reduce this risk, you need to monitor your system closely and promptly\n",
      "switch learning off (and possibly revert to a previously working state) if you detect a\n",
      "drop in performance. Y ou may also want to monitor the input data and react to\n",
      "abnormal data (e.g., using an anomaly detection algorithm).\n",
      "Instance-Based Versus Model-Based Learning\n",
      "One more way to categorize Machine Learning systems is by how they generalize .\n",
      "Most Machine Learning tasks are about making predictions. This means that given a\n",
      "number of training examples, the system needs to be able to generalize to examples it\n",
      "has never seen before. Having a good performance measure on the training data is\n",
      "good, but insufficient; the true goal is to perform well on new instances.\n",
      "There are two main approaches to generalization: instance-based learning and\n",
      "model-based learning.\n",
      "Instance-based learning\n",
      "Possibly the most trivial form of learning is simply to learn by heart. If you were to\n",
      "create a spam filter this way, it would just flag all emails that are identical to emails\n",
      "that have already been flagged by users—not the worst solution, but certainly not the\n",
      "best.\n",
      "Instead of just flagging emails that are identical to known spam emails, your spam\n",
      "filter could be programmed to also flag emails that are very similar to known spam\n",
      "emails. This requires a measure of similarity  between two emails. A (very basic) simi‐\n",
      "larity measure between two emails could be to count the number of words they have\n",
      "in common. The system would flag an email as spam if it has many words in com‐\n",
      "mon with a known spam email.\n",
      "This is called instance-based learning : the system learns the examples by heart, then\n",
      "generalizes to new cases by comparing them to the learned examples (or a subset of\n",
      "them), using a similarity measure. For example, in Figure 1-15  the new instance\n",
      "would be classified as a triangle because the majority of the most similar instances\n",
      "belong to that class.\n",
      "18 | Chapter 1: The Machine Learning Landscape\n",
      "Figure 1-15. Instance-based learning\n",
      "Model-based learning\n",
      "Another way to generalize from a set of examples is to build a model of these exam‐\n",
      "ples, then use that model to make predictions . This is called model-based learning\n",
      "(Figure 1-16 ).\n",
      "Figure 1-16. Model-based learning\n",
      "For example, suppose you want to know if money makes people happy, so you down‐\n",
      "load the Better Life Index  data from the OECD’s website  as well as stats about GDP\n",
      "per capita from the IMF’s website . Then you join the tables and sort by GDP per cap‐\n",
      "ita. Table 1-1  shows an excerpt of what you get.\n",
      "Types of Machine Learning Systems | 19\n",
      "5By convention, the Greek letter θ (theta) is frequently used to represent model parameters.Table 1-1. Does money make people happier?\n",
      "Country GDP per capita (USD) Life satisfaction\n",
      "Hungary 12,240 4.9\n",
      "Korea 27,195 5.8\n",
      "France 37,675 6.5\n",
      "Australia 50,962 7.3\n",
      "United States 55,805 7.2\n",
      "Let’s plot the data for a few random countries ( Figure 1-17 ).\n",
      "Figure 1-17. Do you see a trend here?\n",
      "There does seem to be a trend here! Although the data is noisy  (i.e., partly random), it\n",
      "looks like life satisfaction goes up more or less linearly as the country’s GDP per cap‐\n",
      "ita increases. So you decide to model life satisfaction as a linear function of GDP per\n",
      "capita. This step is called model selection : you selected a linear model  of life satisfac‐\n",
      "tion with just one attribute, GDP per capita ( Equation 1-1 ).\n",
      "Equation 1-1. A simple linear model\n",
      "life_satisfaction = θ0+θ1× GDP_per_capita\n",
      "This model has two model parameters , θ0 and θ1.5 By tweaking these parameters, you\n",
      "can make your model represent any linear function, as shown in Figure 1-18 .\n",
      "20 | Chapter 1: The Machine Learning Landscape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 1-18. A few possible linear models\n",
      "Before you can use your model, you need to define the parameter values θ0 and θ1.\n",
      "How can you know which values will make your model perform best? To answer this\n",
      "question, you need to specify a performance measure. Y ou can either define a utility\n",
      "function  (or fitness  function ) that measures how good  your model is, or you can define\n",
      "a cost function  that measures how bad it is. For linear regression problems, people\n",
      "typically use a cost function that measures the distance between the linear model’s\n",
      "predictions and the training examples; the objective is to minimize this distance.\n",
      "This is where the Linear Regression algorithm comes in: you feed it your training\n",
      "examples and it finds the parameters that make the linear model fit best to your data.\n",
      "This is called training  the model. In our case the algorithm finds that the optimal\n",
      "parameter values are θ0 = 4.85 and θ1 = 4.91 × 10–5.\n",
      "Now the model fits the training data as closely as possible (for a linear model), as you\n",
      "can see in Figure 1-19 .\n",
      "Figure 1-19. The linear model that fits the training data best\n",
      "Types of Machine Learning Systems | 21\n",
      "6The prepare_country_stats()  function’s definition is not shown here (see this chapter’s Jupyter notebook if\n",
      "you want all the gory details). It’s just boring Pandas code that joins the life satisfaction data from the OECD\n",
      "with the GDP per capita data from the IMF.\n",
      "7It’s okay if you don’t understand all the code yet; we will present Scikit-Learn in the following chapters.Y ou are finally ready to run the model to make predictions. For example, say you\n",
      "want to know how happy Cypriots are, and the OECD data does not have the answer.\n",
      "Fortunately, you can use your model to make a good prediction: you look up Cyprus’s\n",
      "GDP per capita, find $22,587, and then apply your model and find that life satisfac‐\n",
      "tion is likely to be somewhere around 4.85 + 22,587 × 4.91 × 10-5 = 5.96.\n",
      "To whet your appetite, Example 1-1  shows the Python code that loads the data, pre‐\n",
      "pares it,6 creates a scatterplot for visualization, and then trains a linear model and\n",
      "makes a prediction.7\n",
      "Example 1-1. Training and running a linear model using Scikit-Learn\n",
      "import matplotlib.pyplot  as plt\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import sklearn.linear_model\n",
      "# Load the data\n",
      "oecd_bli  = pd.read_csv (\"oecd_bli_2015.csv\" , thousands =',')\n",
      "gdp_per_capita  = pd.read_csv (\"gdp_per_capita.csv\" ,thousands =',',delimiter ='\\t',\n",
      "                             encoding ='latin1' , na_values =\"n/a\")\n",
      "# Prepare the data\n",
      "country_stats  = prepare_country_stats (oecd_bli , gdp_per_capita )\n",
      "X = np.c_[country_stats [\"GDP per capita\" ]]\n",
      "y = np.c_[country_stats [\"Life satisfaction\" ]]\n",
      "# Visualize the data\n",
      "country_stats .plot(kind='scatter' , x=\"GDP per capita\" , y='Life satisfaction' )\n",
      "plt.show()\n",
      "# Select a linear model\n",
      "model = sklearn.linear_model .LinearRegression ()\n",
      "# Train the model\n",
      "model.fit(X, y)\n",
      "# Make a prediction for Cyprus\n",
      "X_new = [[22587]]  # Cyprus' GDP per capita\n",
      "print(model.predict(X_new)) # outputs [[ 5.96242338]]\n",
      "22 | Chapter 1: The Machine Learning Landscape\n",
      "If you had used an instance-based learning algorithm instead, you\n",
      "would have found that Slovenia has the closest GDP per capita to\n",
      "that of Cyprus ($20,732), and since the OECD data tells us that\n",
      "Slovenians’ life satisfaction is 5.7, you would have predicted a life\n",
      "satisfaction of 5.7 for Cyprus. If you zoom out a bit and look at the\n",
      "two next closest countries, you will find Portugal and Spain with\n",
      "life satisfactions of 5.1 and 6.5, respectively. Averaging these three\n",
      "values, you get 5.77, which is pretty close to your model-based pre‐\n",
      "diction. This simple algorithm is called k-Nearest Neighbors  regres‐\n",
      "sion (in this example, k = 3).\n",
      "Replacing the Linear Regression model with k-Nearest Neighbors\n",
      "regression in the previous code is as simple as replacing these two\n",
      "lines:\n",
      "import sklearn.linear_model\n",
      "model = sklearn.linear_model .LinearRegression ()\n",
      "with these two:\n",
      "import sklearn.neighbors\n",
      "model = sklearn.neighbors .KNeighborsRegressor (n_neighbors =3)\n",
      "If all went well, your model will make good predictions. If not, you may need to use\n",
      "more attributes (employment rate, health, air pollution, etc.), get more or better qual‐\n",
      "ity training data, or perhaps select a more powerful model (e.g., a Polynomial Regres‐\n",
      "sion model).\n",
      "In summary:\n",
      "•Y ou studied the data.\n",
      "•Y ou selected a model.\n",
      "•Y ou trained it on the training data (i.e., the learning algorithm searched for the\n",
      "model parameter values that minimize a cost function).\n",
      "•Finally, you applied the model to make predictions on new cases (this is called\n",
      "inference ), hoping that this model will generalize well.\n",
      "This is what a typical Machine Learning project looks like. In Chapter 2  you will\n",
      "experience this first-hand by going through an end-to-end project.\n",
      "We have covered a lot of ground so far: you now know what Machine Learning is\n",
      "really about, why it is useful, what some of the most common categories of ML sys‐\n",
      "tems are, and what a typical project workflow looks like. Now let’s look at what can go\n",
      "wrong in learning and prevent you from making accurate predictions.\n",
      "Types of Machine Learning Systems | 23\n",
      "Main Challenges of Machine Learning\n",
      "In short, since your main task is to select a learning algorithm and train it on some\n",
      "data, the two things that can go wrong are “bad algorithm” and “bad data. ” Let’s start\n",
      "with examples of bad data.\n",
      "Insufficient  Quantity of Training Data\n",
      "For a toddler to learn what an apple is, all it takes is for you to point to an apple and\n",
      "say “apple” (possibly repeating this procedure a few times). Now the child is able to\n",
      "recognize apples in all sorts of colors and shapes. Genius.\n",
      "Machine Learning is not quite there yet; it takes a lot of data for most Machine Learn‐\n",
      "ing algorithms to work properly. Even for very simple problems you typically need\n",
      "thousands of examples, and for complex problems such as image or speech recogni‐\n",
      "tion you may need millions of examples (unless you can reuse parts of an existing\n",
      "model).\n",
      "24 | Chapter 1: The Machine Learning Landscape\n",
      "8For example, knowing whether to write “to, ” “two, ” or “too” depending on the context.\n",
      "9Figure reproduced with permission from Banko and Brill (2001), “Learning Curves for Confusion Set Disam‐\n",
      "biguation. ”\n",
      "10“The Unreasonable Effectiveness of Data, ” Peter Norvig et al. (2009).The Unreasonable Effectiveness  of Data\n",
      "In a famous paper  published in 2001, Microsoft researchers Michele Banko and Eric\n",
      "Brill showed that very different Machine Learning algorithms, including fairly simple\n",
      "ones, performed almost identically well on a complex problem of natural language\n",
      "disambiguation8 once they were given enough data (as you can see in Figure 1-20 ).\n",
      "Figure 1-20. The importance of data versus algorithms9\n",
      "As the authors put it: “these results suggest that we may want to reconsider the trade-\n",
      "off between spending time and money on algorithm development versus spending it\n",
      "on corpus development. ”\n",
      "The idea that data matters more than algorithms for complex problems was further\n",
      "popularized by Peter Norvig et al. in a paper titled “The Unreasonable Effectiveness\n",
      "of Data”  published in 2009.10 It should be noted, however, that small- and medium-\n",
      "sized datasets are still very common, and it is not always easy or cheap to get extra\n",
      "training data, so don’t abandon algorithms just yet.\n",
      "Main Challenges of Machine Learning | 25\n",
      "Nonrepresentative Training Data\n",
      "In order to generalize well, it is crucial that your training data be representative of the\n",
      "new cases you want to generalize to. This is true whether you use instance-based\n",
      "learning or model-based learning.\n",
      "For example, the set of countries we used earlier for training the linear model was not\n",
      "perfectly representative; a few countries were missing. Figure 1-21  shows what the\n",
      "data looks like when you add the missing countries.\n",
      "Figure 1-21. A more representative training sample\n",
      "If you train a linear model on this data, you get the solid line, while the old model is\n",
      "represented by the dotted line. As you can see, not only does adding a few missing\n",
      "countries significantly alter the model, but it makes it clear that such a simple linear\n",
      "model is probably never going to work well. It seems that very rich countries are not\n",
      "happier than moderately rich countries (in fact they seem unhappier), and conversely\n",
      "some poor countries seem happier than many rich countries.\n",
      "By using a nonrepresentative training set, we trained a model that is unlikely to make\n",
      "accurate predictions, especially for very poor and very rich countries.\n",
      "It is crucial to use a training set that is representative of the cases you want to general‐\n",
      "ize to. This is often harder than it sounds: if the sample is too small, you will have\n",
      "sampling noise  (i.e., nonrepresentative data as a result of chance), but even very large\n",
      "samples can be nonrepresentative if the sampling method is flawed. This is called\n",
      "sampling bias .\n",
      "A Famous Example of Sampling Bias\n",
      "Perhaps the most famous example of sampling bias happened during the US presi‐\n",
      "dential election in 1936, which pitted Landon against Roosevelt: the Literary Digest\n",
      "conducted a very large poll, sending mail to about 10 million people. It got 2.4 million\n",
      "answers, and predicted with high confidence that Landon would get 57% of the votes.\n",
      "26 | Chapter 1: The Machine Learning Landscape\n",
      "Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest ’s\n",
      "sampling method:\n",
      "•First, to obtain the addresses to send the polls to, the Literary Digest  used tele‐\n",
      "phone directories, lists of magazine subscribers, club membership lists, and the\n",
      "like. All of these lists tend to favor wealthier people, who are more likely to vote\n",
      "Republican (hence Landon).\n",
      "•Second, less than 25% of the people who received the poll answered. Again, this\n",
      "introduces a sampling bias, by ruling out people who don’t care much about poli‐\n",
      "tics, people who don’t like the Literary Digest , and other key groups. This is a spe‐\n",
      "cial type of sampling bias called nonresponse bias .\n",
      "Here is another example: say you want to build a system to recognize funk music vid‐\n",
      "eos. One way to build your training set is to search “funk music” on Y ouTube and use\n",
      "the resulting videos. But this assumes that Y ouTube’s search engine returns a set of\n",
      "videos that are representative of all the funk music videos on Y ouTube. In reality, the\n",
      "search results are likely to be biased toward popular artists (and if you live in Brazil\n",
      "you will get a lot of “funk carioca” videos, which sound nothing like James Brown).\n",
      "On the other hand, how else can you get a large training set?\n",
      "Poor-Quality Data\n",
      "Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\n",
      "quality measurements), it will make it harder for the system to detect the underlying\n",
      "patterns, so your system is less likely to perform well. It is often well worth the effort\n",
      "to spend time cleaning up your training data. The truth is, most data scientists spend\n",
      "a significant part of their time doing just that. For example:\n",
      "•If some instances are clearly outliers, it may help to simply discard them or try to\n",
      "fix the errors manually.\n",
      "•If some instances are missing a few features (e.g., 5% of your customers did not\n",
      "specify their age), you must decide whether you want to ignore this attribute alto‐\n",
      "gether, ignore these instances, fill in the missing values (e.g., with the median\n",
      "age), or train one model with the feature and one model without it, and so on.\n",
      "Irrelevant Features\n",
      "As the saying goes: garbage in, garbage out. Y our system will only be capable of learn‐\n",
      "ing if the training data contains enough relevant features and not too many irrelevant\n",
      "ones. A critical part of the success of a Machine Learning project is coming up with a\n",
      "good set of features to train on. This process, called feature engineering , involves:\n",
      "Main Challenges of Machine Learning | 27\n",
      "•Feature selection : selecting the most useful features to train on among existing\n",
      "features.\n",
      "•Feature extraction : combining existing features to produce a more useful one (as\n",
      "we saw earlier, dimensionality reduction algorithms can help).\n",
      "•Creating new features by gathering new data.\n",
      "Now that we have looked at many examples of bad data, let’s look at a couple of exam‐\n",
      "ples of bad algorithms.\n",
      "Overfitting  the Training Data\n",
      "Say you are visiting a foreign country and the taxi driver rips you off. Y ou might be\n",
      "tempted to say that all taxi drivers in that country are thieves. Overgeneralizing is\n",
      "something that we humans do all too often, and unfortunately machines can fall into\n",
      "the same trap if we are not careful. In Machine Learning this is called overfitting : it\n",
      "means that the model performs well on the training data, but it does not generalize\n",
      "well.\n",
      "Figure 1-22  shows an example of a high-degree polynomial life satisfaction model\n",
      "that strongly overfits the training data. Even though it performs much better on the\n",
      "training data than the simple linear model, would you really trust its predictions?\n",
      "Figure 1-22. Overfitting  the training data\n",
      "Complex models such as deep neural networks can detect subtle patterns in the data,\n",
      "but if the training set is noisy, or if it is too small (which introduces sampling noise),\n",
      "then the model is likely to detect patterns in the noise itself. Obviously these patterns\n",
      "will not generalize to new instances. For example, say you feed your life satisfaction\n",
      "model many more attributes, including uninformative ones such as the country’s\n",
      "name. In that case, a complex model may detect patterns like the fact that all coun‐\n",
      "tries in the training data with a w in their name have a life satisfaction greater than 7:\n",
      "New Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident\n",
      "28 | Chapter 1: The Machine Learning Landscape\n",
      "are you that the W-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously\n",
      "this pattern occurred in the training data by pure chance, but the model has no way\n",
      "to tell whether a pattern is real or simply the result of noise in the data.\n",
      "Overfitting happens when the model is too complex relative to the\n",
      "amount and noisiness of the training data. The possible solutions\n",
      "are:\n",
      "•To simplify the model by selecting one with fewer parameters\n",
      "(e.g., a linear model rather than a high-degree polynomial\n",
      "model), by reducing the number of attributes in the training\n",
      "data or by constraining the model\n",
      "•To gather more training data\n",
      "•To reduce the noise in the training data (e.g., fix data errors\n",
      "and remove outliers)\n",
      "Constraining a model to make it simpler and reduce the risk of overfitting is called\n",
      "regularization . For example, the linear model we defined earlier has two parameters,\n",
      "θ0 and θ1. This gives the learning algorithm two degrees of freedom  to adapt the model\n",
      "to the training data: it can tweak both the height ( θ0) and the slope ( θ1) of the line. If\n",
      "we forced θ1 = 0, the algorithm would have only one degree of freedom and would\n",
      "have a much harder time fitting the data properly: all it could do is move the line up\n",
      "or down to get as close as possible to the training instances, so it would end up\n",
      "around the mean. A very simple model indeed! If we allow the algorithm to modify θ1\n",
      "but we force it to keep it small, then the learning algorithm will effectively have some‐\n",
      "where in between one and two degrees of freedom. It will produce a simpler model\n",
      "than with two degrees of freedom, but more complex than with just one. Y ou want to\n",
      "find the right balance between fitting the training data perfectly and keeping the\n",
      "model simple enough to ensure that it will generalize well.\n",
      "Figure 1-23  shows three models: the dotted line represents the original model that\n",
      "was trained with a few countries missing, the dashed line is our second model trained\n",
      "with all countries, and the solid line is a linear model trained with the same data as\n",
      "the first model but with a regularization constraint. Y ou can see that regularization\n",
      "forced the model to have a smaller slope, which fits a bit less the training data that the\n",
      "model was trained on, but actually allows it to generalize better to new examples.\n",
      "Main Challenges of Machine Learning | 29\n",
      "Figure 1-23. Regularization reduces the risk of overfitting\n",
      "The amount of regularization to apply during learning can be controlled by a hyper‐\n",
      "parameter . A hyperparameter is a parameter of a learning algorithm (not of the\n",
      "model). As such, it is not affected by the learning algorithm itself; it must be set prior\n",
      "to training and remains constant during training. If you set the regularization hyper‐\n",
      "parameter to a very large value, you will get an almost flat model (a slope close to\n",
      "zero); the learning algorithm will almost certainly not overfit the training data, but it\n",
      "will be less likely to find a good solution. Tuning hyperparameters is an important\n",
      "part of building a Machine Learning system (you will see a detailed example in the\n",
      "next chapter).\n",
      "Underfitting  the Training Data\n",
      "As you might guess, underfitting  is the opposite of overfitting: it occurs when your\n",
      "model is too simple to learn the underlying structure of the data. For example, a lin‐\n",
      "ear model of life satisfaction is prone to underfit; reality is just more complex than\n",
      "the model, so its predictions are bound to be inaccurate, even on the training exam‐\n",
      "ples.\n",
      "The main options to fix this problem are:\n",
      "•Selecting a more powerful model, with more parameters\n",
      "•Feeding better features to the learning algorithm (feature engineering)\n",
      "•Reducing the constraints on the model (e.g., reducing the regularization hyper‐\n",
      "parameter)\n",
      "Stepping Back\n",
      "By now you already know a lot about Machine Learning. However, we went through\n",
      "so many concepts that you may be feeling a little lost, so let’s step back and look at the\n",
      "big picture:\n",
      "30 | Chapter 1: The Machine Learning Landscape\n",
      "•Machine Learning is about making machines get better at some task by learning\n",
      "from data, instead of having to explicitly code rules.\n",
      "•There are many different types of ML systems: supervised or not, batch or online,\n",
      "instance-based or model-based, and so on.\n",
      "•In a ML project you gather data in a training set, and you feed the training set to\n",
      "a learning algorithm. If the algorithm is model-based it tunes some parameters to\n",
      "fit the model to the training set (i.e., to make good predictions on the training set\n",
      "itself), and then hopefully it will be able to make good predictions on new cases\n",
      "as well. If the algorithm is instance-based, it just learns the examples by heart and\n",
      "generalizes to new instances by comparing them to the learned instances using a\n",
      "similarity measure.\n",
      "•The system will not perform well if your training set is too small, or if the data is\n",
      "not representative, noisy, or polluted with irrelevant features (garbage in, garbage\n",
      "out). Lastly, your model needs to be neither too simple (in which case it will\n",
      "underfit) nor too complex (in which case it will overfit).\n",
      "There’s just one last important topic to cover: once you have trained a model, you\n",
      "don’t want to just “hope” it generalizes to new cases. Y ou want to evaluate it, and fine-\n",
      "tune it if necessary. Let’s see how.\n",
      "Testing and Validating\n",
      "The only way to know how well a model will generalize to new cases is to actually try\n",
      "it out on new cases. One way to do that is to put your model in production and moni‐\n",
      "tor how well it performs. This works well, but if your model is horribly bad, your\n",
      "users will complain—not the best idea.\n",
      "A better option is to split your data into two sets: the training set  and the test set . As\n",
      "these names imply, you train your model using the training set, and you test it using\n",
      "the test set. The error rate on new cases is called the generalization error  (or out-of-\n",
      "sample error ), and by evaluating your model on the test set, you get an estimate of this\n",
      "error. This value tells you how well your model will perform on instances it has never\n",
      "seen before.\n",
      "If the training error is low (i.e., your model makes few mistakes on the training set)\n",
      "but the generalization error is high, it means that your model is overfitting the train‐\n",
      "ing data.\n",
      "It is common to use 80% of the data for training and hold out  20%\n",
      "for testing. However, this depends on the size of the dataset: if it\n",
      "contains 10 million instances, then holding out 1% means your test\n",
      "set will contain 100,000 instances: that’s probably more than\n",
      "enough to get a good estimate of the generalization error.\n",
      "Testing and Validating | 31\n",
      "Hyperparameter Tuning and Model Selection\n",
      "So evaluating a model is simple enough: just use a test set. Now suppose you are hesi‐\n",
      "tating between two models (say a linear model and a polynomial model): how can\n",
      "you decide? One option is to train both and compare how well they generalize using\n",
      "the test set.\n",
      "Now suppose that the linear model generalizes better, but you want to apply some \n",
      "regularization to avoid overfitting. The question is: how do you choose the value of\n",
      "the regularization hyperparameter? One option is to train 100 different models using\n",
      "100 different values for this hyperparameter. Suppose you find the best hyperparame‐\n",
      "ter value that produces a model with the lowest generalization error, say just 5% error.\n",
      "So you launch this model into production, but unfortunately it does not perform as\n",
      "well as expected and produces 15% errors. What just happened?\n",
      "The problem is that you measured the generalization error multiple times on the test\n",
      "set, and you adapted the model and hyperparameters to produce the best model for\n",
      "that particular set . This means that the model is unlikely to perform as well on new\n",
      "data.\n",
      "A common solution to this problem is called holdout validation : you simply hold out\n",
      "part of the training set to evaluate several candidate models and select the best one.\n",
      "The new heldout set is called the validation set  (or sometimes the development set , or\n",
      "dev set ). More specifically, you train multiple models with various hyperparameters\n",
      "on the reduced training set (i.e., the full training set minus the validation set), and\n",
      "you select the model that performs best on the validation set. After this holdout vali‐\n",
      "dation process, you train the best model on the full training set (including the valida‐\n",
      "tion set), and this gives you the final model. Lastly, you evaluate this final model on\n",
      "the test set to get an estimate of the generalization error.\n",
      "This solution usually works quite well. However, if the validation set is too small, then\n",
      "model evaluations will be imprecise: you may end up selecting a suboptimal model by\n",
      "mistake. Conversely, if the validation set is too large, then the remaining training set\n",
      "will be much smaller than the full training set. Why is this bad? Well, since the final\n",
      "model will be trained on the full training set, it is not ideal to compare candidate\n",
      "models trained on a much smaller training set. It would be like selecting the fastest\n",
      "sprinter to participate in a marathon. One way to solve this problem is to perform\n",
      "repeated cross-validation , using many small validation sets. Each model is evaluated\n",
      "once per validation set, after it is trained on the rest of the data. By averaging out all\n",
      "the evaluations of a model, we get a much more accurate measure of its performance.\n",
      "However, there is a drawback: the training time is multiplied by the number of valida‐\n",
      "tion sets.\n",
      "32 | Chapter 1: The Machine Learning Landscape\n",
      "11“The Lack of A Priori Distinctions Between Learning Algorithms, ” D. Wolpert (1996).Data Mismatch\n",
      "In some cases, it is easy to get a large amount of data for training, but it is not per‐\n",
      "fectly representative of the data that will be used in production. For example, suppose\n",
      "you want to create a mobile app to take pictures of flowers and automatically deter‐\n",
      "mine their species. Y ou can easily download millions of pictures of flowers on the\n",
      "web, but they won’t be perfectly representative of the pictures that will actually be\n",
      "taken using the app on a mobile device. Perhaps you only have 10,000 representative\n",
      "pictures (i.e., actually taken with the app). In this case, the most important rule to\n",
      "remember is that the validation set and the test must be as representative as possible\n",
      "of the data you expect to use in production, so they should be composed exclusively\n",
      "of representative pictures: you can shuffle them and put half in the validation set, and\n",
      "half in the test set (making sure that no duplicates or near-duplicates end up in both\n",
      "sets). After training your model on the web pictures, if you observe that the perfor‐\n",
      "mance of your model on the validation set is disappointing, you will not know\n",
      "whether this is because your model has overfit the training set, or whether this is just\n",
      "due to the mismatch between the web pictures and the mobile app pictures. One sol‐\n",
      "ution is to hold out part of the training pictures (from the web) in yet another set that\n",
      "Andrew Ng calls the train-dev set . After the model is trained (on the training set, not\n",
      "on the train-dev set), you can evaluate it on the train-dev set: if it performs well, then\n",
      "the model is not overfitting the training set, so if performs poorly on the validation\n",
      "set, the problem must come from the data mismatch. Y ou can try to tackle this prob‐\n",
      "lem by preprocessing the web images to make them look more like the pictures that\n",
      "will be taken by the mobile app, and then retraining the model. Conversely, if the\n",
      "model performs poorly on the train-dev set, then the model must have overfit the\n",
      "training set, so you should try to simplify or regularize the model, get more training\n",
      "data and clean up the training data, as discussed earlier.\n",
      "No Free Lunch Theorem\n",
      "A model is a simplified version of the observations. The simplifications are meant to\n",
      "discard the superfluous details that are unlikely to generalize to new instances. How‐\n",
      "ever, to decide what data to discard and what data to keep, you must make assump‐\n",
      "tions . For example, a linear model makes the assumption that the data is\n",
      "fundamentally linear and that the distance between the instances and the straight line\n",
      "is just noise, which can safely be ignored.\n",
      "In a famous 1996 paper ,11 David Wolpert demonstrated that if you make absolutely\n",
      "no assumption about the data, then there is no reason to prefer one model over any\n",
      "other. This is called the No Free Lunch  (NFL) theorem. For some datasets the best\n",
      "Testing and Validating | 33\n",
      "model is a linear model, while for other datasets it is a neural network. There is no\n",
      "model that is a priori  guaranteed to work better (hence the name of the theorem). The\n",
      "only way to know for sure which model is best is to evaluate them all. Since this is not\n",
      "possible, in practice you make some reasonable assumptions about the data and you\n",
      "evaluate only a few reasonable models. For example, for simple tasks you may evalu‐\n",
      "ate linear models with various levels of regularization, and for a complex problem you\n",
      "may evaluate various neural networks.\n",
      "Exercises\n",
      "In this chapter we have covered some of the most important concepts in Machine\n",
      "Learning. In the next chapters we will dive deeper and write more code, but before we\n",
      "do, make sure you know how to answer the following questions:\n",
      "1.How would you define Machine Learning?\n",
      "2.Can you name four types of problems where it shines?\n",
      "3.What is a labeled training set?\n",
      "4.What are the two most common supervised tasks?\n",
      "5.Can you name four common unsupervised tasks?\n",
      "6.What type of Machine Learning algorithm would you use to allow a robot to\n",
      "walk in various unknown terrains?\n",
      "7.What type of algorithm would you use to segment your customers into multiple\n",
      "groups?\n",
      "8.Would you frame the problem of spam detection as a supervised learning prob‐\n",
      "lem or an unsupervised learning problem?\n",
      "9.What is an online learning system?\n",
      "10.What is out-of-core learning?\n",
      "11.What type of learning algorithm relies on a similarity measure to make predic‐\n",
      "tions?\n",
      "12.What is the difference between a model parameter and a learning algorithm’s\n",
      "hyperparameter?\n",
      "13.What do model-based learning algorithms search for? What is the most common\n",
      "strategy they use to succeed? How do they make predictions?\n",
      "14.Can you name four of the main challenges in Machine Learning?\n",
      "15.If your model performs great on the training data but generalizes poorly to new\n",
      "instances, what is happening? Can you name three possible solutions?\n",
      "16.What is a test set and why would you want to use it?\n",
      "34 | Chapter 1: The Machine Learning Landscape\n",
      "17.What is the purpose of a validation set?\n",
      "18.What can go wrong if you tune hyperparameters using the test set?\n",
      "19.What is repeated cross-validation and why would you prefer it to using a single\n",
      "validation set?\n",
      "Solutions to these exercises are available in ???.\n",
      "Exercises | 35\n",
      "\n",
      "1The example project is completely fictitious; the goal is just to illustrate the main steps of a Machine Learning\n",
      "project, not to learn anything about the real estate business.\n",
      "CHAPTER 2\n",
      "End-to-End Machine Learning Project\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 2 in the final\n",
      "release of the book.\n",
      "In this chapter, you will go through an example project end to end, pretending to be a\n",
      "recently hired data scientist in a real estate company.1 Here are the main steps you will\n",
      "go through:\n",
      "1.Look at the big picture.\n",
      "2.Get the data.\n",
      "3.Discover and visualize the data to gain insights.\n",
      "4.Prepare the data for Machine Learning algorithms.\n",
      "5.Select a model and train it.\n",
      "6.Fine-tune your model.\n",
      "7.Present your solution.\n",
      "8.Launch, monitor, and maintain your system.\n",
      "37\n",
      "2The original dataset appeared in R. Kelley Pace and Ronald Barry, “Sparse Spatial Autoregressions, ” Statistics\n",
      "& Probability Letters  33, no. 3 (1997): 291–297.Working with Real Data\n",
      "When you are learning about Machine Learning it is best to actually experiment with\n",
      "real-world data, not just artificial datasets. Fortunately, there are thousands of open\n",
      "datasets to choose from, ranging across all sorts of domains. Here are a few places\n",
      "you can look to get data:\n",
      "•Popular open data repositories:\n",
      "—UC Irvine Machine Learning Repository\n",
      "—Kaggle datasets\n",
      "—Amazon’s AWS datasets\n",
      "•Meta portals (they list open data repositories):\n",
      "—http://dataportals.org/\n",
      "—http://opendatamonitor.eu/\n",
      "—http://quandl.com/\n",
      "•Other pages listing many popular open data repositories:\n",
      "—Wikipedia’s list of Machine Learning datasets\n",
      "—Quora.com question\n",
      "—Datasets subreddit\n",
      "In this chapter we chose the California Housing Prices dataset from the StatLib repos‐\n",
      "itory2 (see Figure 2-1 ). This dataset was based on data from the 1990 California cen‐\n",
      "sus. It is not exactly recent (you could still afford a nice house in the Bay Area at the\n",
      "time), but it has many qualities for learning, so we will pretend it is recent data. We\n",
      "also added a categorical attribute and removed a few features for teaching purposes.\n",
      "38 | Chapter 2: End-to-End Machine Learning Project\n",
      "Figure 2-1. California housing prices\n",
      "Look at the Big Picture\n",
      "Welcome to Machine Learning Housing Corporation! The first task you are asked to\n",
      "perform is to build a model of housing prices in California using the California cen‐\n",
      "sus data. This data has metrics such as the population, median income, median hous‐\n",
      "ing price, and so on for each block group in California. Block groups are the smallest\n",
      "geographical unit for which the US Census Bureau publishes sample data (a block\n",
      "group typically has a population of 600 to 3,000 people). We will just call them “dis‐\n",
      "tricts” for short.\n",
      "Y our model should learn from this data and be able to predict the median housing\n",
      "price in any district, given all the other metrics.\n",
      "Since you are a well-organized data scientist, the first thing you do\n",
      "is to pull out your Machine Learning project checklist. Y ou can\n",
      "start with the one in ???; it should work reasonably well for most\n",
      "Machine Learning projects but make sure to adapt it to your needs.\n",
      "In this chapter we will go through many checklist items, but we will\n",
      "also skip a few, either because they are self-explanatory or because\n",
      "they will be discussed in later chapters.\n",
      "Frame the Problem\n",
      "The first question to ask your boss is what exactly is the business objective; building a\n",
      "model is probably not the end goal. How does the company expect to use and benefit\n",
      "Look at the Big Picture | 39\n",
      "3A piece of information fed to a Machine Learning system is often called a signal  in reference to Shannon’s\n",
      "information theory: you want a high signal/noise ratio.from this model? This is important because it will determine how you frame the\n",
      "problem, what algorithms you will select, what performance measure you will use to\n",
      "evaluate your model, and how much effort you should spend tweaking it.\n",
      "Y our boss answers that your model’s output (a prediction of a district’s median hous‐\n",
      "ing price) will be fed to another Machine Learning system (see Figure 2-2 ), along\n",
      "with many other signals .3 This downstream system will determine whether it is worth\n",
      "investing in a given area or not. Getting this right is critical, as it directly affects reve‐\n",
      "nue.\n",
      "Figure 2-2. A Machine Learning pipeline for real estate investments\n",
      "Pipelines\n",
      "A sequence of data processing components  is called a data pipeline . Pipelines are very\n",
      "common in Machine Learning systems, since there is a lot of data to manipulate and\n",
      "many data transformations to apply.\n",
      "Components typically run asynchronously. Each component pulls in a large amount\n",
      "of data, processes it, and spits out the result in another data store, and then some time\n",
      "later the next component in the pipeline pulls this data and spits out its own output,\n",
      "and so on. Each component is fairly self-contained: the interface between components\n",
      "is simply the data store. This makes the system quite simple to grasp (with the help of\n",
      "a data flow graph), and different teams can focus on different components. Moreover,\n",
      "if a component breaks down, the downstream components can often continue to run\n",
      "normally (at least for a while) by just using the last output from the broken compo‐\n",
      "nent. This makes the architecture quite robust.\n",
      "40 | Chapter 2: End-to-End Machine Learning Project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13In this book, when a code example contains a mix of code and outputs, as is the case here, it is formatted like\n",
      "in the Python interpreter, for better readability: the code lines are prefixed with >>> (or ... for indented\n",
      "blocks), and the outputs have no prefix.\n",
      "14Y ou will often see people set the random seed to 42. This number has no special property, other than to be\n",
      "The Answer to the Ultimate Question of Life, the Universe, and Everything.import numpy as np\n",
      "def split_train_test (data, test_ratio ):\n",
      "    shuffled_indices  = np.random.permutation (len(data))\n",
      "    test_set_size  = int(len(data) * test_ratio )\n",
      "    test_indices  = shuffled_indices [:test_set_size ]\n",
      "    train_indices  = shuffled_indices [test_set_size :]\n",
      "    return data.iloc[train_indices ], data.iloc[test_indices ]\n",
      "Y ou can then use this function like this:13\n",
      ">>> train_set , test_set  = split_train_test (housing, 0.2)\n",
      ">>> len(train_set )\n",
      "16512\n",
      ">>> len(test_set )\n",
      "4128\n",
      "Well, this works, but it is not perfect: if you run the program again, it will generate a\n",
      "different test set! Over time, you (or your Machine Learning algorithms) will get to\n",
      "see the whole dataset, which is what you want to avoid.\n",
      "One solution is to save the test set on the first run and then load it in subsequent\n",
      "runs. Another option is to set the random number generator’s seed (e.g., np.ran\n",
      "dom.seed(42) )14 before calling np.random.permutation() , so that it always generates\n",
      "the same shuffled indices.\n",
      "But both these solutions will break next time you fetch an updated dataset. A com‐\n",
      "mon solution is to use each instance’s identifier to decide whether or not it should go\n",
      "in the test set (assuming instances have a unique and immutable identifier). For\n",
      "example, you could compute a hash of each instance’s identifier and put that instance\n",
      "in the test set if the hash is lower or equal to 20% of the maximum hash value. This\n",
      "ensures that the test set will remain consistent across multiple runs, even if you\n",
      "refresh the dataset. The new test set will contain 20% of the new instances, but it will\n",
      "not contain any instance that was previously in the training set. Here is a possible\n",
      "implementation:\n",
      "from zlib import crc32\n",
      "def test_set_check (identifier , test_ratio ):\n",
      "    return crc32(np.int64(identifier )) & 0xffffffff  < test_ratio  * 2**32\n",
      "def split_train_test_by_id (data, test_ratio , id_column ):\n",
      "    ids = data[id_column ]\n",
      "Get the Data | 55\n",
      "15The location information is actually quite coarse, and as a result many districts will have the exact same ID, so\n",
      "they will end up in the same set (test or train). This introduces some unfortunate sampling bias.    in_test_set  = ids.apply(lambda id_: test_set_check (id_, test_ratio ))\n",
      "    return data.loc[~in_test_set ], data.loc[in_test_set ]\n",
      "Unfortunately, the housing dataset does not have an identifier column. The simplest\n",
      "solution is to use the row index as the ID:\n",
      "housing_with_id  = housing.reset_index ()   # adds an `index` column\n",
      "train_set , test_set  = split_train_test_by_id (housing_with_id , 0.2, \"index\")\n",
      "If you use the row index as a unique identifier, you need to make sure that new data\n",
      "gets appended to the end of the dataset, and no row ever gets deleted. If this is not\n",
      "possible, then you can try to use the most stable features to build a unique identifier.\n",
      "For example, a district’s latitude and longitude are guaranteed to be stable for a few\n",
      "million years, so you could combine them into an ID like so:15\n",
      "housing_with_id [\"id\"] = housing[\"longitude\" ] * 1000 + housing[\"latitude\" ]\n",
      "train_set , test_set  = split_train_test_by_id (housing_with_id , 0.2, \"id\")\n",
      "Scikit-Learn provides a few functions to split datasets into multiple subsets in various\n",
      "ways. The simplest function is train_test_split , which does pretty much the same\n",
      "thing as the function split_train_test  defined earlier, with a couple of additional\n",
      "features. First there is a random_state  parameter that allows you to set the random\n",
      "generator seed as explained previously, and second you can pass it multiple datasets\n",
      "with an identical number of rows, and it will split them on the same indices (this is\n",
      "very useful, for example, if you have a separate DataFrame for labels):\n",
      "from sklearn.model_selection  import train_test_split\n",
      "train_set , test_set  = train_test_split (housing, test_size =0.2, random_state =42)\n",
      "So far we have considered purely random sampling methods. This is generally fine if\n",
      "your dataset is large enough (especially relative to the number of attributes), but if it\n",
      "is not, you run the risk of introducing a significant sampling bias. When a survey\n",
      "company decides to call 1,000 people to ask them a few questions, they don’t just pick\n",
      "1,000 people randomly in a phone book. They try to ensure that these 1,000 people\n",
      "are representative of the whole population. For example, the US population is com‐\n",
      "posed of 51.3% female and 48.7% male, so a well-conducted survey in the US would\n",
      "try to maintain this ratio in the sample: 513 female and 487 male. This is called strati‐\n",
      "fied sampling : the population is divided into homogeneous subgroups called strata ,\n",
      "and the right number of instances is sampled from each stratum to guarantee that the\n",
      "test set is representative of the overall population. If they used purely random sam‐\n",
      "pling, there would be about 12% chance of sampling a skewed test set with either less\n",
      "than 49% female or more than 54% female. Either way, the survey results would be\n",
      "significantly biased.\n",
      "56 | Chapter 2: End-to-End Machine Learning Project\n",
      "Suppose you chatted with experts who told you that the median income is a very\n",
      "important attribute to predict median housing prices. Y ou may want to ensure that\n",
      "the test set is representative of the various categories of incomes in the whole dataset.\n",
      "Since the median income is a continuous numerical attribute, you first need to create\n",
      "an income category attribute. Let’s look at the median income histogram more closely\n",
      "(back in Figure 2-8 ): most median income values are clustered around 1.5 to 6 (i.e.,\n",
      "$15,000–$60,000), but some median incomes go far beyond 6. It is important to have\n",
      "a sufficient number of instances in your dataset for each stratum, or else the estimate\n",
      "of the stratum’s importance may be biased. This means that you should not have too\n",
      "many strata, and each stratum should be large enough. The following code uses the\n",
      "pd.cut()  function to create an income category attribute with 5 categories (labeled\n",
      "from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from\n",
      "1.5 to 3, and so on:\n",
      "housing[\"income_cat\" ] = pd.cut(housing[\"median_income\" ],\n",
      "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
      "                               labels=[1, 2, 3, 4, 5])\n",
      "These income categories are represented in Figure 2-9 :\n",
      "housing[\"income_cat\" ].hist()\n",
      "Figure 2-9. Histogram of income categories\n",
      "Now you are ready to do stratified sampling based on the income category. For this\n",
      "you can use Scikit-Learn’s StratifiedShuffleSplit  class:\n",
      "from sklearn.model_selection  import StratifiedShuffleSplit\n",
      "split = StratifiedShuffleSplit (n_splits =1, test_size =0.2, random_state =42)\n",
      "for train_index , test_index  in split.split(housing, housing[\"income_cat\" ]):\n",
      "    strat_train_set  = housing.loc[train_index ]\n",
      "    strat_test_set  = housing.loc[test_index ]\n",
      "Get the Data | 57\n",
      "Let’s see if this worked as expected. Y ou can start by looking at the income category\n",
      "proportions in the test set:\n",
      ">>> strat_test_set [\"income_cat\" ].value_counts () / len(strat_test_set )\n",
      "3    0.350533\n",
      "2    0.318798\n",
      "4    0.176357\n",
      "5    0.114583\n",
      "1    0.039729\n",
      "Name: income_cat, dtype: float64\n",
      "With similar code you can measure the income category proportions in the full data‐\n",
      "set. Figure 2-10  compares the income category proportions in the overall dataset, in\n",
      "the test set generated with stratified sampling, and in a test set generated using purely\n",
      "random sampling. As you can see, the test set generated using stratified sampling has\n",
      "income category proportions almost identical to those in the full dataset, whereas the\n",
      "test set generated using purely random sampling is quite skewed.\n",
      "Figure 2-10. Sampling bias comparison of stratified  versus purely random sampling\n",
      "Now you should remove the income_cat  attribute so the data is back to its original\n",
      "state:\n",
      "for set_ in (strat_train_set , strat_test_set ):\n",
      "    set_.drop(\"income_cat\" , axis=1, inplace=True)\n",
      "We spent quite a bit of time on test set generation for a good reason: this is an often\n",
      "neglected but critical part of a Machine Learning project. Moreover, many of these\n",
      "ideas will be useful later when we discuss cross-validation. Now it’s time to move on\n",
      "to the next stage: exploring the data.\n",
      "Discover and Visualize the Data to Gain Insights\n",
      "So far you have only taken a quick glance at the data to get a general understanding of\n",
      "the kind of data you are manipulating. Now the goal is to go a little bit more in depth.\n",
      "First, make sure you have put the test set aside and you are only exploring the train‐\n",
      "ing set. Also, if the training set is very large, you may want to sample an exploration\n",
      "58 | Chapter 2: End-to-End Machine Learning Project\n",
      "set, to make manipulations easy and fast. In our case, the set is quite small so you can\n",
      "just work directly on the full set. Let’s create a copy so you can play with it without\n",
      "harming the training set:\n",
      "housing = strat_train_set .copy()\n",
      "Visualizing Geographical Data\n",
      "Since there is geographical information (latitude and longitude), it is a good idea to\n",
      "create a scatterplot of all districts to visualize the data ( Figure 2-11 ):\n",
      "housing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" )\n",
      "Figure 2-11. A geographical scatterplot of the data\n",
      "This looks like California all right, but other than that it is hard to see any particular\n",
      "pattern. Setting the alpha  option to 0.1 makes it much easier to visualize the places\n",
      "where there is a high density of data points ( Figure 2-12 ):\n",
      "housing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" , alpha=0.1)\n",
      "Discover and Visualize the Data to Gain Insights | 59\n",
      "16If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from the Bay Area\n",
      "down to San Diego (as you might expect). Y ou can add a patch of yellow around Sacramento as well.\n",
      "Figure 2-12. A better visualization highlighting high-density areas\n",
      "Now that’s much better: you can clearly see the high-density areas, namely the Bay\n",
      "Area and around Los Angeles and San Diego, plus a long line of fairly high density in\n",
      "the Central Valley, in particular around Sacramento and Fresno.\n",
      "More generally, our brains are very good at spotting patterns on pictures, but you\n",
      "may need to play around with visualization parameters to make the patterns stand\n",
      "out.\n",
      "Now let’s look at the housing prices ( Figure 2-13 ). The radius of each circle represents\n",
      "the district’s population (option s), and the color represents the price (option c). We\n",
      "will use a predefined color map (option cmap ) called jet, which ranges from blue\n",
      "(low values) to red (high prices):16\n",
      "housing.plot(kind=\"scatter\" , x=\"longitude\" , y=\"latitude\" , alpha=0.4,\n",
      "    s=housing[\"population\" ]/100, label=\"population\" , figsize=(10,7),\n",
      "    c=\"median_house_value\" , cmap=plt.get_cmap (\"jet\"), colorbar =True,\n",
      ")\n",
      "plt.legend()\n",
      "60 | Chapter 2: End-to-End Machine Learning Project\n",
      "Figure 2-13. California housing prices\n",
      "Discover and Visualize the Data to Gain Insights | 61\n",
      "This image tells you that the housing prices are very much related to the location\n",
      "(e.g., close to the ocean) and to the population density, as you probably knew already.\n",
      "It will probably be useful to use a clustering algorithm to detect the main clusters, and\n",
      "add new features that measure the proximity to the cluster centers. The ocean prox‐\n",
      "imity attribute may be useful as well, although in Northern California the housing\n",
      "prices in coastal districts are not too high, so it is not a simple rule.\n",
      "Looking for Correlations\n",
      "Since the dataset is not too large, you can easily compute the standard correlation\n",
      "coefficient  (also called Pearson’s r ) between every pair of attributes using the corr()\n",
      "method:\n",
      "corr_matrix  = housing.corr()\n",
      "Now let’s look at how much each attribute correlates with the median house value:\n",
      ">>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\n",
      "median_house_value    1.000000\n",
      "median_income         0.687170\n",
      "total_rooms           0.135231\n",
      "housing_median_age    0.114220\n",
      "households            0.064702\n",
      "total_bedrooms        0.047865\n",
      "population           -0.026699\n",
      "longitude            -0.047279\n",
      "latitude             -0.142826\n",
      "Name: median_house_value, dtype: float64\n",
      "The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that\n",
      "there is a strong positive correlation; for example, the median house value tends to go\n",
      "up when the median income goes up. When the coefficient is close to –1, it means\n",
      "that there is a strong negative correlation; you can see a small negative correlation\n",
      "between the latitude and the median house value (i.e., prices have a slight tendency to\n",
      "go down when you go north). Finally, coefficients close to zero mean that there is no\n",
      "linear correlation. Figure 2-14  shows various plots along with the correlation coeffi‐\n",
      "cient between their horizontal and vertical axes.\n",
      "62 | Chapter 2: End-to-End Machine Learning Project\n",
      "Figure 2-14. Standard correlation coefficient  of various datasets (source: Wikipedia;\n",
      "public domain image)\n",
      "The correlation coefficient only measures linear correlations (“if x\n",
      "goes up, then y generally goes up/down”). It may completely miss\n",
      "out on nonlinear relationships (e.g., “if x is close to zero then y gen‐\n",
      "erally goes up”). Note how all the plots of the bottom row have a\n",
      "correlation coefficient equal to zero despite the fact that their axes\n",
      "are clearly not independent: these are examples of nonlinear rela‐\n",
      "tionships. Also, the second row shows examples where the correla‐\n",
      "tion coefficient is equal to 1 or –1; notice that this has nothing to\n",
      "do with the slope. For example, your height in inches has a correla‐\n",
      "tion coefficient of 1 with your height in feet or in nanometers.\n",
      "Another way to check for correlation between attributes is to use Pandas’\n",
      "scatter_matrix  function, which plots every numerical attribute against every other\n",
      "numerical attribute. Since there are now 11 numerical attributes, you would get 112 =\n",
      "121 plots, which would not fit on a page, so let’s just focus on a few promising\n",
      "attributes that seem most correlated with the median housing value ( Figure 2-15 ):\n",
      "from pandas.plotting  import scatter_matrix\n",
      "attributes  = [\"median_house_value\" , \"median_income\" , \"total_rooms\" ,\n",
      "              \"housing_median_age\" ]\n",
      "scatter_matrix (housing[attributes ], figsize=(12, 8))\n",
      "Discover and Visualize the Data to Gain Insights | 63\n",
      "Figure 2-15. Scatter matrix\n",
      "The main diagonal (top left to bottom right) would be full of straight lines if Pandas\n",
      "plotted each variable against itself, which would not be very useful. So instead Pandas\n",
      "displays a histogram of each attribute (other options are available; see Pandas’ docu‐\n",
      "mentation for more details).\n",
      "The most promising attribute to predict the median house value is the median\n",
      "income, so let’s zoom in on their correlation scatterplot ( Figure 2-16 ):\n",
      "housing.plot(kind=\"scatter\" , x=\"median_income\" , y=\"median_house_value\" ,\n",
      "             alpha=0.1)\n",
      "This plot reveals a few things. First, the correlation is indeed very strong; you can\n",
      "clearly see the upward trend and the points are not too dispersed. Second, the price\n",
      "cap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this\n",
      "plot reveals other less obvious straight lines: a horizontal line around $450,000,\n",
      "another around $350,000, perhaps one around $280,000, and a few more below that.\n",
      "Y ou may want to try removing the corresponding districts to prevent your algorithms\n",
      "from learning to reproduce these data quirks.\n",
      "64 | Chapter 2: End-to-End Machine Learning Project\n",
      "Figure 2-16. Median income versus median house value\n",
      "Experimenting with Attribute Combinations\n",
      "Hopefully the previous sections gave you an idea of a few ways you can explore the\n",
      "data and gain insights. Y ou identified a few data quirks that you may want to clean up\n",
      "before feeding the data to a Machine Learning algorithm, and you found interesting\n",
      "correlations between attributes, in particular with the target attribute. Y ou also\n",
      "noticed that some attributes have a tail-heavy distribution, so you may want to trans‐\n",
      "form them (e.g., by computing their logarithm). Of course, your mileage will vary\n",
      "considerably with each project, but the general ideas are similar.\n",
      "One last thing you may want to do before actually preparing the data for Machine\n",
      "Learning algorithms is to try out various attribute combinations. For example, the\n",
      "total number of rooms in a district is not very useful if you don’t know how many\n",
      "households there are. What you really want is the number of rooms per household.\n",
      "Similarly, the total number of bedrooms by itself is not very useful: you probably\n",
      "want to compare it to the number of rooms. And the population per household also\n",
      "seems like an interesting attribute combination to look at. Let’s create these new\n",
      "attributes:\n",
      "housing[\"rooms_per_household\" ] = housing[\"total_rooms\" ]/housing[\"households\" ]\n",
      "housing[\"bedrooms_per_room\" ] = housing[\"total_bedrooms\" ]/housing[\"total_rooms\" ]\n",
      "housing[\"population_per_household\" ]=housing[\"population\" ]/housing[\"households\" ]\n",
      "And now let’s look at the correlation matrix again:\n",
      ">>> corr_matrix  = housing.corr()\n",
      ">>> corr_matrix [\"median_house_value\" ].sort_values (ascending =False)\n",
      "median_house_value          1.000000\n",
      "Discover and Visualize the Data to Gain Insights | 65\n",
      "median_income               0.687160\n",
      "rooms_per_household         0.146285\n",
      "total_rooms                 0.135097\n",
      "housing_median_age          0.114110\n",
      "households                  0.064506\n",
      "total_bedrooms              0.047689\n",
      "population_per_household   -0.021985\n",
      "population                 -0.026920\n",
      "longitude                  -0.047432\n",
      "latitude                   -0.142724\n",
      "bedrooms_per_room          -0.259984\n",
      "Name: median_house_value, dtype: float64\n",
      "Hey, not bad! The new bedrooms_per_room  attribute is much more correlated with\n",
      "the median house value than the total number of rooms or bedrooms. Apparently\n",
      "houses with a lower bedroom/room ratio tend to be more expensive. The number of\n",
      "rooms per household is also more informative than the total number of rooms in a\n",
      "district—obviously the larger the houses, the more expensive they are.\n",
      "This round of exploration does not have to be absolutely thorough; the point is to\n",
      "start off on the right foot and quickly gain insights that will help you get a first rea‐\n",
      "sonably good prototype. But this is an iterative process: once you get a prototype up\n",
      "and running, you can analyze its output to gain more insights and come back to this\n",
      "exploration step.\n",
      "Prepare the Data for Machine Learning Algorithms\n",
      "It’s time to prepare the data for your Machine Learning algorithms. Instead of just\n",
      "doing this manually, you should write functions to do that, for several good reasons:\n",
      "•This will allow you to reproduce these transformations easily on any dataset (e.g.,\n",
      "the next time you get a fresh dataset).\n",
      "•Y ou will gradually build a library of transformation functions that you can reuse\n",
      "in future projects.\n",
      "•Y ou can use these functions in your live system to transform the new data before\n",
      "feeding it to your algorithms.\n",
      "•This will make it possible for you to easily try various transformations and see\n",
      "which combination of transformations works best.\n",
      "But first let’s revert to a clean training set (by copying strat_train_set  once again),\n",
      "and let’s separate the predictors and the labels since we don’t necessarily want to apply\n",
      "the same transformations to the predictors and the target values (note that drop()  \n",
      "creates a copy of the data and does not affect strat_train_set ):\n",
      "housing = strat_train_set .drop(\"median_house_value\" , axis=1)\n",
      "housing_labels  = strat_train_set [\"median_house_value\" ].copy()\n",
      "66 | Chapter 2: End-to-End Machine Learning Project\n",
      "Data Cleaning\n",
      "Most Machine Learning algorithms cannot work with missing features, so let’s create\n",
      "a few functions to take care of them. Y ou noticed earlier that the total_bedrooms\n",
      "attribute has some missing values, so let’s fix this. Y ou have three options:\n",
      "•Get rid of the corresponding districts.\n",
      "•Get rid of the whole attribute.\n",
      "•Set the values to some value (zero, the mean, the median, etc.).\n",
      "Y ou can accomplish these easily using DataFrame’s dropna() , drop() , and fillna()\n",
      "methods:\n",
      "housing.dropna(subset=[\"total_bedrooms\" ])    # option 1\n",
      "housing.drop(\"total_bedrooms\" , axis=1)       # option 2\n",
      "median = housing[\"total_bedrooms\" ].median()  # option 3\n",
      "housing[\"total_bedrooms\" ].fillna(median, inplace=True)\n",
      "If you choose option 3, you should compute the median value on the training set, and\n",
      "use it to fill the missing values in the training set, but also don’t forget to save the\n",
      "median value that you have computed. Y ou will need it later to replace missing values\n",
      "in the test set when you want to evaluate your system, and also once the system goes\n",
      "live to replace missing values in new data.\n",
      "Scikit-Learn provides a handy class to take care of missing values: SimpleImputer .\n",
      "Here is how to use it. First, you need to create a SimpleImputer  instance, specifying\n",
      "that you want to replace each attribute’s missing values with the median of that\n",
      "attribute:\n",
      "from sklearn.impute  import SimpleImputer\n",
      "imputer = SimpleImputer (strategy =\"median\" )\n",
      "Since the median can only be computed on numerical attributes, we need to create a\n",
      "copy of the data without the text attribute ocean_proximity :\n",
      "housing_num  = housing.drop(\"ocean_proximity\" , axis=1)\n",
      "Now you can fit the imputer  instance to the training data using the fit()  method:\n",
      "imputer.fit(housing_num )\n",
      "The imputer  has simply computed the median of each attribute and stored the result\n",
      "in its statistics_  instance variable. Only the total_bedrooms  attribute had missing\n",
      "values, but we cannot be sure that there won’t be any missing values in new data after\n",
      "the system goes live, so it is safer to apply the imputer  to all the numerical attributes:\n",
      ">>> imputer.statistics_\n",
      "array([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\n",
      "Prepare the Data for Machine Learning Algorithms | 67\n",
      "17For more details on the design principles, see “ API design for machine learning software: experiences from\n",
      "the scikit-learn project, ” L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Müller, et al. (2013).>>> housing_num .median().values\n",
      "array([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\n",
      "Now you can use this “trained” imputer  to transform the training set by replacing\n",
      "missing values by the learned medians:\n",
      "X = imputer.transform (housing_num )\n",
      "The result is a plain NumPy array containing the transformed features. If you want to\n",
      "put it back into a Pandas DataFrame, it’s simple:\n",
      "housing_tr  = pd.DataFrame (X, columns=housing_num .columns)\n",
      "Scikit-Learn Design\n",
      "Scikit-Learn’s API is remarkably well designed. The main design principles  are:17\n",
      "•Consistency . All objects share a consistent and simple interface:\n",
      "—Estimators . Any object that can estimate some parameters based on a dataset\n",
      "is called an estimator  (e.g., an imputer  is an estimator). The estimation itself is\n",
      "performed by the fit()  method, and it takes only a dataset as a parameter (or\n",
      "two for supervised learning algorithms; the second dataset contains the\n",
      "labels). Any other parameter needed to guide the estimation process is con‐\n",
      "sidered a hyperparameter (such as an imputer ’s strategy ), and it must be set\n",
      "as an instance variable (generally via a constructor parameter).\n",
      "—Transformers . Some estimators (such as an imputer ) can also transform a\n",
      "dataset; these are called transformers . Once again, the API is quite simple: the\n",
      "transformation is performed by the transform()  method with the dataset to\n",
      "transform as a parameter. It returns the transformed dataset. This transforma‐\n",
      "tion generally relies on the learned parameters, as is the case for an imputer .\n",
      "All transformers also have a convenience method called fit_transform()  \n",
      "that is equivalent to calling fit()  and then transform()  (but sometimes\n",
      "fit_transform()  is optimized and runs much faster).\n",
      "—Predictors . Finally, some estimators are capable of making predictions given a\n",
      "dataset; they are called predictors . For example, the LinearRegression  model \n",
      "in the previous chapter was a predictor: it predicted life satisfaction given a\n",
      "country’s GDP per capita. A predictor has a predict()  method that takes a\n",
      "dataset of new instances and returns a dataset of corresponding predictions. It\n",
      "also has a score()  method that measures the quality of the predictions given\n",
      "68 | Chapter 2: End-to-End Machine Learning Project\n",
      "18Some predictors also provide methods to measure the confidence of their predictions.\n",
      "19This class is available since Scikit-Learn 0.20. If you use an earlier version, please consider upgrading, or use\n",
      "Pandas’ Series.factorize()  method.a test set (and the corresponding labels in the case of supervised learning\n",
      "algorithms).18\n",
      "•Inspection . All the estimator’s hyperparameters are accessible directly via public\n",
      "instance variables (e.g., imputer.strategy ), and all the estimator’s learned\n",
      "parameters are also accessible via public instance variables with an underscore\n",
      "suffix (e.g., imputer.statistics_ ).\n",
      "•Nonproliferation of classes . Datasets are represented as NumPy arrays or SciPy\n",
      "sparse matrices, instead of homemade classes. Hyperparameters are just regular\n",
      "Python strings or numbers.\n",
      "•Composition . Existing building blocks are reused as much as possible. For\n",
      "example, it is easy to create a Pipeline  estimator from an arbitrary sequence of\n",
      "transformers followed by a final estimator, as we will see.\n",
      "•Sensible defaults . Scikit-Learn provides reasonable default values for most\n",
      "parameters, making it easy to create a baseline working system quickly.\n",
      "Handling Text and Categorical Attributes\n",
      "Earlier we left out the categorical attribute ocean_proximity  because it is a text\n",
      "attribute so we cannot compute its median:\n",
      ">>> housing_cat  = housing[[\"ocean_proximity\" ]]\n",
      ">>> housing_cat .head(10)\n",
      "      ocean_proximity\n",
      "17606       <1H OCEAN\n",
      "18632       <1H OCEAN\n",
      "14650      NEAR OCEAN\n",
      "3230           INLAND\n",
      "3555        <1H OCEAN\n",
      "19480          INLAND\n",
      "8879        <1H OCEAN\n",
      "13685          INLAND\n",
      "4937        <1H OCEAN\n",
      "4861        <1H OCEAN\n",
      "Most Machine Learning algorithms prefer to work with numbers anyway, so let’s con‐\n",
      "vert these categories from text to numbers. For this, we can use Scikit-Learn’s Ordina\n",
      "lEncoder  class19:\n",
      ">>> from sklearn.preprocessing  import OrdinalEncoder\n",
      ">>> ordinal_encoder  = OrdinalEncoder ()\n",
      "Prepare the Data for Machine Learning Algorithms | 69\n",
      "20Before Scikit-Learn 0.20, it could only encode integer categorical values, but since 0.20 it can also handle\n",
      "other types of inputs, including text categorical inputs.>>> housing_cat_encoded  = ordinal_encoder .fit_transform (housing_cat )\n",
      ">>> housing_cat_encoded [:10]\n",
      "array([[0.],\n",
      "       [0.],\n",
      "       [4.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [1.],\n",
      "       [0.],\n",
      "       [0.]])\n",
      "Y ou can get the list of categories using the categories_  instance variable. It is a list\n",
      "containing a 1D array of categories for each categorical attribute (in this case, a list\n",
      "containing a single array since there is just one categorical attribute):\n",
      ">>> ordinal_encoder .categories_\n",
      "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
      "       dtype=object)]\n",
      "One issue with this representation is that ML algorithms will assume that two nearby\n",
      "values are more similar than two distant values. This may be fine in some cases (e.g.,\n",
      "for ordered categories such as “bad” , “average” , “good” , “excellent”), but it is obviously\n",
      "not the case for the ocean_proximity  column (for example, categories 0 and 4 are\n",
      "clearly more similar than categories 0 and 1). To fix this issue, a common solution is\n",
      "to create one binary attribute per category: one attribute equal to 1 when the category\n",
      "is “<1H OCEAN” (and 0 otherwise), another attribute equal to 1 when the category is\n",
      "“INLAND” (and 0 otherwise), and so on. This is called one-hot encoding , because\n",
      "only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new\n",
      "attributes are sometimes called dummy  attributes. Scikit-Learn provides a OneHotEn\n",
      "coder  class to convert categorical values into one-hot vectors20:\n",
      ">>> from sklearn.preprocessing  import OneHotEncoder\n",
      ">>> cat_encoder  = OneHotEncoder ()\n",
      ">>> housing_cat_1hot  = cat_encoder .fit_transform (housing_cat )\n",
      ">>> housing_cat_1hot\n",
      "<16512x5 sparse matrix of type '<class 'numpy.float64'>'\n",
      "  with 16512 stored elements in Compressed Sparse Row format>\n",
      "Notice that the output is a SciPy sparse matrix , instead of a NumPy array. This is very\n",
      "useful when you have categorical attributes with thousands of categories. After one-\n",
      "hot encoding we get a matrix with thousands of columns, and the matrix is full of\n",
      "zeros except for a single 1 per row. Using up tons of memory mostly to store zeros\n",
      "would be very wasteful, so instead a sparse matrix only stores the location of the non‐\n",
      "70 | Chapter 2: End-to-End Machine Learning Project\n",
      "21See SciPy’s documentation for more details.\n",
      "zero elements. Y ou can use it mostly like a normal 2D array,21 but if you really want to\n",
      "convert it to a (dense) NumPy array, just call the toarray()  method:\n",
      ">>> housing_cat_1hot .toarray()\n",
      "array([[1., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       ...,\n",
      "       [0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0.]])\n",
      "Once again, you can get the list of categories using the encoder’s categories_\n",
      "instance variable:\n",
      ">>> cat_encoder .categories_\n",
      "[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'],\n",
      "       dtype=object)]\n",
      "If a categorical attribute has a large number of possible categories\n",
      "(e.g., country code, profession, species, etc.), then one-hot encod‐\n",
      "ing will result in a large number of input features. This may slow\n",
      "down training and degrade performance. If this happens, you may\n",
      "want to replace the categorical input with useful numerical features\n",
      "related to the categories: for example, you could replace the\n",
      "ocean_proximity  feature with the distance to the ocean (similarly,\n",
      "a country code could be replaced with the country’s population and\n",
      "GDP per capita). Alternatively, you could replace each category\n",
      "with a learnable low dimensional vector called an embedding . Each\n",
      "category’s representation would be learned during training: this is\n",
      "an example of representation learning  (see Chapter 13  and ??? for\n",
      "more details).\n",
      "Custom Transformers\n",
      "Although Scikit-Learn provides many useful transformers, you will need to write\n",
      "your own for tasks such as custom cleanup operations or combining specific\n",
      "attributes. Y ou will want your transformer to work seamlessly with Scikit-Learn func‐\n",
      "tionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐\n",
      "itance), all you need is to create a class and implement three methods: fit()\n",
      "(returning self ), transform() , and fit_transform() . Y ou can get the last one for\n",
      "free by simply adding TransformerMixin  as a base class. Also, if you add BaseEstima\n",
      "tor as a base class (and avoid *args  and **kargs  in your constructor) you will get\n",
      "two extra methods ( get_params()  and set_params() ) that will be useful for auto‐\n",
      "Prepare the Data for Machine Learning Algorithms | 71\n",
      "matic hyperparameter tuning. For example, here is a small transformer class that adds\n",
      "the combined attributes we discussed earlier:\n",
      "from sklearn.base  import BaseEstimator , TransformerMixin\n",
      "rooms_ix , bedrooms_ix , population_ix , households_ix  = 3, 4, 5, 6\n",
      "class CombinedAttributesAdder (BaseEstimator , TransformerMixin ):\n",
      "    def __init__ (self, add_bedrooms_per_room  = True): # no *args or **kargs\n",
      "        self.add_bedrooms_per_room  = add_bedrooms_per_room\n",
      "    def fit(self, X, y=None):\n",
      "        return self  # nothing else to do\n",
      "    def transform (self, X, y=None):\n",
      "        rooms_per_household  = X[:, rooms_ix ] / X[:, households_ix ]\n",
      "        population_per_household  = X[:, population_ix ] / X[:, households_ix ]\n",
      "        if self.add_bedrooms_per_room :\n",
      "            bedrooms_per_room  = X[:, bedrooms_ix ] / X[:, rooms_ix ]\n",
      "            return np.c_[X, rooms_per_household , population_per_household ,\n",
      "                         bedrooms_per_room ]\n",
      "        else:\n",
      "            return np.c_[X, rooms_per_household , population_per_household ]\n",
      "attr_adder  = CombinedAttributesAdder (add_bedrooms_per_room =False)\n",
      "housing_extra_attribs  = attr_adder .transform (housing.values)\n",
      "In this example the transformer has one hyperparameter, add_bedrooms_per_room ,\n",
      "set to True  by default (it is often helpful to provide sensible defaults). This hyperpara‐\n",
      "meter will allow you to easily find out whether adding this attribute helps the\n",
      "Machine Learning algorithms or not. More generally, you can add a hyperparameter\n",
      "to gate any data preparation step that you are not 100% sure about. The more you\n",
      "automate these data preparation steps, the more combinations you can automatically\n",
      "try out, making it much more likely that you will find a great combination (and sav‐\n",
      "ing you a lot of time).\n",
      "Feature Scaling\n",
      "One of the most important transformations you need to apply to your data is feature\n",
      "scaling . With few exceptions, Machine Learning algorithms don’t perform well when\n",
      "the input numerical attributes have very different scales. This is the case for the hous‐\n",
      "ing data: the total number of rooms ranges from about 6 to 39,320, while the median\n",
      "incomes only range from 0 to 15. Note that scaling the target values is generally not\n",
      "required.\n",
      "There are two common ways to get all attributes to have the same scale: min-max\n",
      "scaling  and standardization .\n",
      "Min-max scaling (many people call this normalization ) is quite simple: values are\n",
      "shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract‐\n",
      "ing the min value and dividing by the max minus the min. Scikit-Learn provides a\n",
      "72 | Chapter 2: End-to-End Machine Learning Project\n",
      "transformer called MinMaxScaler  for this. It has a feature_range  hyperparameter\n",
      "that lets you change the range if you don’t want 0–1 for some reason.\n",
      "Standardization is quite different: first it subtracts the mean value (so standardized\n",
      "values always have a zero mean), and then it divides by the standard deviation so that\n",
      "the resulting distribution has unit variance. Unlike min-max scaling, standardization\n",
      "does not bound values to a specific range, which may be a problem for some algo‐\n",
      "rithms (e.g., neural networks often expect an input value ranging from 0 to 1). How‐\n",
      "ever, standardization is much less affected by outliers. For example, suppose a district\n",
      "had a median income equal to 100 (by mistake). Min-max scaling would then crush\n",
      "all the other values from 0–15 down to 0–0.15, whereas standardization would not be\n",
      "much affected. Scikit-Learn provides a transformer called StandardScaler  for stand‐\n",
      "ardization.\n",
      "As with all the transformations, it is important to fit the scalers to\n",
      "the training data only, not to the full dataset (including the test set).\n",
      "Only then can you use them to transform the training set and the\n",
      "test set (and new data).\n",
      "Transformation Pipelines\n",
      "As you can see, there are many data transformation steps that need to be executed in\n",
      "the right order. Fortunately, Scikit-Learn provides the Pipeline  class to help with\n",
      "such sequences of transformations. Here is a small pipeline for the numerical\n",
      "attributes:\n",
      "from sklearn.pipeline  import Pipeline\n",
      "from sklearn.preprocessing  import StandardScaler\n",
      "num_pipeline  = Pipeline ([\n",
      "        ('imputer' , SimpleImputer (strategy =\"median\" )),\n",
      "        ('attribs_adder' , CombinedAttributesAdder ()),\n",
      "        ('std_scaler' , StandardScaler ()),\n",
      "    ])\n",
      "housing_num_tr  = num_pipeline .fit_transform (housing_num )\n",
      "The Pipeline  constructor takes a list of name/estimator pairs defining a sequence of\n",
      "steps. All but the last estimator must be transformers (i.e., they must have a\n",
      "fit_transform()  method). The names can be anything you like (as long as they are\n",
      "unique and don’t contain double underscores “ __”): they will come in handy later for\n",
      "hyperparameter tuning.\n",
      "When you call the pipeline’s fit()  method, it calls fit_transform()  sequentially on\n",
      "all transformers, passing the output of each call as the parameter to the next call, until\n",
      "it reaches the final estimator, for which it just calls the fit()  method.\n",
      "Prepare the Data for Machine Learning Algorithms | 73\n",
      "22Just like for pipelines, the name can be anything as long as it does not contain double underscores.The pipeline exposes the same methods as the final estimator. In this example, the last\n",
      "estimator is a StandardScaler , which is a transformer, so the pipeline has a trans\n",
      "form()  method that applies all the transforms to the data in sequence (and of course\n",
      "also a fit_transform()  method, which is the one we used).\n",
      "So far, we have handled the categorical columns and the numerical columns sepa‐\n",
      "rately. It would be more convenient to have a single transformer able to handle all col‐\n",
      "umns, applying the appropriate transformations to each column. In version 0.20,\n",
      "Scikit-Learn introduced the ColumnTransformer  for this purpose, and the good news\n",
      "is that it works great with Pandas DataFrames. Let’s use it to apply all the transforma‐\n",
      "tions to the housing data:\n",
      "from sklearn.compose  import ColumnTransformer\n",
      "num_attribs  = list(housing_num )\n",
      "cat_attribs  = [\"ocean_proximity\" ]\n",
      "full_pipeline  = ColumnTransformer ([\n",
      "        (\"num\", num_pipeline , num_attribs ),\n",
      "        (\"cat\", OneHotEncoder (), cat_attribs ),\n",
      "    ])\n",
      "housing_prepared  = full_pipeline .fit_transform (housing)\n",
      "Here is how this works: first we import the ColumnTransformer  class, next we get the\n",
      "list of numerical column names and the list of categorical column names, and we\n",
      "construct a ColumnTransformer . The constructor requires a list of tuples, where each\n",
      "tuple contains a name22, a transformer and a list of names (or indices) of columns\n",
      "that the transformer should be applied to. In this example, we specify that the numer‐\n",
      "ical columns should be transformed using the num_pipeline  that we defined earlier,\n",
      "and the categorical columns should be transformed using a OneHotEncoder . Finally,\n",
      "we apply this ColumnTransformer  to the housing data: it applies each transformer to\n",
      "the appropriate columns and concatenates the outputs along the second axis (the\n",
      "transformers must return the same number of rows).\n",
      "Note that the OneHotEncoder  returns a sparse matrix, while the num_pipeline  returns\n",
      "a dense matrix. When there is such a mix of sparse and dense matrices, the Colum\n",
      "nTransformer  estimates the density of the final matrix (i.e., the ratio of non-zero\n",
      "cells), and it returns a sparse matrix if the density is lower than a given threshold (by\n",
      "default, sparse_threshold=0.3 ). In this example, it returns a dense matrix. And\n",
      "that’s it! We have a preprocessing pipeline that takes the full housing data and applies\n",
      "the appropriate transformations to each column.\n",
      "74 | Chapter 2: End-to-End Machine Learning Project\n",
      "Instead of a transformer, you can specify the string \"drop\"  if you\n",
      "want the columns to be dropped. Or you can specify \"pass\n",
      "through\"  if you want the columns to be left untouched. By default,\n",
      "the remaining columns (i.e., the ones that were not listed) will be\n",
      "dropped, but you can set the remainder  hyperparameter to any\n",
      "transformer (or to \"passthrough\" ) if you want these columns to be\n",
      "handled differently.\n",
      "If you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\n",
      "sklearn-pandas , or roll out your own custom transformer to get the same function‐\n",
      "ality as the ColumnTransformer . Alternatively, you can use the FeatureUnion  class\n",
      "which can also apply different transformers and concatenate their outputs, but you\n",
      "cannot specify different columns for each transformer, they all apply to the whole\n",
      "data. It is possible to work around this limitation using a custom transformer for col‐\n",
      "umn selection (see the Jupyter notebook for an example).\n",
      "Select and Train a Model\n",
      "At last! Y ou framed the problem, you got the data and explored it, you sampled a\n",
      "training set and a test set, and you wrote transformation pipelines to clean up and\n",
      "prepare your data for Machine Learning algorithms automatically. Y ou are now ready\n",
      "to select and train a Machine Learning model.\n",
      "Training and Evaluating on the Training Set\n",
      "The good news is that thanks to all these previous steps, things are now going to be\n",
      "much simpler than you might think. Let’s first train a Linear Regression model, like\n",
      "we did in the previous chapter:\n",
      "from sklearn.linear_model  import LinearRegression\n",
      "lin_reg = LinearRegression ()\n",
      "lin_reg.fit(housing_prepared , housing_labels )\n",
      "Done! Y ou now have a working Linear Regression model. Let’s try it out on a few\n",
      "instances from the training set:\n",
      ">>> some_data  = housing.iloc[:5]\n",
      ">>> some_labels  = housing_labels .iloc[:5]\n",
      ">>> some_data_prepared  = full_pipeline .transform (some_data )\n",
      ">>> print(\"Predictions:\" , lin_reg.predict(some_data_prepared ))\n",
      "Predictions: [ 210644.6045  317768.8069  210956.4333  59218.9888  189747.5584]\n",
      ">>> print(\"Labels:\" , list(some_labels ))\n",
      "Labels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\n",
      "Select and Train a Model | 75\n",
      "It works, although the predictions are not exactly accurate (e.g., the first prediction is\n",
      "off by close to 40%!). Let’s measure this regression model’s RMSE on the whole train‐\n",
      "ing set using Scikit-Learn’s mean_squared_error  function:\n",
      ">>> from sklearn.metrics  import mean_squared_error\n",
      ">>> housing_predictions  = lin_reg.predict(housing_prepared )\n",
      ">>> lin_mse = mean_squared_error (housing_labels , housing_predictions )\n",
      ">>> lin_rmse  = np.sqrt(lin_mse)\n",
      ">>> lin_rmse\n",
      "68628.19819848922\n",
      "Okay, this is better than nothing but clearly not a great score: most districts’\n",
      "median_housing_values  range between $120,000 and $265,000, so a typical predic‐\n",
      "tion error of $68,628 is not very satisfying. This is an example of a model underfitting\n",
      "the training data. When this happens it can mean that the features do not provide\n",
      "enough information to make good predictions, or that the model is not powerful\n",
      "enough. As we saw in the previous chapter, the main ways to fix underfitting are to\n",
      "select a more powerful model, to feed the training algorithm with better features, or\n",
      "to reduce the constraints on the model. This model is not regularized, so this rules\n",
      "out the last option. Y ou could try to add more features (e.g., the log of the popula‐\n",
      "tion), but first let’s try a more complex model to see how it does.\n",
      "Let’s train a DecisionTreeRegressor . This is a powerful model, capable of finding\n",
      "complex nonlinear relationships in the data (Decision Trees are presented in more\n",
      "detail in Chapter 6 ). The code should look familiar by now:\n",
      "from sklearn.tree  import DecisionTreeRegressor\n",
      "tree_reg  = DecisionTreeRegressor ()\n",
      "tree_reg .fit(housing_prepared , housing_labels )\n",
      "Now that the model is trained, let’s evaluate it on the training set:\n",
      ">>> housing_predictions  = tree_reg .predict(housing_prepared )\n",
      ">>> tree_mse  = mean_squared_error (housing_labels , housing_predictions )\n",
      ">>> tree_rmse  = np.sqrt(tree_mse )\n",
      ">>> tree_rmse\n",
      "0.0\n",
      "Wait, what!? No error at all? Could this model really be absolutely perfect? Of course,\n",
      "it is much more likely that the model has badly overfit the data. How can you be sure?\n",
      "As we saw earlier, you don’t want to touch the test set until you are ready to launch a\n",
      "model you are confident about, so you need to use part of the training set for train‐\n",
      "ing, and part for model validation.\n",
      "Better Evaluation Using Cross-Validation\n",
      "One way to evaluate the Decision Tree model would be to use the train_test_split\n",
      "function to split the training set into a smaller training set and a validation set, then\n",
      "76 | Chapter 2: End-to-End Machine Learning Project\n",
      "train your models against the smaller training set and evaluate them against the vali‐\n",
      "dation set. It’s a bit of work, but nothing too difficult and it would work fairly well.\n",
      "A great alternative is to use Scikit-Learn’s K-fold cross-validation  feature. The follow‐\n",
      "ing code randomly splits the training set into 10 distinct subsets called folds , then it\n",
      "trains and evaluates the Decision Tree model 10 times, picking a different fold for\n",
      "evaluation every time and training on the other 9 folds. The result is an array con‐\n",
      "taining the 10 evaluation scores:\n",
      "from sklearn.model_selection  import cross_val_score\n",
      "scores = cross_val_score (tree_reg , housing_prepared , housing_labels ,\n",
      "                         scoring=\"neg_mean_squared_error\" , cv=10)\n",
      "tree_rmse_scores  = np.sqrt(-scores)\n",
      "Scikit-Learn’s cross-validation features expect a utility function\n",
      "(greater is better) rather than a cost function (lower is better), so\n",
      "the scoring function is actually the opposite of the MSE (i.e., a neg‐\n",
      "ative value), which is why the preceding code computes -scores\n",
      "before calculating the square root.\n",
      "Let’s look at the results:\n",
      ">>> def display_scores (scores):\n",
      "...     print(\"Scores:\" , scores)\n",
      "...     print(\"Mean:\", scores.mean())\n",
      "...     print(\"Standard deviation:\" , scores.std())\n",
      "...\n",
      ">>> display_scores (tree_rmse_scores )\n",
      "Scores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782\n",
      " 71115.88230639 75585.14172901 70262.86139133 70273.6325285\n",
      " 75366.87952553 71231.65726027]\n",
      "Mean: 71407.68766037929\n",
      "Standard deviation: 2439.4345041191004\n",
      "Now the Decision Tree doesn’t look as good as it did earlier. In fact, it seems to per‐\n",
      "form worse than the Linear Regression model! Notice that cross-validation allows\n",
      "you to get not only an estimate of the performance of your model, but also a measure\n",
      "of how precise this estimate is (i.e., its standard deviation). The Decision Tree has a\n",
      "score of approximately 71,407, generally ±2,439. Y ou would not have this information\n",
      "if you just used one validation set. But cross-validation comes at the cost of training\n",
      "the model several times, so it is not always possible.\n",
      "Let’s compute the same scores for the Linear Regression model just to be sure:\n",
      ">>> lin_scores  = cross_val_score (lin_reg, housing_prepared , housing_labels ,\n",
      "...                              scoring=\"neg_mean_squared_error\" , cv=10)\n",
      "...\n",
      ">>> lin_rmse_scores  = np.sqrt(-lin_scores )\n",
      ">>> display_scores (lin_rmse_scores )\n",
      "Select and Train a Model | 77\n",
      "Scores: [66782.73843989 66960.118071   70347.95244419 74739.57052552\n",
      " 68031.13388938 71193.84183426 64969.63056405 68281.61137997\n",
      " 71552.91566558 67665.10082067]\n",
      "Mean: 69052.46136345083\n",
      "Standard deviation: 2731.674001798348\n",
      "That’s right: the Decision Tree model is overfitting so badly that it performs worse\n",
      "than the Linear Regression model.\n",
      "Let’s try one last model now: the RandomForestRegressor . As we will see in Chap‐\n",
      "ter 7 , Random Forests work by training many Decision Trees on random subsets of\n",
      "the features, then averaging out their predictions. Building a model on top of many\n",
      "other models is called Ensemble Learning , and it is often a great way to push ML algo‐\n",
      "rithms even further. We will skip most of the code since it is essentially the same as\n",
      "for the other models:\n",
      ">>> from sklearn.ensemble  import RandomForestRegressor\n",
      ">>> forest_reg  = RandomForestRegressor ()\n",
      ">>> forest_reg .fit(housing_prepared , housing_labels )\n",
      ">>> [...]\n",
      ">>> forest_rmse\n",
      "18603.515021376355\n",
      ">>> display_scores (forest_rmse_scores )\n",
      "Scores: [49519.80364233 47461.9115823  50029.02762854 52325.28068953\n",
      " 49308.39426421 53446.37892622 48634.8036574  47585.73832311\n",
      " 53490.10699751 50021.5852922 ]\n",
      "Mean: 50182.303100336096\n",
      "Standard deviation: 2097.0810550985693\n",
      "Wow, this is much better: Random Forests look very promising. However, note that\n",
      "the score on the training set is still much lower than on the validation sets, meaning\n",
      "that the model is still overfitting the training set. Possible solutions for overfitting are\n",
      "to simplify the model, constrain it (i.e., regularize it), or get a lot more training data.\n",
      "However, before you dive much deeper in Random Forests, you should try out many\n",
      "other models from various categories of Machine Learning algorithms (several Sup‐\n",
      "port Vector Machines with different kernels, possibly a neural network, etc.), without\n",
      "spending too much time tweaking the hyperparameters. The goal is to shortlist a few\n",
      "(two to five) promising models.\n",
      "78 | Chapter 2: End-to-End Machine Learning Project\n",
      "Y ou should save every model you experiment with, so you can\n",
      "come back easily to any model you want. Make sure you save both\n",
      "the hyperparameters and the trained parameters, as well as the\n",
      "cross-validation scores and perhaps the actual predictions as well.\n",
      "This will allow you to easily compare scores across model types,\n",
      "and compare the types of errors they make. Y ou can easily save\n",
      "Scikit-Learn models by using Python’s pickle  module, or using\n",
      "sklearn.externals.joblib , which is more efficient at serializing \n",
      "large NumPy arrays:\n",
      "from sklearn.externals  import joblib\n",
      "joblib.dump(my_model , \"my_model.pkl\" )\n",
      "# and later...\n",
      "my_model_loaded  = joblib.load(\"my_model.pkl\" )\n",
      "Fine-Tune Your Model\n",
      "Let’s assume that you now have a shortlist of promising models. Y ou now need to\n",
      "fine-tune them. Let’s look at a few ways you can do that.\n",
      "Grid Search\n",
      "One way to do that would be to fiddle with the hyperparameters manually, until you\n",
      "find a great combination of hyperparameter values. This would be very tedious work,\n",
      "and you may not have time to explore many combinations.\n",
      "Instead you should get Scikit-Learn’s GridSearchCV  to search for you. All you need to\n",
      "do is tell it which hyperparameters you want it to experiment with, and what values to\n",
      "try out, and it will evaluate all the possible combinations of hyperparameter values,\n",
      "using cross-validation. For example, the following code searches for the best combi‐\n",
      "nation of hyperparameter values for the RandomForestRegressor :\n",
      "from sklearn.model_selection  import GridSearchCV\n",
      "param_grid  = [\n",
      "    {'n_estimators' : [3, 10, 30], 'max_features' : [2, 4, 6, 8]},\n",
      "    {'bootstrap' : [False], 'n_estimators' : [3, 10], 'max_features' : [2, 3, 4]},\n",
      "  ]\n",
      "forest_reg  = RandomForestRegressor ()\n",
      "grid_search  = GridSearchCV (forest_reg , param_grid , cv=5,\n",
      "                           scoring='neg_mean_squared_error' ,\n",
      "                           return_train_score =True)\n",
      "grid_search .fit(housing_prepared , housing_labels )\n",
      "Fine-Tune Your Model | 79\n",
      "When you have no idea what value a hyperparameter should have,\n",
      "a simple approach is to try out consecutive powers of 10 (or a\n",
      "smaller number if you want a more fine-grained search, as shown\n",
      "in this example with the n_estimators  hyperparameter).\n",
      "This param_grid  tells Scikit-Learn to first evaluate all 3 × 4 = 12 combinations of\n",
      "n_estimators  and max_features  hyperparameter values specified in the first dict\n",
      "(don’t worry about what these hyperparameters mean for now; they will be explained\n",
      "in Chapter 7 ), then try all 2 × 3 = 6 combinations of hyperparameter values in the\n",
      "second dict , but this time with the bootstrap  hyperparameter set to False  instead of\n",
      "True  (which is the default value for this hyperparameter).\n",
      "All in all, the grid search will explore 12 + 6 = 18 combinations of RandomForestRe\n",
      "gressor  hyperparameter values, and it will train each model five times (since we are\n",
      "using five-fold cross validation). In other words, all in all, there will be 18 × 5 = 90\n",
      "rounds of training! It may take quite a long time, but when it is done you can get the\n",
      "best combination of parameters like this:\n",
      ">>> grid_search .best_params_\n",
      "{'max_features': 8, 'n_estimators': 30}\n",
      "Since 8 and 30 are the maximum values that were evaluated, you\n",
      "should probably try searching again with higher values, since the\n",
      "score may continue to improve.\n",
      "Y ou can also get the best estimator directly:\n",
      ">>> grid_search .best_estimator_\n",
      "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=30, n_jobs=None, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False)\n",
      "If GridSearchCV  is initialized with refit=True  (which is the\n",
      "default), then once it finds the best estimator using cross-\n",
      "validation, it retrains it on the whole training set. This is usually a\n",
      "good idea since feeding it more data will likely improve its perfor‐\n",
      "mance.\n",
      "And of course the evaluation scores are also available:\n",
      ">>> cvres = grid_search .cv_results_\n",
      ">>> for mean_score , params in zip(cvres[\"mean_test_score\" ], cvres[\"params\" ]):\n",
      "80 | Chapter 2: End-to-End Machine Learning Project\n",
      "...     print(np.sqrt(-mean_score ), params)\n",
      "...\n",
      "63669.05791727153 {'max_features': 2, 'n_estimators': 3}\n",
      "55627.16171305252 {'max_features': 2, 'n_estimators': 10}\n",
      "53384.57867637289 {'max_features': 2, 'n_estimators': 30}\n",
      "60965.99185930139 {'max_features': 4, 'n_estimators': 3}\n",
      "52740.98248528835 {'max_features': 4, 'n_estimators': 10}\n",
      "50377.344409590376 {'max_features': 4, 'n_estimators': 30}\n",
      "58663.84733372485 {'max_features': 6, 'n_estimators': 3}\n",
      "52006.15355973719 {'max_features': 6, 'n_estimators': 10}\n",
      "50146.465964159885 {'max_features': 6, 'n_estimators': 30}\n",
      "57869.25504027614 {'max_features': 8, 'n_estimators': 3}\n",
      "51711.09443660957 {'max_features': 8, 'n_estimators': 10}\n",
      "49682.25345942335 {'max_features': 8, 'n_estimators': 30}\n",
      "62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n",
      "54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n",
      "59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n",
      "52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n",
      "57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n",
      "51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\n",
      "In this example, we obtain the best solution by setting the max_features  hyperpara‐\n",
      "meter to 8, and the n_estimators  hyperparameter to 30. The RMSE score for this\n",
      "combination is 49,682, which is slightly better than the score you got earlier using the\n",
      "default hyperparameter values (which was 50,182). Congratulations, you have suc‐\n",
      "cessfully fine-tuned your best model!\n",
      "Don’t forget that you can treat some of the data preparation steps as\n",
      "hyperparameters. For example, the grid search will automatically\n",
      "find out whether or not to add a feature you were not sure about\n",
      "(e.g., using the add_bedrooms_per_room  hyperparameter of your\n",
      "CombinedAttributesAdder  transformer). It may similarly be used\n",
      "to automatically find the best way to handle outliers, missing fea‐\n",
      "tures, feature selection, and more.\n",
      "Randomized Search\n",
      "The grid search approach is fine when you are exploring relatively few combinations,\n",
      "like in the previous example, but when the hyperparameter search space  is large, it is\n",
      "often preferable to use RandomizedSearchCV  instead. This class can be used in much\n",
      "the same way as the GridSearchCV  class, but instead of trying out all possible combi‐\n",
      "nations, it evaluates a given number of random combinations by selecting a random\n",
      "value for each hyperparameter at every iteration. This approach has two main bene‐\n",
      "fits:\n",
      "Fine-Tune Your Model | 81\n",
      "•If you let the randomized search run for, say, 1,000 iterations, this approach will\n",
      "explore 1,000 different values for each hyperparameter (instead of just a few val‐\n",
      "ues per hyperparameter with the grid search approach).\n",
      "•Y ou have more control over the computing budget you want to allocate to hyper‐\n",
      "parameter search, simply by setting the number of iterations.\n",
      "Ensemble Methods\n",
      "Another way to fine-tune your system is to try to combine the models that perform\n",
      "best. The group (or “ensemble”) will often perform better than the best individual\n",
      "model (just like Random Forests perform better than the individual Decision Trees\n",
      "they rely on), especially if the individual models make very different types of errors.\n",
      "We will cover this topic in more detail in Chapter 7 .\n",
      "Analyze the Best Models and Their Errors\n",
      "Y ou will often gain good insights on the problem by inspecting the best models. For\n",
      "example, the RandomForestRegressor  can indicate the relative importance of each\n",
      "attribute for making accurate predictions:\n",
      ">>> feature_importances  = grid_search .best_estimator_ .feature_importances_\n",
      ">>> feature_importances\n",
      "array([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02,\n",
      "       1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01,\n",
      "       5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02,\n",
      "       1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\n",
      "Let’s display these importance scores next to their corresponding attribute names:\n",
      ">>> extra_attribs  = [\"rooms_per_hhold\" , \"pop_per_hhold\" , \"bedrooms_per_room\" ]\n",
      ">>> cat_encoder  = full_pipeline .named_transformers_ [\"cat\"]\n",
      ">>> cat_one_hot_attribs  = list(cat_encoder .categories_ [0])\n",
      ">>> attributes  = num_attribs  + extra_attribs  + cat_one_hot_attribs\n",
      ">>> sorted(zip(feature_importances , attributes ), reverse=True)\n",
      "[(0.3661589806181342, 'median_income'),\n",
      " (0.1647809935615905, 'INLAND'),\n",
      " (0.10879295677551573, 'pop_per_hhold'),\n",
      " (0.07334423551601242, 'longitude'),\n",
      " (0.0629090704826203, 'latitude'),\n",
      " (0.05641917918195401, 'rooms_per_hhold'),\n",
      " (0.05335107734767581, 'bedrooms_per_room'),\n",
      " (0.041143798478729635, 'housing_median_age'),\n",
      " (0.014874280890402767, 'population'),\n",
      " (0.014672685420543237, 'total_rooms'),\n",
      " (0.014257599323407807, 'households'),\n",
      " (0.014106483453584102, 'total_bedrooms'),\n",
      " (0.010311488326303787, '<1H OCEAN'),\n",
      " (0.002856474637320158, 'NEAR OCEAN'),\n",
      "82 | Chapter 2: End-to-End Machine Learning Project\n",
      " (0.00196041559947807, 'NEAR BAY'),\n",
      " (6.028038672736599e-05, 'ISLAND')]\n",
      "With this information, you may want to try dropping some of the less useful features\n",
      "(e.g., apparently only one ocean_proximity  category is really useful, so you could try\n",
      "dropping the others).\n",
      "Y ou should also look at the specific errors that your system makes, then try to under‐\n",
      "stand why it makes them and what could fix the problem (adding extra features or, on\n",
      "the contrary, getting rid of uninformative ones, cleaning up outliers, etc.).\n",
      "Evaluate Your System on the Test Set\n",
      "After tweaking your models for a while, you eventually have a system that performs\n",
      "sufficiently well. Now is the time to evaluate the final model on the test set. There is\n",
      "nothing special about this process; just get the predictors and the labels from your\n",
      "test set, run your full_pipeline  to transform the data (call transform() , not\n",
      "fit_transform() , you do not want to fit the test set!), and evaluate the final model\n",
      "on the test set:\n",
      "final_model  = grid_search .best_estimator_\n",
      "X_test = strat_test_set .drop(\"median_house_value\" , axis=1)\n",
      "y_test = strat_test_set [\"median_house_value\" ].copy()\n",
      "X_test_prepared  = full_pipeline .transform (X_test)\n",
      "final_predictions  = final_model .predict(X_test_prepared )\n",
      "final_mse  = mean_squared_error (y_test, final_predictions )\n",
      "final_rmse  = np.sqrt(final_mse )   # => evaluates to 47,730.2\n",
      "In some cases, such a point estimate of the generalization error will not be quite\n",
      "enough to convince you to launch: what if it is just 0.1% better than the model cur‐\n",
      "rently in production? Y ou might want to have an idea of how precise this estimate is.\n",
      "For this, you can compute a 95% confidence  interval  for the generalization error using\n",
      "scipy.stats.t.interval() :\n",
      ">>> from scipy import stats\n",
      ">>> confidence  = 0.95\n",
      ">>> squared_errors  = (final_predictions  - y_test) ** 2\n",
      ">>> np.sqrt(stats.t.interval (confidence , len(squared_errors ) - 1,\n",
      "...                          loc=squared_errors .mean(),\n",
      "...                          scale=stats.sem(squared_errors )))\n",
      "...\n",
      "array([45685.10470776, 49691.25001878])\n",
      "The performance will usually be slightly worse than what you measured using cross-\n",
      "validation if you did a lot of hyperparameter tuning (because your system ends up\n",
      "fine-tuned to perform well on the validation data, and will likely not perform as well\n",
      "Fine-Tune Your Model | 83\n",
      "on unknown datasets). It is not the case in this example, but when this happens you\n",
      "must resist the temptation to tweak the hyperparameters to make the numbers look\n",
      "good on the test set; the improvements would be unlikely to generalize to new data.\n",
      "Now comes the project prelaunch phase: you need to present your solution (high‐\n",
      "lighting what you have learned, what worked and what did not, what assumptions\n",
      "were made, and what your system’s limitations are), document everything, and create\n",
      "nice presentations with clear visualizations and easy-to-remember statements (e.g.,\n",
      "“the median income is the number one predictor of housing prices”). In this Califor‐\n",
      "nia housing example, the final performance of the system is not better than the\n",
      "experts’ , but it may still be a good idea to launch it, especially if this frees up some\n",
      "time for the experts so they can work on more interesting and productive tasks.\n",
      "Launch, Monitor, and Maintain Your System\n",
      "Perfect, you got approval to launch! Y ou need to get your solution ready for produc‐\n",
      "tion, in particular by plugging the production input data sources into your system\n",
      "and writing tests.\n",
      "Y ou also need to write monitoring code to check your system’s live performance at\n",
      "regular intervals and trigger alerts when it drops. This is important to catch not only\n",
      "sudden breakage, but also performance degradation. This is quite common because\n",
      "models tend to “rot” as data evolves over time, unless the models are regularly trained\n",
      "on fresh data.\n",
      "Evaluating your system’s performance will require sampling the system’s predictions\n",
      "and evaluating them. This will generally require a human analysis. These analysts\n",
      "may be field experts, or workers on a crowdsourcing platform (such as Amazon\n",
      "Mechanical Turk or CrowdFlower). Either way, you need to plug the human evalua‐\n",
      "tion pipeline into your system.\n",
      "Y ou should also make sure you evaluate the system’s input data quality. Sometimes\n",
      "performance will degrade slightly because of a poor quality signal (e.g., a malfunc‐\n",
      "tioning sensor sending random values, or another team’s output becoming stale), but\n",
      "it may take a while before your system’s performance degrades enough to trigger an\n",
      "alert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the\n",
      "inputs is particularly important for online learning systems.\n",
      "Finally, you will generally want to train your models on a regular basis using fresh\n",
      "data. Y ou should automate this process as much as possible. If you don’t, you are very\n",
      "likely to refresh your model only every six months (at best), and your system’s perfor‐\n",
      "mance may fluctuate severely over time. If your system is an online learning system,\n",
      "you should make sure you save snapshots of its state at regular intervals so you can\n",
      "easily roll back to a previously working state.\n",
      "84 | Chapter 2: End-to-End Machine Learning Project\n",
      "Try It Out!\n",
      "Hopefully this chapter gave you a good idea of what a Machine Learning project\n",
      "looks like, and showed you some of the tools you can use to train a great system. As\n",
      "you can see, much of the work is in the data preparation step, building monitoring\n",
      "tools, setting up human evaluation pipelines, and automating regular model training.\n",
      "The Machine Learning algorithms are also important, of course, but it is probably\n",
      "preferable to be comfortable with the overall process and know three or four algo‐\n",
      "rithms well rather than to spend all your time exploring advanced algorithms and not\n",
      "enough time on the overall process.\n",
      "So, if you have not already done so, now is a good time to pick up a laptop, select a\n",
      "dataset that you are interested in, and try to go through the whole process from A to\n",
      "Z. A good place to start is on a competition website such as http://kaggle.com/ : you\n",
      "will have a dataset to play with, a clear goal, and people to share the experience with.\n",
      "Exercises\n",
      "Using this chapter’s housing dataset:\n",
      "1.Try a Support Vector Machine regressor ( sklearn.svm.SVR ), with various hyper‐\n",
      "parameters such as kernel=\"linear\"  (with various values for the C hyperpara‐\n",
      "meter) or kernel=\"rbf\"  (with various values for the C and gamma\n",
      "hyperparameters). Don’t worry about what these hyperparameters mean for now.\n",
      "How does the best SVR predictor perform?\n",
      "2.Try replacing GridSearchCV  with RandomizedSearchCV .\n",
      "3.Try adding a transformer in the preparation pipeline to select only the most\n",
      "important attributes.\n",
      "4.Try creating a single pipeline that does the full data preparation plus the final\n",
      "prediction.\n",
      "5.Automatically explore some preparation options using GridSearchCV .\n",
      "Solutions to these exercises are available in the online Jupyter notebooks at https://\n",
      "github.com/ageron/handson-ml2 .\n",
      "Try It Out! | 85\n",
      "\n",
      "1By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data .\n",
      "CHAPTER 3\n",
      "Classification\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 3 in the final\n",
      "release of the book.\n",
      "In Chapter 1  we mentioned that the most common supervised learning tasks are\n",
      "regression (predicting values) and classification (predicting classes). In Chapter 2  we\n",
      "explored a regression task, predicting housing values, using various algorithms such\n",
      "as Linear Regression, Decision Trees, and Random Forests (which will be explained\n",
      "in further detail in later chapters). Now we will turn our attention to classification\n",
      "systems.\n",
      "MNIST\n",
      "In this chapter, we will be using the MNIST dataset, which is a set of 70,000 small\n",
      "images of digits handwritten by high school students and employees of the US Cen‐\n",
      "sus Bureau. Each image is labeled with the digit it represents. This set has been stud‐\n",
      "ied so much that it is often called the “Hello World” of Machine Learning: whenever\n",
      "people come up with a new classification algorithm, they are curious to see how it\n",
      "will perform on MNIST. Whenever someone learns Machine Learning, sooner or\n",
      "later they tackle MNIST.\n",
      "Scikit-Learn provides many helper functions to download popular datasets. MNIST is\n",
      "one of them. The following code fetches the MNIST dataset:1\n",
      "87\n",
      ">>> from sklearn.datasets  import fetch_openml\n",
      ">>> mnist = fetch_openml ('mnist_784' , version=1)\n",
      ">>> mnist.keys()\n",
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'details',\n",
      "           'categories', 'url'])\n",
      "Datasets loaded by Scikit-Learn generally have a similar dictionary structure includ‐\n",
      "ing:\n",
      "•A DESCR  key describing the dataset\n",
      "•A data  key containing an array with one row per instance and one column per\n",
      "feature\n",
      "•A target  key containing an array with the labels\n",
      "Let’s look at these arrays:\n",
      ">>> X, y = mnist[\"data\"], mnist[\"target\" ]\n",
      ">>> X.shape\n",
      "(70000, 784)\n",
      ">>> y.shape\n",
      "(70000,)\n",
      "There are 70,000 images, and each image has 784 features. This is because each image\n",
      "is 28×28 pixels, and each feature simply represents one pixel’s intensity, from 0\n",
      "(white) to 255 (black). Let’s take a peek at one digit from the dataset. All you need to\n",
      "do is grab an instance’s feature vector, reshape it to a 28×28 array, and display it using\n",
      "Matplotlib’s imshow()  function:\n",
      "import matplotlib  as mpl\n",
      "import matplotlib.pyplot  as plt\n",
      "some_digit  = X[0]\n",
      "some_digit_image  = some_digit .reshape(28, 28)\n",
      "plt.imshow(some_digit_image , cmap = mpl.cm.binary, interpolation =\"nearest\" )\n",
      "plt.axis(\"off\")\n",
      "plt.show()\n",
      "This looks like a 5, and indeed that’s what the label tells us:\n",
      "88 | Chapter 3: Classification\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> y[0]\n",
      "'5'\n",
      "Note that the label is a string. We prefer numbers, so let’s cast y to integers:\n",
      ">>> y = y.astype(np.uint8)\n",
      "Figure 3-1  shows a few more images from the MNIST dataset to give you a feel for\n",
      "the complexity of the classification task.\n",
      "Figure 3-1. A few digits from the MNIST dataset\n",
      "But wait! Y ou should always create a test set and set it aside before inspecting the data\n",
      "closely. The MNIST dataset is actually already split into a training set (the first 60,000\n",
      "images) and a test set (the last 10,000 images):\n",
      "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
      "The training set is already shuffled for us, which is good as this guarantees that all\n",
      "cross-validation folds will be similar (you don’t want one fold to be missing some dig‐\n",
      "its). Moreover, some learning algorithms are sensitive to the order of the training\n",
      "MNIST | 89\n",
      "2Shuffling may be a bad idea in some contexts—for example, if you are working on time series data (such as\n",
      "stock market prices or weather conditions). We will explore this in the next chapters.\n",
      "instances, and they perform poorly if they get many similar instances in a row. Shuf‐\n",
      "fling the dataset ensures that this won’t happen.2\n",
      "Training a Binary Classifier\n",
      "Let’s simplify the problem for now and only try to identify one digit—for example,\n",
      "the number 5. This “5-detector” will be an example of a binary classifier , capable of\n",
      "distinguishing between just two classes, 5 and not-5. Let’s create the target vectors for\n",
      "this classification task:\n",
      "y_train_5  = (y_train == 5)  # True for all 5s, False for all other digits.\n",
      "y_test_5  = (y_test == 5)\n",
      "Okay, now let’s pick a classifier and train it. A good place to start is with a Stochastic\n",
      "Gradient Descent  (SGD) classifier, using Scikit-Learn’s SGDClassifier  class. This clas‐\n",
      "sifier has the advantage of being capable of handling very large datasets efficiently.\n",
      "This is in part because SGD deals with training instances independently, one at a time\n",
      "(which also makes SGD well suited for online learning ), as we will see later. Let’s create\n",
      "an SGDClassifier  and train it on the whole training set:\n",
      "from sklearn.linear_model  import SGDClassifier\n",
      "sgd_clf = SGDClassifier (random_state =42)\n",
      "sgd_clf.fit(X_train, y_train_5 )\n",
      "The SGDClassifier  relies on randomness during training (hence\n",
      "the name “stochastic”). If you want reproducible results, you\n",
      "should set the random_state  parameter.\n",
      "Now you can use it to detect images of the number 5:\n",
      ">>> sgd_clf.predict([some_digit ])\n",
      "array([ True])\n",
      "The classifier guesses that this image represents a 5 ( True ). Looks like it guessed right\n",
      "in this particular case! Now, let’s evaluate this model’s performance.\n",
      "Performance Measures\n",
      "Evaluating a classifier is often significantly trickier than evaluating a regressor, so we\n",
      "will spend a large part of this chapter on this topic. There are many performance\n",
      "90 | Chapter 3: Classification\n",
      "measures available, so grab another coffee and get ready to learn many new concepts\n",
      "and acronyms!\n",
      "Measuring Accuracy Using Cross-Validation\n",
      "A good way to evaluate a model is to use cross-validation, just as you did in Chap‐\n",
      "ter 2 .\n",
      "Implementing Cross-Validation\n",
      "Occasionally you will need more control over the cross-validation process than what\n",
      "Scikit-Learn provides off-the-shelf. In these cases, you can implement cross-\n",
      "validation yourself; it is actually fairly straightforward. The following code does\n",
      "roughly the same thing as Scikit-Learn’s cross_val_score()  function, and prints the \n",
      "same result:\n",
      "from sklearn.model_selection  import StratifiedKFold\n",
      "from sklearn.base  import clone\n",
      "skfolds = StratifiedKFold (n_splits =3, random_state =42)\n",
      "for train_index , test_index  in skfolds.split(X_train, y_train_5 ):\n",
      "    clone_clf  = clone(sgd_clf)\n",
      "    X_train_folds  = X_train[train_index ]\n",
      "    y_train_folds  = y_train_5 [train_index ]\n",
      "    X_test_fold  = X_train[test_index ]\n",
      "    y_test_fold  = y_train_5 [test_index ]\n",
      "    clone_clf .fit(X_train_folds , y_train_folds )\n",
      "    y_pred = clone_clf .predict(X_test_fold )\n",
      "    n_correct  = sum(y_pred == y_test_fold )\n",
      "    print(n_correct  / len(y_pred))  # prints 0.9502, 0.96565 and 0.96495\n",
      "The StratifiedKFold  class performs stratified sampling (as explained in Chapter 2 )\n",
      "to produce folds that contain a representative ratio of each class. At each iteration the\n",
      "code creates a clone of the classifier, trains that clone on the training folds, and makes\n",
      "predictions on the test fold. Then it counts the number of correct predictions and\n",
      "outputs the ratio of correct predictions.\n",
      "Let’s use the cross_val_score()  function to evaluate your SGDClassifier  model\n",
      "using K-fold cross-validation, with three folds. Remember that K-fold cross-\n",
      "validation means splitting the training set into K-folds (in this case, three), then mak‐\n",
      "ing predictions and evaluating them on each fold using a model trained on the\n",
      "remaining folds (see Chapter 2 ):\n",
      "Performance Measures | 91\n",
      ">>> from sklearn.model_selection  import cross_val_score\n",
      ">>> cross_val_score (sgd_clf, X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\n",
      "array([0.96355, 0.93795, 0.95615])\n",
      "Wow! Above 93% accuracy  (ratio of correct predictions) on all cross-validation folds? \n",
      "This looks amazing, doesn’t it? Well, before you get too excited, let’s look at a very\n",
      "dumb classifier that just classifies every single image in the “not-5” class:\n",
      "from sklearn.base  import BaseEstimator\n",
      "class Never5Classifier (BaseEstimator ):\n",
      "    def fit(self, X, y=None):\n",
      "        pass\n",
      "    def predict(self, X):\n",
      "        return np.zeros((len(X), 1), dtype=bool)\n",
      "Can you guess this model’s accuracy? Let’s find out:\n",
      ">>> never_5_clf  = Never5Classifier ()\n",
      ">>> cross_val_score (never_5_clf , X_train, y_train_5 , cv=3, scoring=\"accuracy\" )\n",
      "array([0.91125, 0.90855, 0.90915])\n",
      "That’s right, it has over 90% accuracy! This is simply because only about 10% of the\n",
      "images are 5s, so if you always guess that an image is not a 5, you will be right about\n",
      "90% of the time. Beats Nostradamus.\n",
      "This demonstrates why accuracy is generally not the preferred performance measure\n",
      "for classifiers, especially when you are dealing with skewed datasets  (i.e., when some\n",
      "classes are much more frequent than others).\n",
      "Confusion Matrix\n",
      "A much better way to evaluate the performance of a classifier is to look at the confu‐\n",
      "sion matrix . The general idea is to count the number of times instances of class A are\n",
      "classified as class B. For example, to know the number of times the classifier confused\n",
      "images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion\n",
      "matrix.\n",
      "To compute the confusion matrix, you first need to have a set of predictions, so they\n",
      "can be compared to the actual targets. Y ou could make predictions on the test set, but\n",
      "let’s keep it untouched for now (remember that you want to use the test set only at the\n",
      "very end of your project, once you have a classifier that you are ready to launch).\n",
      "Instead, you can use the cross_val_predict()  function:\n",
      "from sklearn.model_selection  import cross_val_predict\n",
      "y_train_pred  = cross_val_predict (sgd_clf, X_train, y_train_5 , cv=3)\n",
      "Just like the cross_val_score()  function, cross_val_predict()  performs K-fold\n",
      "cross-validation, but instead of returning the evaluation scores, it returns the predic‐\n",
      "92 | Chapter 3: Classification\n",
      "tions made on each test fold. This means that you get a clean prediction for each\n",
      "instance in the training set (“clean” meaning that the prediction is made by a model\n",
      "that never saw the data during training).\n",
      "Now you are ready to get the confusion matrix using the confusion_matrix()  func‐\n",
      "tion. Just pass it the target classes ( y_train_5 ) and the predicted classes\n",
      "(y_train_pred ):\n",
      ">>> from sklearn.metrics  import confusion_matrix\n",
      ">>> confusion_matrix (y_train_5 , y_train_pred )\n",
      "array([[53057,  1522],\n",
      "       [ 1325,  4096]])\n",
      "Each row in a confusion matrix represents an actual class , while each column repre‐\n",
      "sents a predicted class . The first row of this matrix considers non-5 images (the nega‐\n",
      "tive class ): 53,057 of them were correctly classified as non-5s (they are called true\n",
      "negatives ), while the remaining 1,522 were wrongly classified as 5s ( false positives ).\n",
      "The second row considers the images of 5s (the positive class ): 1,325 were wrongly\n",
      "classified as non-5s ( false negatives ), while the remaining 4,096 were correctly classi‐\n",
      "fied as 5s ( true positives ). A perfect classifier would have only true positives and true\n",
      "negatives, so its confusion matrix would have nonzero values only on its main diago‐\n",
      "nal (top left to bottom right):\n",
      ">>> y_train_perfect_predictions  = y_train_5   # pretend we reached perfection\n",
      ">>> confusion_matrix (y_train_5 , y_train_perfect_predictions )\n",
      "array([[54579,     0],\n",
      "       [    0,  5421]])\n",
      "The confusion matrix gives you a lot of information, but sometimes you may prefer a\n",
      "more concise metric. An interesting one to look at is the accuracy of the positive pre‐\n",
      "dictions; this is called the precision  of the classifier ( Equation 3-1 ).\n",
      "Equation 3-1. Precision\n",
      "precision =TP\n",
      "TP+FP\n",
      "TP is the number of true positives, and FP is the number of false positives.\n",
      "A trivial way to have perfect precision is to make one single positive prediction and\n",
      "ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the\n",
      "classifier would ignore all but one positive instance. So precision is typically used\n",
      "along with another metric named recall , also called sensitivity  or true positive rate\n",
      "Performance Measures | 93\n",
      "(TPR ): this is the ratio of positive instances that are correctly detected by the classifier\n",
      "(Equation 3-2 ).\n",
      "Equation 3-2. Recall\n",
      "recall =TP\n",
      "TP+FN\n",
      "FN is of course the number of false negatives.\n",
      "If you are confused about the confusion matrix, Figure 3-2  may help.\n",
      "Figure 3-2. An illustrated confusion matrix\n",
      "Precision and Recall\n",
      "Scikit-Learn provides several functions to compute classifier metrics, including preci‐\n",
      "sion and recall:\n",
      ">>> from sklearn.metrics  import precision_score , recall_score\n",
      ">>> precision_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1522)\n",
      "0.7290850836596654\n",
      ">>> recall_score (y_train_5 , y_train_pred ) # == 4096 / (4096 + 1325)\n",
      "0.7555801512636044\n",
      "Now your 5-detector does not look as shiny as it did when you looked at its accuracy.\n",
      "When it claims an image represents a 5, it is correct only 72.9% of the time. More‐\n",
      "over, it only detects 75.6% of the 5s.\n",
      "It is often convenient to combine precision and recall into a single metric called the F1\n",
      "score , in particular if you need a simple way to compare two classifiers. The F1 score is \n",
      "the harmonic mean  of precision and recall ( Equation 3-3 ). Whereas the regular mean\n",
      "94 | Chapter 3: Classification\n",
      "treats all values equally, the harmonic mean gives much more weight to low values.\n",
      "As a result, the classifier will only get a high F1 score if both recall and precision are\n",
      "high.\n",
      "Equation 3-3. F1\n",
      "F1=2\n",
      "1\n",
      "precision+1\n",
      "recall= 2 ×precision × recall\n",
      "precision + recall=TP\n",
      "TP+FN+FP\n",
      "2\n",
      "To compute the F1 score, simply call the f1_score()  function:\n",
      ">>> from sklearn.metrics  import f1_score\n",
      ">>> f1_score (y_train_5 , y_train_pred )\n",
      "0.7420962043663375\n",
      "The F1 score favors classifiers that have similar precision and recall. This is not always\n",
      "what you want: in some contexts you mostly care about precision, and in other con‐\n",
      "texts you really care about recall. For example, if you trained a classifier to detect vid‐\n",
      "eos that are safe for kids, you would probably prefer a classifier that rejects many\n",
      "good videos (low recall) but keeps only safe ones (high precision), rather than a clas‐\n",
      "sifier that has a much higher recall but lets a few really bad videos show up in your\n",
      "product (in such cases, you may even want to add a human pipeline to check the clas‐\n",
      "sifier’s video selection). On the other hand, suppose you train a classifier to detect\n",
      "shoplifters on surveillance images: it is probably fine if your classifier has only 30%\n",
      "precision as long as it has 99% recall (sure, the security guards will get a few false\n",
      "alerts, but almost all shoplifters will get caught).\n",
      "Unfortunately, you can’t have it both ways: increasing precision reduces recall, and\n",
      "vice versa. This is called the precision/recall tradeoff .\n",
      "Precision/Recall Tradeoff\n",
      "To understand this tradeoff, let’s look at how the SGDClassifier  makes its classifica‐\n",
      "tion decisions. For each instance, it computes a score based on a decision function , \n",
      "and if that score is greater than a threshold, it assigns the instance to the positive\n",
      "class, or else it assigns it to the negative class. Figure 3-3  shows a few digits positioned\n",
      "from the lowest score on the left to the highest score on the right. Suppose the deci‐\n",
      "sion threshold  is positioned at the central arrow (between the two 5s): you will find 4\n",
      "true positives (actual 5s) on the right of that threshold, and one false positive (actually\n",
      "a 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\n",
      "actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you\n",
      "raise the threshold (move it to the arrow on the right), the false positive (the 6)\n",
      "becomes a true negative, thereby increasing precision (up to 100% in this case), but\n",
      "one true positive becomes a false negative, decreasing recall down to 50%. Conversely,\n",
      "lowering the threshold increases recall and reduces precision.\n",
      "Performance Measures | 95\n",
      "Figure 3-3. Decision threshold and precision/recall tradeoff\n",
      "Scikit-Learn does not let you set the threshold directly, but it does give you access to\n",
      "the decision scores that it uses to make predictions. Instead of calling the classifier’s\n",
      "predict()  method, you can call its decision_function()  method, which returns a\n",
      "score for each instance, and then make predictions based on those scores using any\n",
      "threshold you want:\n",
      ">>> y_scores  = sgd_clf.decision_function ([some_digit ])\n",
      ">>> y_scores\n",
      "array([2412.53175101])\n",
      ">>> threshold  = 0\n",
      ">>> y_some_digit_pred  = (y_scores  > threshold )\n",
      "array([ True])\n",
      "The SGDClassifier  uses a threshold equal to 0, so the previous code returns the same\n",
      "result as the predict()  method (i.e., True ). Let’s raise the threshold:\n",
      ">>> threshold  = 8000\n",
      ">>> y_some_digit_pred  = (y_scores  > threshold )\n",
      ">>> y_some_digit_pred\n",
      "array([False])\n",
      "This confirms that raising the threshold decreases recall. The image actually repre‐\n",
      "sents a 5, and the classifier detects it when the threshold is 0, but it misses it when the\n",
      "threshold is increased to 8,000.\n",
      "Now how do you decide which threshold to use? For this you will first need to get the\n",
      "scores of all instances in the training set using the cross_val_predict()  function\n",
      "again, but this time specifying that you want it to return decision scores instead of\n",
      "predictions:\n",
      "y_scores  = cross_val_predict (sgd_clf, X_train, y_train_5 , cv=3,\n",
      "                             method=\"decision_function\" )\n",
      "Now with these scores you can compute precision and recall for all possible thresh‐\n",
      "olds using the precision_recall_curve()  function:\n",
      "96 | Chapter 3: Classification\n",
      "from sklearn.metrics  import precision_recall_curve\n",
      "precisions , recalls, thresholds  = precision_recall_curve (y_train_5 , y_scores )\n",
      "Finally, you can plot precision and recall as functions of the threshold value using\n",
      "Matplotlib ( Figure 3-4 ):\n",
      "def plot_precision_recall_vs_threshold (precisions , recalls, thresholds ):\n",
      "    plt.plot(thresholds , precisions [:-1], \"b--\", label=\"Precision\" )\n",
      "    plt.plot(thresholds , recalls[:-1], \"g-\", label=\"Recall\" )\n",
      "    [...] # highlight the threshold, add the legend, axis label and grid\n",
      "plot_precision_recall_vs_threshold (precisions , recalls, thresholds )\n",
      "plt.show()\n",
      "Figure 3-4. Precision and recall versus the decision threshold\n",
      "Y ou may wonder why the precision curve is bumpier than the recall\n",
      "curve in Figure 3-4 . The reason is that precision may sometimes go\n",
      "down when you raise the threshold (although in general it will go\n",
      "up). To understand why, look back at Figure 3-3  and notice what\n",
      "happens when you start from the central threshold and move it just\n",
      "one digit to the right: precision goes from 4/5 (80%) down to 3/4\n",
      "(75%). On the other hand, recall can only go down when the thres‐\n",
      "hold is increased, which explains why its curve looks smooth.\n",
      "Another way to select a good precision/recall tradeoff is to plot precision directly\n",
      "against recall, as shown in Figure 3-5  (the same threshold as earlier is highlighed).\n",
      "Performance Measures | 97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 3-5. Precision versus recall\n",
      "Y ou can see that precision really starts to fall sharply around 80% recall. Y ou will\n",
      "probably want to select a precision/recall tradeoff just before that drop—for example,\n",
      "at around 60% recall. But of course the choice depends on your project.\n",
      "So let’s suppose you decide to aim for 90% precision. Y ou look up the first plot and\n",
      "find that you need to use a threshold of about 8,000. To be more precise you can\n",
      "search for the lowest threshold that gives you at least 90% precision ( np.argmax()\n",
      "will give us the first index of the maximum value, which in this case means the first\n",
      "True  value):\n",
      "threshold_90_precision  = thresholds [np.argmax(precisions  >= 0.90)] # ~7816\n",
      "To make predictions (on the training set for now), instead of calling the classifier’s\n",
      "predict()  method, you can just run this code:\n",
      "y_train_pred_90  = (y_scores  >= threshold_90_precision )\n",
      "Let’s check these predictions’ precision and recall:\n",
      ">>> precision_score (y_train_5 , y_train_pred_90 )\n",
      "0.9000380083618396\n",
      ">>> recall_score (y_train_5 , y_train_pred_90 )\n",
      "0.4368197749492714\n",
      "Great, you have a 90% precision classifier ! As you can see, it is fairly easy to create a\n",
      "classifier with virtually any precision you want: just set a high enough threshold, and\n",
      "you’re done. Hmm, not so fast. A high-precision classifier is not very useful if its \n",
      "recall is too low!\n",
      "98 | Chapter 3: Classification\n",
      "If someone says “let’s reach 99% precision, ” you should ask, “at\n",
      "what recall?”\n",
      "The ROC Curve\n",
      "The receiver operating characteristic  (ROC) curve is another common tool used with\n",
      "binary classifiers. It is very similar to the precision/recall curve, but instead of plot‐\n",
      "ting precision versus recall, the ROC curve plots the true positive rate  (another name\n",
      "for recall) against the false positive rate . The FPR is the ratio of negative instances that\n",
      "are incorrectly classified as positive. It is equal to one minus the true negative rate , \n",
      "which is the ratio of negative instances that are correctly classified as negative. The\n",
      "TNR is also called specificity . Hence the ROC curve plots sensitivity  (recall) versus\n",
      "1 – specificity .\n",
      "To plot the ROC curve, you first need to compute the TPR and FPR for various thres‐\n",
      "hold values, using the roc_curve()  function:\n",
      "from sklearn.metrics  import roc_curve\n",
      "fpr, tpr, thresholds  = roc_curve (y_train_5 , y_scores )\n",
      "Then you can plot the FPR against the TPR using Matplotlib. This code produces the\n",
      "plot in Figure 3-6 :\n",
      "def plot_roc_curve (fpr, tpr, label=None):\n",
      "    plt.plot(fpr, tpr, linewidth =2, label=label)\n",
      "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
      "    [...] # Add axis labels and grid\n",
      "plot_roc_curve (fpr, tpr)\n",
      "plt.show()\n",
      "Performance Measures | 99\n",
      "Figure 3-6. ROC curve\n",
      "Once again there is a tradeoff: the higher the recall (TPR), the more false positives\n",
      "(FPR) the classifier produces. The dotted line represents the ROC curve of a purely\n",
      "random classifier; a good classifier stays as far away from that line as possible (toward\n",
      "the top-left corner).\n",
      "One way to compare classifiers is to measure the area under the curve  (AUC). A per‐\n",
      "fect classifier will have a ROC AUC  equal to 1, whereas a purely random classifier will\n",
      "have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC\n",
      "AUC:\n",
      ">>> from sklearn.metrics  import roc_auc_score\n",
      ">>> roc_auc_score (y_train_5 , y_scores )\n",
      "0.9611778893101814\n",
      "Since the ROC curve is so similar to the precision/recall (or PR)\n",
      "curve, you may wonder how to decide which one to use. As a rule\n",
      "of thumb, you should prefer the PR curve whenever the positive\n",
      "class is rare or when you care more about the false positives than\n",
      "the false negatives, and the ROC curve otherwise. For example,\n",
      "looking at the previous ROC curve (and the ROC AUC score), you\n",
      "may think that the classifier is really good. But this is mostly\n",
      "because there are few positives (5s) compared to the negatives\n",
      "(non-5s). In contrast, the PR curve makes it clear that the classifier\n",
      "has room for improvement (the curve could be closer to the top-\n",
      "right corner).\n",
      "100 | Chapter 3: Classification\n",
      "Let’s train a RandomForestClassifier  and compare its ROC curve and ROC AUC\n",
      "score to the SGDClassifier . First, you need to get scores for each instance in the\n",
      "training set. But due to the way it works (see Chapter 7 ), the RandomForestClassi\n",
      "fier  class does not have a decision_function()  method. Instead it has a pre\n",
      "dict_proba()  method. Scikit-Learn classifiers generally have one or the other. The\n",
      "predict_proba()  method returns an array containing a row per instance and a col‐\n",
      "umn per class, each containing the probability that the given instance belongs to the\n",
      "given class (e.g., 70% chance that the image represents a 5):\n",
      "from sklearn.ensemble  import RandomForestClassifier\n",
      "forest_clf  = RandomForestClassifier (random_state =42)\n",
      "y_probas_forest  = cross_val_predict (forest_clf , X_train, y_train_5 , cv=3,\n",
      "                                    method=\"predict_proba\" )\n",
      "But to plot a ROC curve, you need scores, not probabilities. A simple solution is to\n",
      "use the positive class’s probability as the score:\n",
      "y_scores_forest  = y_probas_forest [:, 1]   # score = proba of positive class\n",
      "fpr_forest , tpr_forest , thresholds_forest  = roc_curve (y_train_5 ,y_scores_forest )\n",
      "Now you are ready to plot the ROC curve. It is useful to plot the first ROC curve as\n",
      "well to see how they compare ( Figure 3-7 ):\n",
      "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n",
      "plot_roc_curve (fpr_forest , tpr_forest , \"Random Forest\" )\n",
      "plt.legend(loc=\"lower right\" )\n",
      "plt.show()\n",
      "Figure 3-7. Comparing ROC curves\n",
      "Performance Measures | 101\n",
      "As you can see in Figure 3-7 , the RandomForestClassifier ’s ROC curve looks much\n",
      "better than the SGDClassifier ’s: it comes much closer to the top-left corner. As a\n",
      "result, its ROC AUC score is also significantly better:\n",
      ">>> roc_auc_score (y_train_5 , y_scores_forest )\n",
      "0.9983436731328145\n",
      "Try measuring the precision and recall scores: you should find 99.0% precision and\n",
      "86.6% recall. Not too bad!\n",
      "Hopefully you now know how to train binary classifiers, choose the appropriate met‐\n",
      "ric for your task, evaluate your classifiers using cross-validation, select the precision/\n",
      "recall tradeoff that fits your needs, and compare various models using ROC curves\n",
      "and ROC AUC scores. Now let’s try to detect more than just the 5s.\n",
      "Multiclass Classification\n",
      "Whereas binary classifiers distinguish between two classes, multiclass classifiers  (also\n",
      "called multinomial classifiers ) can distinguish between more than two classes.\n",
      "Some algorithms (such as Random Forest classifiers or naive Bayes classifiers) are\n",
      "capable of handling multiple classes directly. Others (such as Support Vector Machine\n",
      "classifiers or Linear classifiers) are strictly binary classifiers. However, there are vari‐\n",
      "ous strategies that you can use to perform multiclass classification using multiple\n",
      "binary classifiers.\n",
      "For example, one way to create a system that can classify the digit images into 10\n",
      "classes (from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector, a\n",
      "1-detector, a 2-detector, and so on). Then when you want to classify an image, you get\n",
      "the decision score from each classifier for that image and you select the class whose\n",
      "classifier outputs the highest score. This is called the one-versus-all  (OvA) strategy \n",
      "(also called one-versus-the-rest ).\n",
      "Another strategy is to train a binary classifier for every pair of digits: one to distin‐\n",
      "guish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on.\n",
      "This is called the one-versus-one  (OvO) strategy. If there are N classes, you need to\n",
      "train N × (N – 1) / 2 classifiers. For the MNIST problem, this means training 45\n",
      "binary classifiers! When you want to classify an image, you have to run the image\n",
      "through all 45 classifiers and see which class wins the most duels. The main advan‐\n",
      "tage of OvO is that each classifier only needs to be trained on the part of the training\n",
      "set for the two classes that it must distinguish.\n",
      "Some algorithms (such as Support Vector Machine classifiers) scale poorly with the\n",
      "size of the training set, so for these algorithms OvO is preferred since it is faster to\n",
      "train many classifiers on small training sets than training few classifiers on large\n",
      "training sets. For most binary classification algorithms, however, OvA is preferred.\n",
      "102 | Chapter 3: Classification\n",
      "Scikit-Learn detects when you try to use a binary classification algorithm for a multi‐\n",
      "class classification task, and it automatically runs OvA (except for SVM classifiers for\n",
      "which it uses OvO). Let’s try this with the SGDClassifier :\n",
      ">>> sgd_clf.fit(X_train, y_train)  # y_train, not y_train_5\n",
      ">>> sgd_clf.predict([some_digit ])\n",
      "array([5], dtype=uint8)\n",
      "That was easy! This code trains the SGDClassifier  on the training set using the origi‐\n",
      "nal target classes from 0 to 9 ( y_train ), instead of the 5-versus-all target classes\n",
      "(y_train_5 ). Then it makes a prediction (a correct one in this case). Under the hood,\n",
      "Scikit-Learn actually trained 10 binary classifiers, got their decision scores for the\n",
      "image, and selected the class with the highest score.\n",
      "To see that this is indeed the case, you can call the decision_function()  method.\n",
      "Instead of returning just one score per instance, it now returns 10 scores, one per\n",
      "class:\n",
      ">>> some_digit_scores  = sgd_clf.decision_function ([some_digit ])\n",
      ">>> some_digit_scores\n",
      "array([[-15955.22627845, -38080.96296175, -13326.66694897,\n",
      "           573.52692379, -17680.6846644 ,   2412.53175101,\n",
      "        -25526.86498156, -12290.15704709,  -7946.05205023,\n",
      "        -10631.35888549]])\n",
      "The highest score is indeed the one corresponding to class 5:\n",
      ">>> np.argmax(some_digit_scores )\n",
      "5\n",
      ">>> sgd_clf.classes_\n",
      "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\n",
      ">>> sgd_clf.classes_ [5]\n",
      "5\n",
      "When a classifier is trained, it stores the list of target classes in its\n",
      "classes_  attribute, ordered by value. In this case, the index of each\n",
      "class in the classes_  array conveniently matches the class itself\n",
      "(e.g., the class at index 5 happens to be class 5), but in general you\n",
      "won’t be so lucky.\n",
      "If you want to force ScikitLearn to use one-versus-one or one-versus-all, you can use\n",
      "the OneVsOneClassifier  or OneVsRestClassifier  classes. Simply create an instance\n",
      "and pass a binary classifier to its constructor. For example, this code creates a multi‐\n",
      "class classifier using the OvO strategy, based on a SGDClassifier :\n",
      ">>> from sklearn.multiclass  import OneVsOneClassifier\n",
      ">>> ovo_clf = OneVsOneClassifier (SGDClassifier (random_state =42))\n",
      ">>> ovo_clf.fit(X_train, y_train)\n",
      ">>> ovo_clf.predict([some_digit ])\n",
      "Multiclass Classification  | 103\n",
      "array([5], dtype=uint8)\n",
      ">>> len(ovo_clf.estimators_ )\n",
      "45\n",
      "Training a RandomForestClassifier  is just as easy:\n",
      ">>> forest_clf .fit(X_train, y_train)\n",
      ">>> forest_clf .predict([some_digit ])\n",
      "array([5], dtype=uint8)\n",
      "This time Scikit-Learn did not have to run OvA or OvO because Random Forest\n",
      "classifiers  can directly classify instances into multiple classes. Y ou can call\n",
      "predict_proba()  to get the list of probabilities that the classifier assigned to each\n",
      "instance for each class:\n",
      ">>> forest_clf .predict_proba ([some_digit ])\n",
      "array([[0.  , 0.  , 0.01, 0.08, 0.  , 0.9 , 0.  , 0.  , 0.  , 0.01]])\n",
      "Y ou can see that the classifier is fairly confident about its prediction: the 0.9 at the 5th\n",
      "index in the array means that the model estimates a 90% probability that the image\n",
      "represents a 5. It also thinks that the image could instead be a 2, a 3 or a 9, respec‐\n",
      "tively with 1%, 8% and 1% probability.\n",
      "Now of course you want to evaluate these classifiers. As usual, you want to use cross-\n",
      "validation. Let’s evaluate the SGDClassifier ’s accuracy using the cross_val_score()\n",
      "function:\n",
      ">>> cross_val_score (sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\" )\n",
      "array([0.8489802 , 0.87129356, 0.86988048])\n",
      "It gets over 84% on all test folds. If you used a random classifier, you would get 10%\n",
      "accuracy, so this is not such a bad score, but you can still do much better. For exam‐\n",
      "ple, simply scaling the inputs (as discussed in Chapter 2 ) increases accuracy above\n",
      "89%:\n",
      ">>> from sklearn.preprocessing  import StandardScaler\n",
      ">>> scaler = StandardScaler ()\n",
      ">>> X_train_scaled  = scaler.fit_transform (X_train.astype(np.float64))\n",
      ">>> cross_val_score (sgd_clf, X_train_scaled , y_train, cv=3, scoring=\"accuracy\" )\n",
      "array([0.89707059, 0.8960948 , 0.90693604])\n",
      "Error Analysis\n",
      "Of course, if this were a real project, you would follow the steps in your Machine\n",
      "Learning project checklist (see ???): exploring data preparation options, trying out\n",
      "multiple models, shortlisting the best ones and fine-tuning their hyperparameters\n",
      "using GridSearchCV , and automating as much as possible, as you did in the previous\n",
      "chapter. Here, we will assume that you have found a promising model and you want\n",
      "to find ways to improve it. One way to do this is to analyze the types of errors it\n",
      "makes.\n",
      "104 | Chapter 3: Classification\n",
      "First, you can look at the confusion matrix. Y ou need to make predictions using the\n",
      "cross_val_predict()  function, then call the confusion_matrix()  function, just like\n",
      "you did earlier:\n",
      ">>> y_train_pred  = cross_val_predict (sgd_clf, X_train_scaled , y_train, cv=3)\n",
      ">>> conf_mx = confusion_matrix (y_train, y_train_pred )\n",
      ">>> conf_mx\n",
      "array([[5578,    0,   22,    7,    8,   45,   35,    5,  222,    1],\n",
      "       [   0, 6410,   35,   26,    4,   44,    4,    8,  198,   13],\n",
      "       [  28,   27, 5232,  100,   74,   27,   68,   37,  354,   11],\n",
      "       [  23,   18,  115, 5254,    2,  209,   26,   38,  373,   73],\n",
      "       [  11,   14,   45,   12, 5219,   11,   33,   26,  299,  172],\n",
      "       [  26,   16,   31,  173,   54, 4484,   76,   14,  482,   65],\n",
      "       [  31,   17,   45,    2,   42,   98, 5556,    3,  123,    1],\n",
      "       [  20,   10,   53,   27,   50,   13,    3, 5696,  173,  220],\n",
      "       [  17,   64,   47,   91,    3,  125,   24,   11, 5421,   48],\n",
      "       [  24,   18,   29,   67,  116,   39,    1,  174,  329, 5152]])\n",
      "That’s a lot of numbers. It’s often more convenient to look at an image representation\n",
      "of the confusion matrix, using Matplotlib’s matshow()  function:\n",
      "plt.matshow(conf_mx, cmap=plt.cm.gray)\n",
      "plt.show()\n",
      "This confusion matrix looks fairly good, since most images are on the main diagonal,\n",
      "which means that they were classified correctly. The 5s look slightly darker than the\n",
      "other digits, which could mean that there are fewer images of 5s in the dataset or that\n",
      "the classifier does not perform as well on 5s as on other digits. In fact, you can verify\n",
      "that both are the case.\n",
      "Let’s focus the plot on the errors. First, you need to divide each value in the confusion\n",
      "matrix by the number of images in the corresponding class, so you can compare error\n",
      "Error Analysis | 105\n",
      "rates instead of absolute number of errors (which would make abundant classes look\n",
      "unfairly bad):\n",
      "row_sums  = conf_mx.sum(axis=1, keepdims =True)\n",
      "norm_conf_mx  = conf_mx / row_sums\n",
      "Now let’s fill the diagonal with zeros to keep only the errors, and let’s plot the result:\n",
      "np.fill_diagonal (norm_conf_mx , 0)\n",
      "plt.matshow(norm_conf_mx , cmap=plt.cm.gray)\n",
      "plt.show()\n",
      "Now you can clearly see the kinds of errors the classifier makes. Remember that rows\n",
      "represent actual classes, while columns represent predicted classes. The column for\n",
      "class 8 is quite bright, which tells you that many images get misclassified as 8s. How‐\n",
      "ever, the row for class 8 is not that bad, telling you that actual 8s in general get prop‐\n",
      "erly classified as 8s. As you can see, the confusion matrix is not necessarily\n",
      "symmetrical. Y ou can also see that 3s and 5s often get confused (in both directions).\n",
      "Analyzing the confusion matrix can often give you insights on ways to improve your\n",
      "classifier. Looking at this plot, it seems that your efforts should be spent on reducing\n",
      "the false 8s. For example, you could try to gather more training data for digits that\n",
      "look like 8s (but are not) so the classifier can learn to distinguish them from real 8s.\n",
      "Or you could engineer new features that would help the classifier—for example, writ‐\n",
      "ing an algorithm to count the number of closed loops (e.g., 8 has two, 6 has one, 5 has\n",
      "none). Or you could preprocess the images (e.g., using Scikit-Image, Pillow, or\n",
      "OpenCV) to make some patterns stand out more, such as closed loops.\n",
      "Analyzing individual errors can also be a good way to gain insights on what your\n",
      "classifier is doing and why it is failing, but it is more difficult and time-consuming.\n",
      "106 | Chapter 3: Classification\n",
      "3But remember that our brain is a fantastic pattern recognition system, and our visual system does a lot of\n",
      "complex preprocessing before any information reaches our consciousness, so the fact that it feels simple does\n",
      "not mean that it is.For example, let’s plot examples of 3s and 5s (the plot_digits()  function just uses\n",
      "Matplotlib’s imshow()  function; see this chapter’s Jupyter notebook for details):\n",
      "cl_a, cl_b = 3, 5\n",
      "X_aa = X_train[(y_train == cl_a) & (y_train_pred  == cl_a)]\n",
      "X_ab = X_train[(y_train == cl_a) & (y_train_pred  == cl_b)]\n",
      "X_ba = X_train[(y_train == cl_b) & (y_train_pred  == cl_a)]\n",
      "X_bb = X_train[(y_train == cl_b) & (y_train_pred  == cl_b)]\n",
      "plt.figure(figsize=(8,8))\n",
      "plt.subplot(221); plot_digits (X_aa[:25], images_per_row =5)\n",
      "plt.subplot(222); plot_digits (X_ab[:25], images_per_row =5)\n",
      "plt.subplot(223); plot_digits (X_ba[:25], images_per_row =5)\n",
      "plt.subplot(224); plot_digits (X_bb[:25], images_per_row =5)\n",
      "plt.show()\n",
      "The two 5×5 blocks on the left show digits classified as 3s, and the two 5×5 blocks on\n",
      "the right show images classified as 5s. Some of the digits that the classifier gets wrong\n",
      "(i.e., in the bottom-left and top-right blocks) are so badly written that even a human\n",
      "would have trouble classifying them (e.g., the 5 on the 1st row and 2nd column truly\n",
      "looks like a badly written 3). However, most misclassified images seem like obvious\n",
      "errors to us, and it’s hard to understand why the classifier made the mistakes it did.3\n",
      "The reason is that we used a simple SGDClassifier , which is a linear model. All it\n",
      "does is assign a weight per class to each pixel, and when it sees a new image it just\n",
      "sums up the weighted pixel intensities to get a score for each class. So since 3s and 5s\n",
      "differ only by a few pixels, this model will easily confuse them.\n",
      "Error Analysis | 107\n",
      "The main difference between 3s and 5s is the position of the small line that joins the\n",
      "top line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\n",
      "the classifier might classify it as a 5, and vice versa. In other words, this classifier is\n",
      "quite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\n",
      "would be to preprocess the images to ensure that they are well centered and not too\n",
      "rotated. This will probably help reduce other errors as well.\n",
      "Multilabel Classification\n",
      "Until now each instance has always been assigned to just one class. In some cases you\n",
      "may want your classifier to output multiple classes for each instance. For example,\n",
      "consider a face-recognition classifier: what should it do if it recognizes several people\n",
      "on the same picture? Of course it should attach one tag per person it recognizes. Say\n",
      "the classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then\n",
      "when it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning\n",
      "“ Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple\n",
      "binary tags is called a multilabel classification  system.\n",
      "We won’t go into face recognition just yet, but let’s look at a simpler example, just for\n",
      "illustration purposes:\n",
      "from sklearn.neighbors  import KNeighborsClassifier\n",
      "y_train_large  = (y_train >= 7)\n",
      "y_train_odd  = (y_train % 2 == 1)\n",
      "y_multilabel  = np.c_[y_train_large , y_train_odd ]\n",
      "knn_clf = KNeighborsClassifier ()\n",
      "knn_clf.fit(X_train, y_multilabel )\n",
      "This code creates a y_multilabel  array containing two target labels for each digit\n",
      "image: the first indicates whether or not the digit is large (7, 8, or 9) and the second\n",
      "indicates whether or not it is odd. The next lines create a KNeighborsClassifier  \n",
      "instance (which supports multilabel classification, but not all classifiers do) and we\n",
      "train it using the multiple targets array. Now you can make a prediction, and notice\n",
      "that it outputs two labels:\n",
      ">>> knn_clf.predict([some_digit ])\n",
      "array([[False,  True]])\n",
      "And it gets it right! The digit 5 is indeed not large ( False ) and odd ( True ).\n",
      "There are many ways to evaluate a multilabel classifier, and selecting the right metric\n",
      "really depends on your project. For example, one approach is to measure the F1 score\n",
      "for each individual label (or any other binary classifier metric discussed earlier), then\n",
      "simply compute the average score. This code computes the average F1 score across all\n",
      "labels:\n",
      "108 | Chapter 3: Classification\n",
      "4Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\n",
      "more details.\n",
      ">>> y_train_knn_pred  = cross_val_predict (knn_clf, X_train, y_multilabel , cv=3)\n",
      ">>> f1_score (y_multilabel , y_train_knn_pred , average=\"macro\")\n",
      "0.976410265560605\n",
      "This assumes that all labels are equally important, which may not be the case. In par‐\n",
      "ticular, if you have many more pictures of Alice than of Bob or Charlie, you may want\n",
      "to give more weight to the classifier’s score on pictures of Alice. One simple option is\n",
      "to give each label a weight equal to its support  (i.e., the number of instances with that\n",
      "target label). To do this, simply set average=\"weighted\"  in the preceding code.4\n",
      "Multioutput Classification\n",
      "The last type of classification task we are going to discuss here is called multioutput-\n",
      "multiclass classification  (or simply multioutput classification ). It is simply a generaliza‐\n",
      "tion of multilabel classification where each label can be multiclass (i.e., it can have\n",
      "more than two possible values).\n",
      "To illustrate this, let’s build a system that removes noise from images. It will take as\n",
      "input a noisy digit image, and it will (hopefully) output a clean digit image, repre‐\n",
      "sented as an array of pixel intensities, just like the MNIST images. Notice that the\n",
      "classifier’s output is multilabel (one label per pixel) and each label can have multiple\n",
      "values (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\n",
      "classification system.\n",
      "The line between classification and regression is sometimes blurry,\n",
      "such as in this example. Arguably, predicting pixel intensity is more\n",
      "akin to regression than to classification. Moreover, multioutput\n",
      "systems are not limited to classification tasks; you could even have\n",
      "a system that outputs multiple labels per instance, including both\n",
      "class labels and value labels.\n",
      "Let’s start by creating the training and test sets by taking the MNIST images and\n",
      "adding noise to their pixel intensities using NumPy’s randint()  function. The target\n",
      "images will be the original images:\n",
      "noise = np.random.randint(0, 100, (len(X_train), 784))\n",
      "X_train_mod  = X_train + noise\n",
      "noise = np.random.randint(0, 100, (len(X_test), 784))\n",
      "X_test_mod  = X_test + noise\n",
      "y_train_mod  = X_train\n",
      "y_test_mod  = X_test\n",
      "Multioutput Classification  | 109\n",
      "5Y ou can use the shift()  function from the scipy.ndimage.interpolation  module. For example,\n",
      "shift(image, [2, 1], cval=0)  shifts the image 2 pixels down and 1 pixel to the right.Let’s take a peek at an image from the test set (yes, we’re snooping on the test data, so\n",
      "you should be frowning right now):\n",
      "On the left is the noisy input image, and on the right is the clean target image. Now\n",
      "let’s train the classifier and make it clean this image:\n",
      "knn_clf.fit(X_train_mod , y_train_mod )\n",
      "clean_digit  = knn_clf.predict([X_test_mod [some_index ]])\n",
      "plot_digit (clean_digit )\n",
      "Looks close enough to the target! This concludes our tour of classification. Hopefully\n",
      "you should now know how to select good metrics for classification tasks, pick the\n",
      "appropriate precision/recall tradeoff, compare classifiers, and more generally build\n",
      "good classification systems for a variety of tasks.\n",
      "Exercises\n",
      "1.Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\n",
      "on the test set. Hint: the KNeighborsClassifier  works quite well for this task;\n",
      "you just need to find good hyperparameter values (try a grid search on the\n",
      "weights  and n_neighbors  hyperparameters).\n",
      "2.Write a function that can shift an MNIST image in any direction (left, right, up,\n",
      "or down) by one pixel.5 Then, for each image in the training set, create four shif‐\n",
      "110 | Chapter 3: Classification\n",
      "ted copies (one per direction) and add them to the training set. Finally, train your\n",
      "best model on this expanded training set and measure its accuracy on the test set.\n",
      "Y ou should observe that your model performs even better now! This technique of\n",
      "artificially growing the training set is called data augmentation  or training set\n",
      "expansion .\n",
      "3.Tackle the Titanic  dataset. A great place to start is on Kaggle .\n",
      "4.Build a spam classifier (a more challenging exercise):\n",
      "•Download examples of spam and ham from Apache SpamAssassin’s public\n",
      "datasets .\n",
      "•Unzip the datasets and familiarize yourself with the data format.\n",
      "•Split the datasets into a training set and a test set.\n",
      "•Write a data preparation pipeline to convert each email into a feature vector.\n",
      "Y our preparation pipeline should transform an email into a (sparse) vector\n",
      "indicating the presence or absence of each possible word. For example, if all\n",
      "emails only ever contain four words, “Hello, ” “how, ” “are, ” “you, ” then the email\n",
      "“Hello you Hello Hello you” would be converted into a vector [1, 0, 0, 1]\n",
      "(meaning [“Hello” is present, “how” is absent, “are” is absent, “you” is\n",
      "present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of\n",
      "each word.\n",
      "•Y ou may want to add hyperparameters to your preparation pipeline to control\n",
      "whether or not to strip off email headers, convert each email to lowercase,\n",
      "remove punctuation, replace all URLs with “URL, ” replace all numbers with\n",
      "“NUMBER, ” or even perform stemming  (i.e., trim off word endings; there are\n",
      "Python libraries available to do this).\n",
      "•Then try out several classifiers and see if you can build a great spam classifier,\n",
      "with both high recall and high precision.\n",
      "Solutions to these exercises are available in the online Jupyter notebooks at https://\n",
      "github.com/ageron/handson-ml2 .\n",
      "Exercises | 111\n",
      "\n",
      "CHAPTER 4\n",
      "Training Models\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 4 in the final\n",
      "release of the book.\n",
      "So far we have treated Machine Learning models and their training algorithms mostly\n",
      "like black boxes. If you went through some of the exercises in the previous chapters,\n",
      "you may have been surprised by how much you can get done without knowing any‐\n",
      "thing about what’s under the hood: you optimized a regression system, you improved\n",
      "a digit image classifier, and you even built a spam classifier from scratch—all this\n",
      "without knowing how they actually work. Indeed, in many situations you don’t really\n",
      "need to know the implementation details.\n",
      "However, having a good understanding of how things work can help you quickly\n",
      "home in on the appropriate model, the right training algorithm to use, and a good set\n",
      "of hyperparameters for your task. Understanding what’s under the hood will also help\n",
      "you debug issues and perform error analysis more efficiently. Lastly, most of the top‐\n",
      "ics discussed in this chapter will be essential in understanding, building, and training\n",
      "neural networks (discussed in Part II  of this book).\n",
      "In this chapter, we will start by looking at the Linear Regression model, one of the\n",
      "simplest models there is. We will discuss two very different ways to train it:\n",
      "•Using a direct “closed-form” equation that directly computes the model parame‐\n",
      "ters that best fit the model to the training set (i.e., the model parameters that\n",
      "minimize the cost function over the training set).\n",
      "113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That wasn’t too hard! Let’s look at the resulting theta :\n",
      ">>> theta\n",
      "array([[4.21509616],\n",
      "       [2.77011339]])\n",
      "Hey, that’s exactly what the Normal Equation found! Gradient Descent worked per‐\n",
      "fectly. But what if you had used a different learning rate eta? Figure 4-8  shows the\n",
      "first 10 steps of Gradient Descent using three different learning rates (the dashed line\n",
      "represents the starting point).\n",
      "Figure 4-8. Gradient Descent with various learning rates\n",
      "On the left, the learning rate is too low: the algorithm will eventually reach the solu‐\n",
      "tion, but it will take a long time. In the middle, the learning rate looks pretty good: in\n",
      "just a few iterations, it has already converged to the solution. On the right, the learn‐\n",
      "ing rate is too high: the algorithm diverges, jumping all over the place and actually\n",
      "getting further and further away from the solution at every step.\n",
      "To find a good learning rate, you can use grid search (see Chapter 2 ). However, you\n",
      "may want to limit the number of iterations so that grid search can eliminate models\n",
      "that take too long to converge.\n",
      "Y ou may wonder how to set the number of iterations. If it is too low, you will still be\n",
      "far away from the optimal solution when the algorithm stops, but if it is too high, you\n",
      "will waste time while the model parameters do not change anymore. A simple solu‐\n",
      "tion is to set a very large number of iterations but to interrupt the algorithm when the\n",
      "gradient vector becomes tiny—that is, when its norm becomes smaller than a tiny\n",
      "number ϵ (called the tolerance )—because this happens when Gradient Descent has\n",
      "(almost) reached the minimum.\n",
      "Gradient Descent | 125\n",
      "7Out-of-core algorithms are discussed in Chapter 1 .Convergence Rate\n",
      "When the cost function is convex and its slope does not change abruptly (as is the\n",
      "case for the MSE cost function), Batch Gradient Descent with a fixed learning rate\n",
      "will eventually converge to the optimal solution, but you may have to wait a while: it\n",
      "can take O(1/ ϵ) iterations to reach the optimum within a range of ϵ depending on the\n",
      "shape of the cost function. If you divide the tolerance by 10 to have a more precise\n",
      "solution, then the algorithm may have to run about 10 times longer.\n",
      "Stochastic Gradient Descent\n",
      "The main problem with Batch Gradient Descent is the fact that it uses the whole\n",
      "training set to compute the gradients at every step, which makes it very slow when\n",
      "the training set is large. At the opposite extreme, Stochastic Gradient Descent  just\n",
      "picks a random instance in the training set at every step and computes the gradients\n",
      "based only on that single instance. Obviously this makes the algorithm much faster\n",
      "since it has very little data to manipulate at every iteration. It also makes it possible to\n",
      "train on huge training sets, since only one instance needs to be in memory at each\n",
      "iteration (SGD can be implemented as an out-of-core algorithm.7)\n",
      "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\n",
      "less regular than Batch Gradient Descent: instead of gently decreasing until it reaches\n",
      "the minimum, the cost function will bounce up and down, decreasing only on aver‐\n",
      "age. Over time it will end up very close to the minimum, but once it gets there it will\n",
      "continue to bounce around, never settling down (see Figure 4-9 ). So once the algo‐\n",
      "rithm stops, the final parameter values are good, but not optimal.\n",
      "Figure 4-9. Stochastic Gradient Descent\n",
      "126 | Chapter 4: Training Models\n",
      "When the cost function is very irregular (as in Figure 4-6 ), this can actually help the\n",
      "algorithm jump out of local minima, so Stochastic Gradient Descent has a better\n",
      "chance of finding the global minimum than Batch Gradient Descent does.\n",
      "Therefore randomness is good to escape from local optima, but bad because it means\n",
      "that the algorithm can never settle at the minimum. One solution to this dilemma is\n",
      "to gradually reduce the learning rate. The steps start out large (which helps make\n",
      "quick progress and escape local minima), then get smaller and smaller, allowing the\n",
      "algorithm to settle at the global minimum. This process is akin to simulated anneal‐\n",
      "ing, an algorithm inspired from the process of annealing in metallurgy where molten\n",
      "metal is slowly cooled down. The function that determines the learning rate at each\n",
      "iteration is called the learning schedule . If the learning rate is reduced too quickly, you\n",
      "may get stuck in a local minimum, or even end up frozen halfway to the minimum. If\n",
      "the learning rate is reduced too slowly, you may jump around the minimum for a\n",
      "long time and end up with a suboptimal solution if you halt training too early.\n",
      "This code implements Stochastic Gradient Descent using a simple learning schedule:\n",
      "n_epochs  = 50\n",
      "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
      "def learning_schedule (t):\n",
      "    return t0 / (t + t1)\n",
      "theta = np.random.randn(2,1)  # random initialization\n",
      "for epoch in range(n_epochs ):\n",
      "    for i in range(m):\n",
      "        random_index  = np.random.randint(m)\n",
      "        xi = X_b[random_index :random_index +1]\n",
      "        yi = y[random_index :random_index +1]\n",
      "        gradients  = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
      "        eta = learning_schedule (epoch * m + i)\n",
      "        theta = theta - eta * gradients\n",
      "By convention we iterate by rounds of m iterations; each round is called an epoch . \n",
      "While the Batch Gradient Descent code iterated 1,000 times through the whole train‐\n",
      "ing set, this code goes through the training set only 50 times and reaches a fairly good\n",
      "solution:\n",
      ">>> theta\n",
      "array([[4.21076011],\n",
      "       [2.74856079]])\n",
      "Figure 4-10  shows the first 20 steps of training (notice how irregular the steps are).\n",
      "Gradient Descent | 127\n",
      "Figure 4-10. Stochastic Gradient Descent first 20 steps\n",
      "Note that since instances are picked randomly, some instances may be picked several\n",
      "times per epoch while others may not be picked at all. If you want to be sure that the\n",
      "algorithm goes through every instance at each epoch, another approach is to shuffle\n",
      "the training set (making sure to shuffle the input features and the labels jointly), then\n",
      "go through it instance by instance, then shuffle it again, and so on. However, this gen‐\n",
      "erally converges more slowly.\n",
      "When using Stochastic Gradient Descent, the training instances\n",
      "must be independent and identically distributed (IID), to ensure\n",
      "that the parameters get pulled towards the global optimum, on\n",
      "average. A simple way to ensure this is to shuffle the instances dur‐\n",
      "ing training (e.g., pick each instance randomly, or shuffle the train‐\n",
      "ing set at the beginning of each epoch). If you do not do this, for\n",
      "example if the instances are sorted by label, then SGD will start by\n",
      "optimizing for one label, then the next, and so on, and it will not\n",
      "settle close to the global minimum.\n",
      "To perform Linear Regression using SGD with Scikit-Learn, you can use the SGDRe\n",
      "gressor  class, which defaults to optimizing the squared error cost function. The fol‐\n",
      "lowing code runs for maximum 1000 epochs ( max_iter=1000 ) or until the loss drops\n",
      "by less than 1e-3 during one epoch ( tol=1e-3 ), starting with a learning rate of 0.1\n",
      "(eta0=0.1 ), using the default learning schedule (different from the preceding one),\n",
      "and it does not use any regularization ( penalty=None ; more details on this shortly):\n",
      "from sklearn.linear_model  import SGDRegressor\n",
      "sgd_reg = SGDRegressor (max_iter =1000, tol=1e-3, penalty=None, eta0=0.1)\n",
      "sgd_reg.fit(X, y.ravel())\n",
      "128 | Chapter 4: Training Models\n",
      "Once again, you find a solution quite close to the one returned by the Normal Equa‐\n",
      "tion:\n",
      ">>> sgd_reg.intercept_ , sgd_reg.coef_\n",
      "(array([4.24365286]), array([2.8250878]))\n",
      "Mini-batch Gradient Descent\n",
      "The last Gradient Descent algorithm we will look at is called Mini-batch Gradient\n",
      "Descent . It is quite simple to understand once you know Batch and Stochastic Gradi‐\n",
      "ent Descent: at each step, instead of computing the gradients based on the full train‐\n",
      "ing set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-\n",
      "batch GD computes the gradients on small random sets of instances called mini-\n",
      "batches . The main advantage of Mini-batch GD over Stochastic GD is that you can\n",
      "get a performance boost from hardware optimization of matrix operations, especially\n",
      "when using GPUs.\n",
      "The algorithm’s progress in parameter space is less erratic than with SGD, especially\n",
      "with fairly large mini-batches. As a result, Mini-batch GD will end up walking\n",
      "around a bit closer to the minimum than SGD. But, on the other hand, it may be\n",
      "harder for it to escape from local minima (in the case of problems that suffer from\n",
      "local minima, unlike Linear Regression as we saw earlier). Figure 4-11  shows the\n",
      "paths taken by the three Gradient Descent algorithms in parameter space during\n",
      "training. They all end up near the minimum, but Batch GD’s path actually stops at the\n",
      "minimum, while both Stochastic GD and Mini-batch GD continue to walk around.\n",
      "However, don’t forget that Batch GD takes a lot of time to take each step, and Stochas‐\n",
      "tic GD and Mini-batch GD would also reach the minimum if you used a good learn‐\n",
      "ing schedule.\n",
      "Figure 4-11. Gradient Descent paths in parameter space\n",
      "Gradient Descent | 129\n",
      "8While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\n",
      "used to train many other models, as we will see.\n",
      "9A quadratic equation is of the form y = ax2 + bx + c.\n",
      "Let’s compare the algorithms we’ve discussed so far for Linear Regression8 (recall that\n",
      "m is the number of training instances and n is the number of features); see Table 4-1 .\n",
      "Table 4-1. Comparison of algorithms for Linear Regression\n",
      "Algorithm Large m Out-of-core support Large n Hyperparams Scaling required Scikit-Learn\n",
      "Normal Equation Fast No Slow 0 No n/a\n",
      "SVD Fast No Slow 0 No LinearRegression\n",
      "Batch GD Slow No Fast 2 Yes SGDRegressor\n",
      "Stochastic GD Fast Yes Fast ≥2 Yes SGDRegressor\n",
      "Mini-batch GD Fast Yes Fast ≥2 Yes SGDRegressor\n",
      "There is almost no difference after training: all these algorithms\n",
      "end up with very similar models and make predictions in exactly \n",
      "the same way.\n",
      "Polynomial Regression\n",
      "What if your data is actually more complex than a simple straight line? Surprisingly,\n",
      "you can actually use a linear model to fit nonlinear data. A simple way to do this is to\n",
      "add powers of each feature as new features, then train a linear model on this extended\n",
      "set of features. This technique is called Polynomial Regression .\n",
      "Let’s look at an example. First, let’s generate some nonlinear data, based on a simple\n",
      "quadratic equation9 (plus some noise; see Figure 4-12 ):\n",
      "m = 100\n",
      "X = 6 * np.random.rand(m, 1) - 3\n",
      "y = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n",
      "130 | Chapter 4: Training Models\n",
      "Figure 4-12. Generated nonlinear and noisy dataset\n",
      "Clearly, a straight line will never fit this data properly. So let’s use Scikit-Learn’s Poly\n",
      "nomialFeatures  class to transform our training data, adding the square (2nd-degree\n",
      "polynomial) of each feature in the training set as new features (in this case there is\n",
      "just one feature):\n",
      ">>> from sklearn.preprocessing  import PolynomialFeatures\n",
      ">>> poly_features  = PolynomialFeatures (degree=2, include_bias =False)\n",
      ">>> X_poly = poly_features .fit_transform (X)\n",
      ">>> X[0]\n",
      "array([-0.75275929])\n",
      ">>> X_poly[0]\n",
      "array([-0.75275929, 0.56664654])\n",
      "X_poly  now contains the original feature of X plus the square of this feature. Now you\n",
      "can fit a LinearRegression  model to this extended training data ( Figure 4-13 ):\n",
      ">>> lin_reg = LinearRegression ()\n",
      ">>> lin_reg.fit(X_poly, y)\n",
      ">>> lin_reg.intercept_ , lin_reg.coef_\n",
      "(array([1.78134581]), array([[0.93366893, 0.56456263]]))\n",
      "Polynomial Regression | 131\n",
      "Figure 4-13. Polynomial Regression model predictions\n",
      "Not bad: the model estimates y= 0 . 56 x12+ 0 . 93 x1+ 1 . 78  when in fact the original\n",
      "function was y= 0 . 5 x12+ 1 . 0 x1+ 2 . 0 + Gaussian noise .\n",
      "Note that when there are multiple features, Polynomial Regression is capable of find‐\n",
      "ing relationships between features (which is something a plain Linear Regression\n",
      "model cannot do). This is made possible by the fact that PolynomialFeatures  also\n",
      "adds all combinations of features up to the given degree. For example, if there were\n",
      "two features a and b, PolynomialFeatures  with degree=3  would not only add the\n",
      "features a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2.\n",
      "PolynomialFeatures(degree=d)  transforms an array containing n\n",
      "features into an array containing n+d!\n",
      "d!n! features, where n! is the\n",
      "factorial  of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinato‐\n",
      "rial explosion of the number of features!\n",
      "Learning Curves\n",
      "If you perform high-degree Polynomial Regression, you will likely fit the training\n",
      "data much better than with plain Linear Regression. For example, Figure 4-14  applies\n",
      "a 300-degree polynomial model to the preceding training data, and compares the\n",
      "result with a pure linear model and a quadratic model (2nd-degree polynomial).\n",
      "Notice how the 300-degree polynomial model wiggles around to get as close as possi‐\n",
      "ble to the training instances.\n",
      "132 | Chapter 4: Training Models\n",
      "Figure 4-14. High-degree Polynomial Regression\n",
      "Of course, this high-degree Polynomial Regression model is severely overfitting the\n",
      "training data, while the linear model is underfitting it. The model that will generalize\n",
      "best in this case is the quadratic model. It makes sense since the data was generated\n",
      "using a quadratic model, but in general you won’t know what function generated the\n",
      "data, so how can you decide how complex your model should be? How can you tell\n",
      "that your model is overfitting or underfitting the data?\n",
      "In Chapter 2  you used cross-validation to get an estimate of a model’s generalization\n",
      "performance. If a model performs well on the training data but generalizes poorly\n",
      "according to the cross-validation metrics, then your model is overfitting. If it per‐\n",
      "forms poorly on both, then it is underfitting. This is one way to tell when a model is\n",
      "too simple or too complex.\n",
      "Another way is to look at the learning curves : these are plots of the model’s perfor‐\n",
      "mance on the training set and the validation set as a function of the training set size\n",
      "(or the training iteration). To generate the plots, simply train the model several times\n",
      "on different sized subsets of the training set. The following code defines a function\n",
      "that plots the learning curves of a model given some training data:\n",
      "from sklearn.metrics  import mean_squared_error\n",
      "from sklearn.model_selection  import train_test_split\n",
      "def plot_learning_curves (model, X, y):\n",
      "    X_train, X_val, y_train, y_val = train_test_split (X, y, test_size =0.2)\n",
      "    train_errors , val_errors  = [], []\n",
      "    for m in range(1, len(X_train)):\n",
      "        model.fit(X_train[:m], y_train[:m])\n",
      "        y_train_predict  = model.predict(X_train[:m])\n",
      "Learning Curves | 133\n",
      "        y_val_predict  = model.predict(X_val)\n",
      "        train_errors .append(mean_squared_error (y_train[:m], y_train_predict ))\n",
      "        val_errors .append(mean_squared_error (y_val, y_val_predict ))\n",
      "    plt.plot(np.sqrt(train_errors ), \"r-+\", linewidth =2, label=\"train\")\n",
      "    plt.plot(np.sqrt(val_errors ), \"b-\", linewidth =3, label=\"val\")\n",
      "Let’s look at the learning curves of the plain Linear Regression model (a straight line;\n",
      "Figure 4-15 ):\n",
      "lin_reg = LinearRegression ()\n",
      "plot_learning_curves (lin_reg, X, y)\n",
      "Figure 4-15. Learning curves\n",
      "This deserves a bit of explanation. First, let’s look at the performance on the training\n",
      "data: when there are just one or two instances in the training set, the model can fit\n",
      "them perfectly, which is why the curve starts at zero. But as new instances are added\n",
      "to the training set, it becomes impossible for the model to fit the training data per‐\n",
      "fectly, both because the data is noisy and because it is not linear at all. So the error on\n",
      "the training data goes up until it reaches a plateau, at which point adding new instan‐\n",
      "ces to the training set doesn’t make the average error much better or worse. Now let’s\n",
      "look at the performance of the model on the validation data. When the model is\n",
      "trained on very few training instances, it is incapable of generalizing properly, which\n",
      "is why the validation error is initially quite big. Then as the model is shown more\n",
      "training examples, it learns and thus the validation error slowly goes down. However,\n",
      "once again a straight line cannot do a good job modeling the data, so the error ends\n",
      "up at a plateau, very close to the other curve.\n",
      "These learning curves are typical of an underfitting model. Both curves have reached\n",
      "a plateau; they are close and fairly high.\n",
      "134 | Chapter 4: Training Models\n",
      "If your model is underfitting the training data, adding more train‐\n",
      "ing examples will not help. Y ou need to use a more complex model\n",
      "or come up with better features.\n",
      "Now let’s look at the learning curves of a 10th-degree polynomial model on the same\n",
      "data ( Figure 4-16 ):\n",
      "from sklearn.pipeline  import Pipeline\n",
      "polynomial_regression  = Pipeline ([\n",
      "        (\"poly_features\" , PolynomialFeatures (degree=10, include_bias =False)),\n",
      "        (\"lin_reg\" , LinearRegression ()),\n",
      "    ])\n",
      "plot_learning_curves (polynomial_regression , X, y)\n",
      "These learning curves look a bit like the previous ones, but there are two very impor‐\n",
      "tant differences:\n",
      "•The error on the training data is much lower than with the Linear Regression\n",
      "model.\n",
      "•There is a gap between the curves. This means that the model performs signifi‐\n",
      "cantly better on the training data than on the validation data, which is the hall‐\n",
      "mark of an overfitting model. However, if you used a much larger training set,\n",
      "the two curves would continue to get closer.\n",
      "Figure 4-16. Learning curves for the polynomial model\n",
      "Learning Curves | 135\n",
      "10This notion of bias is not to be confused with the bias term of linear models.\n",
      "One way to improve an overfitting model is to feed it more training\n",
      "data until the validation error reaches the training error.\n",
      "The Bias/Variance Tradeoff\n",
      "An important theoretical result of statistics and Machine Learning is the fact that a\n",
      "model’s generalization error can be expressed as the sum of three very different\n",
      "errors:\n",
      "Bias\n",
      "This part of the generalization error is due to wrong assumptions, such as assum‐\n",
      "ing that the data is linear when it is actually quadratic. A high-bias model is most\n",
      "likely to underfit the training data.10\n",
      "Variance\n",
      "This part is due to the model’s excessive sensitivity to small variations in the\n",
      "training data. A model with many degrees of freedom (such as a high-degree pol‐\n",
      "ynomial model) is likely to have high variance, and thus to overfit the training\n",
      "data.\n",
      "Irreducible error\n",
      "This part is due to the noisiness of the data itself. The only way to reduce this\n",
      "part of the error is to clean up the data (e.g., fix the data sources, such as broken\n",
      "sensors, or detect and remove outliers).\n",
      "Increasing a model’s complexity will typically increase its variance and reduce its bias.\n",
      "Conversely, reducing a model’s complexity increases its bias and reduces its variance. \n",
      "This is why it is called a tradeoff.\n",
      "Regularized Linear Models\n",
      "As we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\n",
      "model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be\n",
      "for it to overfit the data. For example, a simple way to regularize a polynomial model\n",
      "is to reduce the number of polynomial degrees.\n",
      "For a linear model, regularization is typically achieved by constraining the weights of\n",
      "the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\n",
      "which implement three different ways to constrain the weights.\n",
      "136 | Chapter 4: Training Models\n",
      "11It is common to use the notation J(θ) for cost functions that don’t have a short name; we will often use this\n",
      "notation throughout the rest of this book. The context will make it clear which cost function is being dis‐\n",
      "cussed.\n",
      "12Norms are discussed in Chapter 2 .\n",
      "Ridge Regression\n",
      "Ridge Regression  (also called Tikhonov regularization ) is a regularized version of Lin‐\n",
      "ear Regression: a regularization term  equal to α∑i= 1nθi2 is added to the cost function. \n",
      "This forces the learning algorithm to not only fit the data but also keep the model\n",
      "weights as small as possible. Note that the regularization term should only be added\n",
      "to the cost function during training. Once the model is trained, you want to evaluate\n",
      "the model’s performance using the unregularized performance measure.\n",
      "It is quite common for the cost function used during training to be\n",
      "different from the performance measure used for testing. Apart\n",
      "from regularization, another reason why they might be different is\n",
      "that a good training cost function should have optimization-\n",
      "friendly derivatives, while the performance measure used for test‐\n",
      "ing should be as close as possible to the final objective. A good\n",
      "example of this is a classifier trained using a cost function such as\n",
      "the log loss (discussed in a moment) but evaluated using precision/\n",
      "recall.\n",
      "The hyperparameter α controls how much you want to regularize the model. If α = 0\n",
      "then Ridge Regression is just Linear Regression. If α is very large, then all weights end\n",
      "up very close to zero and the result is a flat line going through the data’s mean. Equa‐\n",
      "tion 4-8  presents the Ridge Regression cost function.11\n",
      "Equation 4-8. Ridge Regression cost function\n",
      "Jθ= MSE θ+α1\n",
      "2∑i= 1nθi2\n",
      "Note that the bias term θ0 is not regularized (the sum starts at i = 1, not 0). If we\n",
      "define w as the vector of feature weights ( θ1 to θn), then the regularization term is\n",
      "simply equal to ½( ∥ w ∥2)2, where ∥ w ∥2 represents the ℓ2 norm of the weight vector.12\n",
      "For Gradient Descent, just add αw to the MSE gradient vector ( Equation 4-6 ).\n",
      "It is important to scale the data (e.g., using a StandardScaler ) \n",
      "before performing Ridge Regression, as it is sensitive to the scale of\n",
      "the input features. This is true of most regularized models.\n",
      "Regularized Linear Models | 137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13A square matrix full of 0s except for 1s on the main diagonal (top-left to bottom-right).Figure 4-17  shows several Ridge models trained on some linear data using different α\n",
      "value. On the left, plain Ridge models are used, leading to linear predictions. On the\n",
      "right, the data is first expanded using PolynomialFeatures(degree=10) , then it is\n",
      "scaled using a StandardScaler , and finally the Ridge models are applied to the result‐\n",
      "ing features: this is Polynomial Regression with Ridge regularization. Note how\n",
      "increasing α leads to flatter (i.e., less extreme, more reasonable) predictions; this\n",
      "reduces the model’s variance but increases its bias.\n",
      "As with Linear Regression, we can perform Ridge Regression either by computing a \n",
      "closed-form equation or by performing Gradient Descent. The pros and cons are the\n",
      "same. Equation 4-9  shows the closed-form solution (where A is the ( n + 1) × ( n + 1)\n",
      "identity matrix13 except with a 0 in the top-left cell, corresponding to the bias term).\n",
      "Figure 4-17. Ridge Regression\n",
      "Equation 4-9. Ridge Regression closed-form solution\n",
      "θ=XTX+αA−1 XT y\n",
      "Here is how to perform Ridge Regression with Scikit-Learn using a closed-form solu‐\n",
      "tion (a variant of Equation 4-9  using a matrix factorization technique by André-Louis\n",
      "Cholesky):\n",
      ">>> from sklearn.linear_model  import Ridge\n",
      ">>> ridge_reg  = Ridge(alpha=1, solver=\"cholesky\" )\n",
      ">>> ridge_reg .fit(X, y)\n",
      "138 | Chapter 4: Training Models\n",
      "14Alternatively you can use the Ridge  class with the \"sag\"  solver. Stochastic Average GD is a variant of SGD.\n",
      "For more details, see the presentation “Minimizing Finite Sums with the Stochastic Average Gradient Algo‐\n",
      "rithm”  by Mark Schmidt et al. from the University of British Columbia.>>> ridge_reg .predict([[1.5]])\n",
      "array([[1.55071465]])\n",
      "And using Stochastic Gradient Descent:14\n",
      ">>> sgd_reg = SGDRegressor (penalty=\"l2\")\n",
      ">>> sgd_reg.fit(X, y.ravel())\n",
      ">>> sgd_reg.predict([[1.5]])\n",
      "array([1.47012588])\n",
      "The penalty  hyperparameter sets the type of regularization term to use. Specifying\n",
      "\"l2\"  indicates that you want SGD to add a regularization term to the cost function \n",
      "equal to half the square of the ℓ2 norm of the weight vector: this is simply Ridge\n",
      "Regression.\n",
      "Lasso Regression\n",
      "Least Absolute Shrinkage and Selection Operator Regression  (simply called Lasso\n",
      "Regression ) is another regularized version of Linear Regression: just like Ridge\n",
      "Regression, it adds a regularization term to the cost function, but it uses the ℓ1 norm\n",
      "of the weight vector instead of half the square of the ℓ2 norm (see Equation 4-10 ).\n",
      "Equation 4-10. Lasso Regression cost function\n",
      "Jθ= MSE θ+α∑i= 1nθi\n",
      "Figure 4-18  shows the same thing as Figure 4-17  but replaces Ridge models with\n",
      "Lasso models and uses smaller α values.\n",
      "Regularized Linear Models | 139\n",
      "Figure 4-18. Lasso Regression\n",
      "An important characteristic of Lasso Regression is that it tends to completely elimi‐\n",
      "nate the weights of the least important features (i.e., set them to zero). For example,\n",
      "the dashed line in the right plot on Figure 4-18  (with α = 10-7) looks quadratic, almost\n",
      "linear: all the weights for the high-degree polynomial features are equal to zero. In\n",
      "other words, Lasso Regression automatically performs feature selection and outputs a\n",
      "sparse model  (i.e., with few nonzero feature weights).\n",
      "Y ou can get a sense of why this is the case by looking at Figure 4-19 : on the top-left\n",
      "plot, the background contours (ellipses) represent an unregularized MSE cost func‐\n",
      "tion ( α = 0), and the white circles show the Batch Gradient Descent path with that\n",
      "cost function. The foreground contours (diamonds) represent the ℓ1 penalty, and the\n",
      "triangles show the BGD path for this penalty only ( α → ∞ ). Notice how the path first\n",
      "reaches θ1 = 0, then rolls down a gutter until it reaches θ2 = 0. On the top-right plot,\n",
      "the contours represent the same cost function plus an ℓ1 penalty with α = 0.5. The\n",
      "global minimum is on the θ2 = 0 axis. BGD first reaches θ2 = 0, then rolls down the\n",
      "gutter until it reaches the global minimum. The two bottom plots show the same\n",
      "thing but uses an ℓ2 penalty instead. The regularized minimum is closer to θ = 0 than\n",
      "the unregularized minimum, but the weights do not get fully eliminated.\n",
      "140 | Chapter 4: Training Models\n",
      "15Y ou can think of a subgradient vector at a nondifferentiable point as an intermediate vector between the gra‐\n",
      "dient vectors around that point.\n",
      "Figure 4-19. Lasso versus Ridge regularization\n",
      "On the Lasso cost function, the BGD path tends to bounce across\n",
      "the gutter toward the end. This is because the slope changes\n",
      "abruptly at θ2 = 0. Y ou need to gradually reduce the learning rate in\n",
      "order to actually converge to the global minimum.\n",
      "The Lasso cost function is not differentiable at θi = 0 (for i = 1, 2, ⋯, n), but Gradient\n",
      "Descent still works fine if you use a subgradient vector  g15 instead when any θi = 0.\n",
      "Equation 4-11  shows a subgradient vector equation you can use for Gradient Descent\n",
      "with the Lasso cost function.\n",
      "Equation 4-11. Lasso Regression subgradient vector\n",
      "gθ,J=∇θMSE θ+αsign θ1\n",
      "sign θ2\n",
      "⋮\n",
      "sign θn   where  sign θi=−1 if  θi< 0\n",
      "0 if  θi= 0\n",
      "+1 if  θi> 0\n",
      "Regularized Linear Models | 141\n",
      "Here is a small Scikit-Learn example using the Lasso  class. Note that you could\n",
      "instead use an SGDRegressor(penalty=\"l1\") .\n",
      ">>> from sklearn.linear_model  import Lasso\n",
      ">>> lasso_reg  = Lasso(alpha=0.1)\n",
      ">>> lasso_reg .fit(X, y)\n",
      ">>> lasso_reg .predict([[1.5]])\n",
      "array([1.53788174])\n",
      "Elastic Net\n",
      "Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The\n",
      "regularization term is a simple mix of both Ridge and Lasso’s regularization terms,\n",
      "and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\n",
      "Regression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12 ).\n",
      "Equation 4-12. Elastic Net cost function\n",
      "Jθ= MSE θ+rα∑i= 1nθi+1 −r\n",
      "2α∑i= 1nθi2\n",
      "So when should you use plain Linear Regression (i.e., without any regularization),\n",
      "Ridge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\n",
      "regularization, so generally you should avoid plain Linear Regression. Ridge is a good\n",
      "default, but if you suspect that only a few features are actually useful, you should pre‐\n",
      "fer Lasso or Elastic Net since they tend to reduce the useless features’ weights down to\n",
      "zero as we have discussed. In general, Elastic Net is preferred over Lasso since Lasso\n",
      "may behave erratically when the number of features is greater than the number of\n",
      "training instances or when several features are strongly correlated.\n",
      "Here is a short example using Scikit-Learn’s ElasticNet  (l1_ratio  corresponds to\n",
      "the mix ratio r):\n",
      ">>> from sklearn.linear_model  import ElasticNet\n",
      ">>> elastic_net  = ElasticNet (alpha=0.1, l1_ratio =0.5)\n",
      ">>> elastic_net .fit(X, y)\n",
      ">>> elastic_net .predict([[1.5]])\n",
      "array([1.54333232])\n",
      "Early Stopping\n",
      "A very different way to regularize iterative learning algorithms such as Gradient\n",
      "Descent is to stop training as soon as the validation error reaches a minimum. This is\n",
      "called early stopping . Figure 4-20  shows a complex model (in this case a high-degree\n",
      "Polynomial Regression model) being trained using Batch Gradient Descent. As the\n",
      "epochs go by, the algorithm learns and its prediction error (RMSE) on the training set\n",
      "naturally goes down, and so does its prediction error on the validation set. However,\n",
      "142 | Chapter 4: Training Models\n",
      "after a while the validation error stops decreasing and actually starts to go back up.\n",
      "This indicates that the model has started to overfit the training data. With early stop‐\n",
      "ping you just stop training as soon as the validation error reaches the minimum. It is\n",
      "such a simple and efficient regularization technique that Geoffrey Hinton called it a\n",
      "“beautiful free lunch. ”\n",
      "Figure 4-20. Early stopping regularization\n",
      "With Stochastic and Mini-batch Gradient Descent, the curves are\n",
      "not so smooth, and it may be hard to know whether you have\n",
      "reached the minimum or not. One solution is to stop only after the\n",
      "validation error has been above the minimum for some time (when\n",
      "you are confident that the model will not do any better), then roll\n",
      "back the model parameters to the point where the validation error\n",
      "was at a minimum.\n",
      "Here is a basic implementation of early stopping:\n",
      "from sklearn.base  import clone\n",
      "# prepare the data\n",
      "poly_scaler  = Pipeline ([\n",
      "        (\"poly_features\" , PolynomialFeatures (degree=90, include_bias =False)),\n",
      "        (\"std_scaler\" , StandardScaler ())\n",
      "    ])\n",
      "X_train_poly_scaled  = poly_scaler .fit_transform (X_train)\n",
      "X_val_poly_scaled  = poly_scaler .transform (X_val)\n",
      "sgd_reg = SGDRegressor (max_iter =1, tol=-np.infty, warm_start =True,\n",
      "                       penalty=None, learning_rate =\"constant\" , eta0=0.0005)\n",
      "Regularized Linear Models | 143\n",
      "minimum_val_error  = float(\"inf\")\n",
      "best_epoch  = None\n",
      "best_model  = None\n",
      "for epoch in range(1000):\n",
      "    sgd_reg.fit(X_train_poly_scaled , y_train)  # continues where it left off\n",
      "    y_val_predict  = sgd_reg.predict(X_val_poly_scaled )\n",
      "    val_error  = mean_squared_error (y_val, y_val_predict )\n",
      "    if val_error  < minimum_val_error :\n",
      "        minimum_val_error  = val_error\n",
      "        best_epoch  = epoch\n",
      "        best_model  = clone(sgd_reg)\n",
      "Note that with warm_start=True , when the fit()  method is called, it just continues\n",
      "training where it left off instead of restarting from scratch.\n",
      "Logistic Regression\n",
      "As we discussed in Chapter 1 , some regression algorithms can be used for classifica‐\n",
      "tion as well (and vice versa). Logistic Regression  (also called Logit Regression ) is com‐\n",
      "monly used to estimate the probability that an instance belongs to a particular class\n",
      "(e.g., what is the probability that this email is spam?). If the estimated probability is\n",
      "greater than 50%, then the model predicts that the instance belongs to that class\n",
      "(called the positive class, labeled “1”), or else it predicts that it does not (i.e., it\n",
      "belongs to the negative class, labeled “0”). This makes it a binary classifier.\n",
      "Estimating Probabilities\n",
      "So how does it work? Just like a Linear Regression model, a Logistic Regression\n",
      "model computes a weighted sum of the input features (plus a bias term), but instead\n",
      "of outputting the result directly like the Linear Regression model does, it outputs the\n",
      "logistic  of this result (see Equation 4-13 ).\n",
      "Equation 4-13. Logistic Regression model estimated probability (vectorized form)\n",
      "p=hθx=σxTθ\n",
      "The logistic—noted σ(·)—is a sigmoid function  (i.e., S-shaped) that outputs a number\n",
      "between 0 and 1. It is defined as shown in Equation 4-14  and Figure 4-21 .\n",
      "Equation 4-14. Logistic function\n",
      "σt=1\n",
      "1 + exp −t\n",
      "144 | Chapter 4: Training Models\n",
      "Figure 4-21. Logistic function\n",
      "Once the Logistic Regression model has estimated the probability p = hθ(x) that an\n",
      "instance x belongs to the positive class, it can make its prediction ŷ easily (see Equa‐\n",
      "tion 4-15 ).\n",
      "Equation 4-15. Logistic Regression model prediction\n",
      "y=0 if p< 0 . 5\n",
      "1 if p≥ 0 . 5\n",
      "Notice that σ(t) < 0.5 when t < 0, and σ(t) ≥ 0.5 when t ≥ 0, so a Logistic Regression\n",
      "model predicts 1 if xT θ is positive, and 0 if it is negative.\n",
      "The score t is often called the logit : this name comes from the fact\n",
      "that the logit function, defined as logit( p) = log( p / (1 - p)), is the\n",
      "inverse of the logistic function. Indeed, if you compute the logit of\n",
      "the estimated probability p, you will find that the result is t. The\n",
      "logit is also called the log-odds , since it is the log of the ratio\n",
      "between the estimated probability for the positive class and the\n",
      "estimated probability for the negative class.\n",
      "Training and Cost Function\n",
      "Good, now you know how a Logistic Regression model estimates probabilities and\n",
      "makes predictions. But how is it trained? The objective of training is to set the param‐\n",
      "eter vector θ so that the model estimates high probabilities for positive instances ( y =\n",
      "1) and low probabilities for negative instances ( y = 0). This idea is captured by the\n",
      "cost function shown in Equation 4-16  for a single training instance x.\n",
      "Equation 4-16. Cost function of a single training instance\n",
      "cθ=−log p if y= 1\n",
      "−log 1 −pif y= 0\n",
      "Logistic Regression | 145\n",
      "This cost function makes sense because – log( t) grows very large when t approaches\n",
      "0, so the cost will be large if the model estimates a probability close to 0 for a positive\n",
      "instance, and it will also be very large if the model estimates a probability close to 1\n",
      "for a negative instance. On the other hand, – log( t) is close to 0 when t is close to 1, so\n",
      "the cost will be close to 0 if the estimated probability is close to 0 for a negative\n",
      "instance or close to 1 for a positive instance, which is precisely what we want.\n",
      "The cost function over the whole training set is simply the average cost over all train‐\n",
      "ing instances. It can be written in a single expression (as you can verify easily), called \n",
      "the log loss , shown in Equation 4-17 .\n",
      "Equation 4-17. Logistic Regression cost function (log loss)\n",
      "Jθ= −1\n",
      "m∑i= 1myilogpi+1 −yilog1 −pi\n",
      "The bad news is that there is no known closed-form equation to compute the value of\n",
      "θ that minimizes this cost function (there is no equivalent of the Normal Equation).\n",
      "But the good news is that this cost function is convex, so Gradient Descent (or any\n",
      "other optimization algorithm) is guaranteed to find the global minimum (if the learn‐\n",
      "ing rate is not too large and you wait long enough). The partial derivatives of the cost\n",
      "function with regards to the jth model parameter θj is given by Equation 4-18 .\n",
      "Equation 4-18. Logistic cost function partial derivatives\n",
      "∂\n",
      "∂θjJθ=1\n",
      "m∑\n",
      "i= 1m\n",
      "σθTxi−yixji\n",
      "This equation looks very much like Equation 4-5 : for each instance it computes the\n",
      "prediction error and multiplies it by the jth feature value, and then it computes the\n",
      "average over all training instances. Once you have the gradient vector containing all\n",
      "the partial derivatives you can use it in the Batch Gradient Descent algorithm. That’s\n",
      "it: you now know how to train a Logistic Regression model. For Stochastic GD you\n",
      "would of course just take one instance at a time, and for Mini-batch GD you would\n",
      "use a mini-batch at a time.\n",
      "Decision Boundaries\n",
      "Let’s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\n",
      "contains the sepal and petal length and width of 150 iris flowers of three different\n",
      "species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see Figure 4-22 ).\n",
      "146 | Chapter 4: Training Models\n",
      "16Photos reproduced from the corresponding Wikipedia pages. Iris-Virginica photo by Frank Mayfield ( Crea‐\n",
      "tive Commons BY-SA 2.0 ), Iris-Versicolor photo by D. Gordon E. Robertson ( Creative Commons BY-SA 3.0 ),\n",
      "and Iris-Setosa photo is public domain.\n",
      "17NumPy’s reshape()  function allows one dimension to be –1, which means “unspecified”: the value is inferred\n",
      "from the length of the array and the remaining dimensions.\n",
      "Figure 4-22. Flowers of three iris plant species16\n",
      "Let’s try to build a classifier to detect the Iris-Virginica type based only on the petal\n",
      "width feature. First let’s load the data:\n",
      ">>> from sklearn import datasets\n",
      ">>> iris = datasets .load_iris ()\n",
      ">>> list(iris.keys())\n",
      "['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']\n",
      ">>> X = iris[\"data\"][:, 3:]  # petal width\n",
      ">>> y = (iris[\"target\" ] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0\n",
      "Now let’s train a Logistic Regression model:\n",
      "from sklearn.linear_model  import LogisticRegression\n",
      "log_reg = LogisticRegression ()\n",
      "log_reg.fit(X, y)\n",
      "Let’s look at the model’s estimated probabilities for flowers with petal widths varying\n",
      "from 0 to 3 cm ( Figure 4-23 )17:\n",
      "X_new = np.linspace (0, 3, 1000).reshape(-1, 1)\n",
      "y_proba = log_reg.predict_proba (X_new)\n",
      "plt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris-Virginica\" )\n",
      "Logistic Regression | 147\n",
      "18It is the the set of points x such that θ0 + θ1x1 + θ2x2 = 0, which defines a straight line.plt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris-Virginica\" )\n",
      "# + more Matplotlib code to make the image look pretty\n",
      "Figure 4-23. Estimated probabilities and decision boundary\n",
      "The petal width of Iris-Virginica flowers (represented by triangles) ranges from 1.4\n",
      "cm to 2.5 cm, while the other iris flowers (represented by squares) generally have a\n",
      "smaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is a bit of over‐\n",
      "lap. Above about 2 cm the classifier is highly confident that the flower is an Iris-\n",
      "Virginica (it outputs a high probability to that class), while below 1 cm it is highly\n",
      "confident that it is not an Iris-Virginica (high probability for the “Not Iris-Virginica”\n",
      "class). In between these extremes, the classifier is unsure. However, if you ask it to\n",
      "predict the class (using the predict()  method rather than the predict_proba()\n",
      "method), it will return whichever class is the most likely. Therefore, there is a decision\n",
      "boundary  at around 1.6 cm where both probabilities are equal to 50%: if the petal\n",
      "width is higher than 1.6 cm, the classifier will predict that the flower is an Iris-\n",
      "Virginica, or else it will predict that it is not (even if it is not very confident):\n",
      ">>> log_reg.predict([[1.7], [1.5]])\n",
      "array([1, 0])\n",
      "Figure 4-24  shows the same dataset but this time displaying two features: petal width\n",
      "and length. Once trained, the Logistic Regression classifier can estimate the probabil‐\n",
      "ity that a new flower is an Iris-Virginica based on these two features. The dashed line\n",
      "represents the points where the model estimates a 50% probability: this is the model’s\n",
      "decision boundary. Note that it is a linear boundary.18 Each parallel line represents the\n",
      "points where the model outputs a specific probability, from 15% (bottom left) to 90%\n",
      "(top right). All the flowers beyond the top-right line have an over 90% chance of\n",
      "being Iris-Virginica according to the model.\n",
      "148 | Chapter 4: Training Models\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 4-24. Linear decision boundary\n",
      "Just like the other linear models, Logistic Regression models can be regularized using \n",
      "ℓ1 or ℓ2 penalties. Scitkit-Learn actually adds an ℓ2 penalty by default.\n",
      "The hyperparameter controlling the regularization strength of a\n",
      "Scikit-Learn LogisticRegression  model is not alpha  (as in other\n",
      "linear models), but its inverse: C. The higher the value of C, the less\n",
      "the model is regularized.\n",
      "Softmax Regression\n",
      "The Logistic Regression model can be generalized to support multiple classes directly,\n",
      "without having to train and combine multiple binary classifiers (as discussed in\n",
      "Chapter 3 ). This is called Softmax  Regression , or Multinomial Logistic Regression .\n",
      "The idea is quite simple: when given an instance x, the Softmax Regression model\n",
      "first computes a score sk(x) for each class k, then estimates the probability of each\n",
      "class by applying the softmax  function  (also called the normalized exponential ) to the\n",
      "scores. The equation to compute sk(x) should look familiar, as it is just like the equa‐\n",
      "tion for Linear Regression prediction (see Equation 4-19 ).\n",
      "Equation 4-19. Softmax  score for class k\n",
      "skx=xTθk\n",
      "Note that each class has its own dedicated parameter vector θ(k). All these vectors are\n",
      "typically stored as rows in a parameter matrix  Θ.\n",
      "Once you have computed the score of every class for the instance x, you can estimate\n",
      "the probability pk that the instance belongs to class k by running the scores through\n",
      "the softmax function ( Equation 4-20 ): it computes the exponential of every score,\n",
      "Logistic Regression | 149\n",
      "then normalizes them (dividing by the sum of all the exponentials). The scores are\n",
      "generally called logits or log-odds (although they are actually unnormalized log-\n",
      "odds).\n",
      "Equation 4-20. Softmax  function\n",
      "pk=σsxk=exp skx\n",
      "∑j= 1Kexp sjx\n",
      "•K is the number of classes.\n",
      "•s(x) is a vector containing the scores of each class for the instance x.\n",
      "•σ(s(x))k is the estimated probability that the instance x belongs to class k given\n",
      "the scores of each class for that instance.\n",
      "Just like the Logistic Regression classifier, the Softmax Regression classifier predicts\n",
      "the class with the highest estimated probability (which is simply the class with the\n",
      "highest score), as shown in Equation 4-21 .\n",
      "Equation 4-21. Softmax  Regression classifier  prediction\n",
      "y= argmax\n",
      "kσsxk= argmax\n",
      "kskx= argmax\n",
      "kθkTx\n",
      "•The argmax  operator returns the value of a variable that maximizes a function. In\n",
      "this equation, it returns the value of k that maximizes the estimated probability\n",
      "σ(s(x))k.\n",
      "The Softmax Regression classifier predicts only one class at a time\n",
      "(i.e., it is multiclass, not multioutput) so it should be used only with\n",
      "mutually exclusive classes such as different types of plants. Y ou\n",
      "cannot use it to recognize multiple people in one picture.\n",
      "Now that you know how the model estimates probabilities and makes predictions,\n",
      "let’s take a look at training. The objective is to have a model that estimates a high\n",
      "probability for the target class (and consequently a low probability for the other\n",
      "classes). Minimizing the cost function shown in Equation 4-22 , called the cross\n",
      "entropy , should lead to this objective because it penalizes the model when it estimates\n",
      "a low probability for a target class. Cross entropy is frequently used to measure how\n",
      "150 | Chapter 4: Training Models\n",
      "well a set of estimated class probabilities match the target classes (we will use it again\n",
      "several times in the following chapters).\n",
      "Equation 4-22. Cross entropy cost function\n",
      "JΘ= −1\n",
      "m∑i= 1m∑k= 1Kykilogpki\n",
      "•yki is the target probability that the ith instance belongs to class k. In general, it is\n",
      "either equal to 1 or 0, depending on whether the instance belongs to the class or\n",
      "not.\n",
      "Notice that when there are just two classes ( K = 2), this cost function is equivalent to\n",
      "the Logistic Regression’s cost function (log loss; see Equation 4-17 ).\n",
      "Cross Entropy\n",
      "Cross entropy originated from information theory. Suppose you want to efficiently\n",
      "transmit information about the weather every day. If there are eight options (sunny,\n",
      "rainy, etc.), you could encode each option using 3 bits since 23 = 8. However, if you\n",
      "think it will be sunny almost every day, it would be much more efficient to code\n",
      "“sunny” on just one bit (0) and the other seven options on 4 bits (starting with a 1).\n",
      "Cross entropy measures the average number of bits you actually send per option. If\n",
      "your assumption about the weather is perfect, cross entropy will just be equal to the\n",
      "entropy of the weather itself (i.e., its intrinsic unpredictability). But if your assump‐\n",
      "tions are wrong (e.g., if it rains often), cross entropy will be greater by an amount \n",
      "called the Kullback–Leibler divergence .\n",
      "The cross entropy between two probability distributions p and q is defined as\n",
      "Hp,q= − ∑xpxlogqx (at least when the distributions are discrete). For more\n",
      "details, check out this video .\n",
      "The gradient vector of this cost function with regards to θ(k) is given by Equation\n",
      "4-23 :\n",
      "Equation 4-23. Cross entropy gradient vector for class k\n",
      "∇\n",
      "θkJΘ=1\n",
      "m∑\n",
      "i= 1m\n",
      "pki−ykixi\n",
      "Now you can compute the gradient vector for every class, then use Gradient Descent\n",
      "(or any other optimization algorithm) to find the parameter matrix Θ that minimizes\n",
      "the cost function.\n",
      "Logistic Regression | 151\n",
      "Let’s use Softmax Regression to classify the iris flowers into all three classes. Scikit-\n",
      "Learn’s LogisticRegression  uses one-versus-all by default when you train it on more\n",
      "than two classes, but you can set the multi_class  hyperparameter to \"multinomial\"\n",
      "to switch it to Softmax Regression instead. Y ou must also specify a solver that sup‐\n",
      "ports Softmax Regression, such as the \"lbfgs\"  solver (see Scikit-Learn’s documenta‐\n",
      "tion for more details). It also applies ℓ2 regularization by default, which you can\n",
      "control using the hyperparameter C.\n",
      "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
      "y = iris[\"target\" ]\n",
      "softmax_reg  = LogisticRegression (multi_class =\"multinomial\" ,solver=\"lbfgs\", C=10)\n",
      "softmax_reg .fit(X, y)\n",
      "So the next time you find an iris with 5 cm long and 2 cm wide petals, you can ask\n",
      "your model to tell you what type of iris it is, and it will answer Iris-Virginica (class 2)\n",
      "with 94.2% probability (or Iris-Versicolor with 5.8% probability):\n",
      ">>> softmax_reg .predict([[5, 2]])\n",
      "array([2])\n",
      ">>> softmax_reg .predict_proba ([[5, 2]])\n",
      "array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])\n",
      "Figure 4-25  shows the resulting decision boundaries, represented by the background\n",
      "colors. Notice that the decision boundaries between any two classes are linear. The\n",
      "figure also shows the probabilities for the Iris-Versicolor class, represented by the\n",
      "curved lines (e.g., the line labeled with 0.450 represents the 45% probability bound‐\n",
      "ary). Notice that the model can predict a class that has an estimated probability below\n",
      "50%. For example, at the point where all decision boundaries meet, all classes have an\n",
      "equal estimated probability of 33%.\n",
      "Figure 4-25. Softmax  Regression decision boundaries\n",
      "152 | Chapter 4: Training Models\n",
      "Exercises\n",
      "1.What Linear Regression training algorithm can you use if you have a training set\n",
      "with millions of features?\n",
      "2.Suppose the features in your training set have very different scales. What algo‐\n",
      "rithms might suffer from this, and how? What can you do about it?\n",
      "3.Can Gradient Descent get stuck in a local minimum when training a Logistic\n",
      "Regression model?\n",
      "4.Do all Gradient Descent algorithms lead to the same model provided you let\n",
      "them run long enough?\n",
      "5.Suppose you use Batch Gradient Descent and you plot the validation error at\n",
      "every epoch. If you notice that the validation error consistently goes up, what is\n",
      "likely going on? How can you fix this?\n",
      "6.Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali‐\n",
      "dation error goes up?\n",
      "7.Which Gradient Descent algorithm (among those we discussed) will reach the\n",
      "vicinity of the optimal solution the fastest? Which will actually converge? How\n",
      "can you make the others converge as well?\n",
      "8.Suppose you are using Polynomial Regression. Y ou plot the learning curves and\n",
      "you notice that there is a large gap between the training error and the validation\n",
      "error. What is happening? What are three ways to solve this?\n",
      "9.Suppose you are using Ridge Regression and you notice that the training error\n",
      "and the validation error are almost equal and fairly high. Would you say that the\n",
      "model suffers from high bias or high variance? Should you increase the regulari‐\n",
      "zation hyperparameter α or reduce it?\n",
      "10.Why would you want to use:\n",
      "•Ridge Regression instead of plain Linear Regression (i.e., without any regulari‐\n",
      "zation)?\n",
      "•Lasso instead of Ridge Regression?\n",
      "•Elastic Net instead of Lasso?\n",
      "11.Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime.\n",
      "Should you implement two Logistic Regression classifiers or one Softmax Regres‐\n",
      "sion classifier?\n",
      "12.Implement Batch Gradient Descent with early stopping for Softmax Regression \n",
      "(without using Scikit-Learn).\n",
      "Solutions to these exercises are available in ???.\n",
      "Exercises | 153\n",
      "\n",
      "CHAPTER 5\n",
      "Support Vector Machines\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 5 in the final\n",
      "release of the book.\n",
      "A Support Vector Machine  (SVM) is a very powerful and versatile Machine Learning\n",
      "model, capable of performing linear or nonlinear classification, regression, and even\n",
      "outlier detection. It is one of the most popular models in Machine Learning, and any‐\n",
      "one interested in Machine Learning should have it in their toolbox. SVMs are partic‐\n",
      "ularly well suited for classification of complex but small- or medium-sized datasets.\n",
      "This chapter will explain the core concepts of SVMs, how to use them, and how they\n",
      "work.\n",
      "Linear SVM Classification\n",
      "The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\n",
      "shows part of the iris dataset that was introduced at the end of Chapter 4 . The two\n",
      "classes can clearly be separated easily with a straight line (they are linearly separable ).\n",
      "The left plot shows the decision boundaries of three possible linear classifiers. The\n",
      "model whose decision boundary is represented by the dashed line is so bad that it\n",
      "does not even separate the classes properly. The other two models work perfectly on\n",
      "this training set, but their decision boundaries come so close to the instances that\n",
      "these models will probably not perform as well on new instances. In contrast, the\n",
      "solid line in the plot on the right represents the decision boundary of an SVM classi‐\n",
      "fier; this line not only separates the two classes but also stays as far away from the\n",
      "closest training instances as possible. Y ou can think of an SVM classifier as fitting the\n",
      "155\n",
      "widest possible street (represented by the parallel dashed lines) between the classes.\n",
      "This is called large margin classification .\n",
      "Figure 5-1. Large margin classification\n",
      "Notice that adding more training instances “off the street” will not affect the decision\n",
      "boundary at all: it is fully determined (or “supported”) by the instances located on the\n",
      "edge of the street. These instances are called the support vectors  (they are circled in\n",
      "Figure 5-1 ).\n",
      "SVMs are sensitive to the feature scales, as you can see in\n",
      "Figure 5-2 : on the left plot, the vertical scale is much larger than the\n",
      "horizontal scale, so the widest possible street is close to horizontal.\n",
      "After feature scaling (e.g., using Scikit-Learn’s StandardScaler ), \n",
      "the decision boundary looks much better (on the right plot).\n",
      "Figure 5-2. Sensitivity to feature scales\n",
      "Soft Margin Classification\n",
      "If we strictly impose that all instances be off the street and on the right side, this is\n",
      "called hard margin classification . There are two main issues with hard margin classifi‐\n",
      "cation. First, it only works if the data is linearly separable, and second it is quite sensi‐\n",
      "tive to outliers. Figure 5-3  shows the iris dataset with just one additional outlier: on\n",
      "the left, it is impossible to find a hard margin, and on the right the decision boundary\n",
      "ends up very different from the one we saw in Figure 5-1  without the outlier, and it\n",
      "will probably not generalize as well.\n",
      "156 | Chapter 5: Support Vector Machines\n",
      "Figure 5-3. Hard margin sensitivity to outliers\n",
      "To avoid these issues it is preferable to use a more flexible model. The objective is to\n",
      "find a good balance between keeping the street as large as possible and limiting the\n",
      "margin violations  (i.e., instances that end up in the middle of the street or even on the\n",
      "wrong side). This is called soft margin classification .\n",
      "In Scikit-Learn’s SVM classes, you can control this balance using the C hyperparame‐\n",
      "ter: a smaller C value leads to a wider street but more margin violations. Figure 5-4\n",
      "shows the decision boundaries and margins of two soft margin SVM classifiers on a\n",
      "nonlinearly separable dataset. On the left, using a low C value the margin is quite\n",
      "large, but many instances end up on the street. On the right, using a high C value the\n",
      "classifier makes fewer margin violations but ends up with a smaller margin. However,\n",
      "it seems likely that the first classifier will generalize better: in fact even on this train‐\n",
      "ing set it makes fewer prediction errors, since most of the margin violations are\n",
      "actually on the correct side of the decision boundary.\n",
      "Figure 5-4. Large margin (left)  versus fewer margin violations (right)\n",
      "If your SVM model is overfitting, you can try regularizing it by\n",
      "reducing C.\n",
      "The following Scikit-Learn code loads the iris dataset, scales the features, and then\n",
      "trains a linear SVM model (using the LinearSVC  class with C = 1 and the hinge loss\n",
      "function, described shortly) to detect Iris-Virginica flowers. The resulting model is\n",
      "represented on the left of Figure 5-4 .\n",
      "Linear SVM Classification  | 157\n",
      "import numpy as np\n",
      "from sklearn import datasets\n",
      "from sklearn.pipeline  import Pipeline\n",
      "from sklearn.preprocessing  import StandardScaler\n",
      "from sklearn.svm  import LinearSVC\n",
      "iris = datasets .load_iris ()\n",
      "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
      "y = (iris[\"target\" ] == 2).astype(np.float64)  # Iris-Virginica\n",
      "svm_clf = Pipeline ([\n",
      "        (\"scaler\" , StandardScaler ()),\n",
      "        (\"linear_svc\" , LinearSVC (C=1, loss=\"hinge\")),\n",
      "    ])\n",
      "svm_clf.fit(X, y)\n",
      "Then, as usual, you can use the model to make predictions:\n",
      ">>> svm_clf.predict([[5.5, 1.7]])\n",
      "array([1.])\n",
      "Unlike Logistic Regression classifiers, SVM classifiers do not out‐\n",
      "put probabilities for each class.\n",
      "Alternatively, you could use the SVC class, using SVC(kernel=\"linear\", C=1) , but it\n",
      "is much slower, especially with large training sets, so it is not recommended. Another\n",
      "option is to use the SGDClassifier  class, with SGDClassifier(loss=\"hinge\",\n",
      "alpha=1/(m*C)) . This applies regular Stochastic Gradient Descent (see Chapter 4 ) to\n",
      "train a linear SVM classifier. It does not converge as fast as the LinearSVC  class, but it\n",
      "can be useful to handle huge datasets that do not fit in memory (out-of-core train‐\n",
      "ing), or to handle online classification tasks.\n",
      "The LinearSVC  class regularizes the bias term, so you should center\n",
      "the training set first by subtracting its mean. This is automatic if\n",
      "you scale the data using the StandardScaler . Moreover, make sure\n",
      "you set the loss  hyperparameter to \"hinge\" , as it is not the default\n",
      "value. Finally, for better performance you should set the dual\n",
      "hyperparameter to False , unless there are more features than\n",
      "training instances (we will discuss duality later in the chapter).\n",
      "158 | Chapter 5: Support Vector Machines\n",
      "Nonlinear SVM Classification\n",
      "Although linear SVM classifiers are efficient and work surprisingly well in many\n",
      "cases, many datasets are not even close to being linearly separable. One approach to\n",
      "handling nonlinear datasets is to add more features, such as polynomial features (as\n",
      "you did in Chapter 4 ); in some cases this can result in a linearly separable dataset.\n",
      "Consider the left plot in Figure 5-5 : it represents a simple dataset with just one feature\n",
      "x1. This dataset is not linearly separable, as you can see. But if you add a second fea‐\n",
      "ture x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable.\n",
      "Figure 5-5. Adding features to make a dataset linearly separable\n",
      "To implement this idea using Scikit-Learn, you can create a Pipeline  containing a\n",
      "PolynomialFeatures  transformer (discussed in “Polynomial Regression” on page\n",
      "130), followed by a StandardScaler  and a LinearSVC . Let’s test this on the moons\n",
      "dataset: this is a toy dataset for binary classification in which the data points are sha‐\n",
      "ped as two interleaving half circles (see Figure 5-6 ). Y ou can generate this dataset\n",
      "using the make_moons()  function:\n",
      "from sklearn.datasets  import make_moons\n",
      "from sklearn.pipeline  import Pipeline\n",
      "from sklearn.preprocessing  import PolynomialFeatures\n",
      "polynomial_svm_clf  = Pipeline ([\n",
      "        (\"poly_features\" , PolynomialFeatures (degree=3)),\n",
      "        (\"scaler\" , StandardScaler ()),\n",
      "        (\"svm_clf\" , LinearSVC (C=10, loss=\"hinge\"))\n",
      "    ])\n",
      "polynomial_svm_clf .fit(X, y)\n",
      "Nonlinear SVM Classification  | 159\n",
      "Figure 5-6. Linear SVM classifier  using polynomial features\n",
      "Polynomial Kernel\n",
      "Adding polynomial features is simple to implement and can work great with all sorts\n",
      "of Machine Learning algorithms (not just SVMs), but at a low polynomial degree it\n",
      "cannot deal with very complex datasets, and with a high polynomial degree it creates\n",
      "a huge number of features, making the model too slow.\n",
      "Fortunately, when using SVMs you can apply an almost miraculous mathematical\n",
      "technique called the kernel trick  (it is explained in a moment). It makes it possible to\n",
      "get the same result as if you added many polynomial features, even with very high-\n",
      "degree polynomials, without actually having to add them. So there is no combinato‐\n",
      "rial explosion of the number of features since you don’t actually add any features. This\n",
      "trick is implemented by the SVC class. Let’s test it on the moons dataset:\n",
      "from sklearn.svm  import SVC\n",
      "poly_kernel_svm_clf  = Pipeline ([\n",
      "        (\"scaler\" , StandardScaler ()),\n",
      "        (\"svm_clf\" , SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
      "    ])\n",
      "poly_kernel_svm_clf .fit(X, y)\n",
      "This code trains an SVM classifier using a 3rd-degree polynomial kernel. It is repre‐\n",
      "sented on the left of Figure 5-7 . On the right is another SVM classifier using a 10th-\n",
      "degree polynomial kernel. Obviously, if your model is overfitting, you might want to\n",
      "160 | Chapter 5: Support Vector Machines\n",
      "reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\n",
      "it. The hyperparameter coef0  controls how much the model is influenced by high-\n",
      "degree polynomials versus low-degree polynomials.\n",
      "Figure 5-7. SVM classifiers  with a polynomial kernel\n",
      "A common approach to find the right hyperparameter values is to\n",
      "use grid search (see Chapter 2 ). It is often faster to first do a very\n",
      "coarse grid search, then a finer grid search around the best values\n",
      "found. Having a good sense of what each hyperparameter actually\n",
      "does can also help you search in the right part of the hyperparame‐\n",
      "ter space.\n",
      "Adding Similarity Features\n",
      "Another technique to tackle nonlinear problems is to add features computed using a\n",
      "similarity function  that measures how much each instance resembles a particular\n",
      "landmark . For example, let’s take the one-dimensional dataset discussed earlier and\n",
      "add two landmarks to it at x1 = –2 and x1 = 1 (see the left plot in Figure 5-8 ). Next,\n",
      "let’s define the similarity function to be the Gaussian Radial Basis Function  (RBF )\n",
      "with γ = 0.3 (see Equation 5-1 ).\n",
      "Equation 5-1. Gaussian RBF\n",
      "ϕγx, ℓ= exp −γ∥x− ℓ∥2\n",
      "It is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at\n",
      "the landmark). Now we are ready to compute the new features. For example, let’s look\n",
      "at the instance x1 = –1: it is located at a distance of 1 from the first landmark, and 2\n",
      "from the second landmark. Therefore its new features are x2 = exp (–0.3 × 12) ≈ 0.74\n",
      "and x3 = exp (–0.3 × 22) ≈ 0.30. The plot on the right of Figure 5-8  shows the trans‐\n",
      "formed dataset (dropping the original features). As you can see, it is now linearly\n",
      "separable.\n",
      "Nonlinear SVM Classification  | 161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 5-8. Similarity features using the Gaussian RBF\n",
      "Y ou may wonder how to select the landmarks. The simplest approach is to create a\n",
      "landmark at the location of each and every instance in the dataset. This creates many\n",
      "dimensions and thus increases the chances that the transformed training set will be\n",
      "linearly separable. The downside is that a training set with m instances and n features\n",
      "gets transformed into a training set with m instances and m features (assuming you\n",
      "drop the original features). If your training set is very large, you end up with an\n",
      "equally large number of features.\n",
      "Gaussian RBF Kernel\n",
      "Just like the polynomial features method, the similarity features method can be useful\n",
      "with any Machine Learning algorithm, but it may be computationally expensive to\n",
      "compute all the additional features, especially on large training sets. However, once\n",
      "again the kernel trick does its SVM magic: it makes it possible to obtain a similar\n",
      "result as if you had added many similarity features, without actually having to add\n",
      "them. Let’s try the Gaussian RBF kernel using the SVC class:\n",
      "rbf_kernel_svm_clf  = Pipeline ([\n",
      "        (\"scaler\" , StandardScaler ()),\n",
      "        (\"svm_clf\" , SVC(kernel=\"rbf\", gamma=5, C=0.001))\n",
      "    ])\n",
      "rbf_kernel_svm_clf .fit(X, y)\n",
      "This model is represented on the bottom left of Figure 5-9 . The other plots show\n",
      "models trained with different values of hyperparameters gamma  (γ) and C. Increasing\n",
      "gamma  makes the bell-shape curve narrower (see the left plot of Figure 5-8 ), and as a\n",
      "result each instance’s range of influence is smaller: the decision boundary ends up\n",
      "being more irregular, wiggling around individual instances. Conversely, a small gamma  \n",
      "value makes the bell-shaped curve wider, so instances have a larger range of influ‐\n",
      "ence, and the decision boundary ends up smoother. So γ acts like a regularization\n",
      "hyperparameter: if your model is overfitting, you should reduce it, and if it is under‐\n",
      "fitting, you should increase it (similar to the C hyperparameter).\n",
      "162 | Chapter 5: Support Vector Machines\n",
      "1“ A Dual Coordinate Descent Method for Large-scale Linear SVM, ” Lin et al. (2008).\n",
      "Figure 5-9. SVM classifiers  using an RBF kernel\n",
      "Other kernels exist but are used much more rarely. For example, some kernels are\n",
      "specialized for specific data structures. String kernels  are sometimes used when classi‐\n",
      "fying text documents or DNA sequences (e.g., using the string subsequence kernel  or\n",
      "kernels based on the Levenshtein distance ).\n",
      "With so many kernels to choose from, how can you decide which\n",
      "one to use? As a rule of thumb, you should always try the linear\n",
      "kernel first (remember that LinearSVC  is much faster than SVC(ker\n",
      "nel=\"linear\") ), especially if the training set is very large or if it\n",
      "has plenty of features. If the training set is not too large, you should\n",
      "try the Gaussian RBF kernel as well; it works well in most cases.\n",
      "Then if you have spare time and computing power, you can also\n",
      "experiment with a few other kernels using cross-validation and grid\n",
      "search, especially if there are kernels specialized for your training\n",
      "set’s data structure.\n",
      "Computational Complexity\n",
      "The LinearSVC  class is based on the liblinear  library, which implements an optimized\n",
      "algorithm  for linear SVMs.1 It does not support the kernel trick, but it scales almost\n",
      "Nonlinear SVM Classification  | 163\n",
      "2“Sequential Minimal Optimization (SMO), ” J. Platt (1998).linearly with the number of training instances and the number of features: its training\n",
      "time complexity is roughly O(m × n).\n",
      "The algorithm takes longer if you require a very high precision. This is controlled by\n",
      "the tolerance hyperparameter ϵ (called tol in Scikit-Learn). In most classification\n",
      "tasks, the default tolerance is fine.\n",
      "The SVC class is based on the libsvm  library, which implements an algorithm  that sup‐\n",
      "ports the kernel trick.2 The training time complexity is usually between O(m2 × n)\n",
      "and O(m3 × n). Unfortunately, this means that it gets dreadfully slow when the num‐\n",
      "ber of training instances gets large (e.g., hundreds of thousands of instances). This\n",
      "algorithm is perfect for complex but small or medium training sets. However, it scales\n",
      "well with the number of features, especially with sparse features  (i.e., when each\n",
      "instance has few nonzero features). In this case, the algorithm scales roughly with the\n",
      "average number of nonzero features per instance. Table 5-1  compares Scikit-Learn’s\n",
      "SVM classification classes.\n",
      "Table 5-1. Comparison of Scikit-Learn classes for SVM classification\n",
      "Class Time complexity Out-of-core support Scaling required Kernel trick\n",
      "LinearSVC O(m × n) No Yes No\n",
      "SGDClassifier O(m × n) Yes Yes No\n",
      "SVC O(m² × n) to O( m³ × n)No Yes Yes\n",
      "SVM Regression\n",
      "As we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup‐\n",
      "port linear and nonlinear classification, but it also supports linear and nonlinear\n",
      "regression. The trick is to reverse the objective: instead of trying to fit the largest pos‐\n",
      "sible street between two classes while limiting margin violations, SVM Regression\n",
      "tries to fit as many instances as possible on the street while limiting margin violations\n",
      "(i.e., instances off the street). The width of the street is controlled by a hyperparame‐\n",
      "ter ϵ. Figure 5-10  shows two linear SVM Regression models trained on some random\n",
      "linear data, one with a large margin ( ϵ = 1.5) and the other with a small margin ( ϵ =\n",
      "0.5).\n",
      "164 | Chapter 5: Support Vector Machines\n",
      "Figure 5-10. SVM Regression\n",
      "Adding more training instances within the margin does not affect the model’s predic‐\n",
      "tions; thus, the model is said to be ϵ-insensitive .\n",
      "Y ou can use Scikit-Learn’s LinearSVR  class to perform linear SVM Regression. The\n",
      "following code produces the model represented on the left of Figure 5-10  (the train‐\n",
      "ing data should be scaled and centered first):\n",
      "from sklearn.svm  import LinearSVR\n",
      "svm_reg = LinearSVR (epsilon=1.5)\n",
      "svm_reg.fit(X, y)\n",
      "To tackle nonlinear regression tasks, you can use a kernelized SVM model. For exam‐\n",
      "ple, Figure 5-11  shows SVM Regression on a random quadratic training set, using a\n",
      "2nd-degree polynomial kernel. There is little regularization on the left plot (i.e., a large\n",
      "C value), and much more regularization on the right plot (i.e., a small C value).\n",
      "Figure 5-11. SVM regression using a 2nd-degree polynomial kernel\n",
      "SVM Regression | 165\n",
      "The following code produces the model represented on the left of Figure 5-11  using\n",
      "Scikit-Learn’s SVR class (which supports the kernel trick). The SVR class is the regres‐\n",
      "sion equivalent of the SVC class, and the LinearSVR  class is the regression equivalent\n",
      "of the LinearSVC  class. The LinearSVR  class scales linearly with the size of the train‐\n",
      "ing set (just like the LinearSVC  class), while the SVR class gets much too slow when\n",
      "the training set grows large (just like the SVC class).\n",
      "from sklearn.svm  import SVR\n",
      "svm_poly_reg  = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
      "svm_poly_reg .fit(X, y)\n",
      "SVMs can also be used for outlier detection; see Scikit-Learn’s doc‐\n",
      "umentation for more details.\n",
      "Under the Hood\n",
      "This section explains how SVMs make predictions and how their training algorithms\n",
      "work, starting with linear SVM classifiers. Y ou can safely skip it and go straight to the\n",
      "exercises at the end of this chapter if you are just getting started with Machine Learn‐\n",
      "ing, and come back later when you want to get a deeper understanding of SVMs.\n",
      "First, a word about notations: in Chapter 4  we used the convention of putting all the \n",
      "model parameters in one vector θ, including the bias term θ0 and the input feature\n",
      "weights θ1 to θn, and adding a bias input x0 = 1 to all instances. In this chapter, we will\n",
      "use a different convention, which is more convenient (and more common) when you\n",
      "are dealing with SVMs: the bias term will be called b and the feature weights vector\n",
      "will be called w. No bias feature will be added to the input feature vectors.\n",
      "Decision Function and Predictions\n",
      "The linear SVM classifier model predicts the class of a new instance x by simply com‐\n",
      "puting the decision function wT x + b = w1 x1 + ⋯ + wn xn + b: if the result is positive,\n",
      "the predicted class ŷ is the positive class (1), or else it is the negative class (0); see\n",
      "Equation 5-2 .\n",
      "Equation 5-2. Linear SVM classifier  prediction\n",
      "y=0 if wTx+b< 0,\n",
      "1 if wTx+b≥ 0\n",
      "166 | Chapter 5: Support Vector Machines\n",
      "3More generally, when there are n features, the decision function is an n-dimensional hyperplane , and the deci‐\n",
      "sion boundary is an ( n – 1)-dimensional hyperplane.Figure 5-12  shows the decision function that corresponds to the model on the left of\n",
      "Figure 5-4 : it is a two-dimensional plane since this dataset has two features (petal\n",
      "width and petal length). The decision boundary is the set of points where the decision\n",
      "function is equal to 0: it is the intersection of two planes, which is a straight line (rep‐\n",
      "resented by the thick solid line).3\n",
      "Figure 5-12. Decision function for the iris dataset\n",
      "The dashed lines represent the points where the decision function is equal to 1 or –1:\n",
      "they are parallel and at equal distance to the decision boundary, forming a margin\n",
      "around it. Training a linear SVM classifier means finding the value of w and b that\n",
      "make this margin as wide as possible while avoiding margin violations (hard margin)\n",
      "or limiting them (soft margin).\n",
      "Training Objective\n",
      "Consider the slope of the decision function: it is equal to the norm of the weight vec‐\n",
      "tor, ∥ w ∥. If we divide this slope by 2, the points where the decision function is equal\n",
      "to ±1 are going to be twice as far away from the decision boundary. In other words,\n",
      "dividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to visual‐\n",
      "ize in 2D in Figure 5-13 . The smaller the weight vector w, the larger the margin.\n",
      "Under the Hood | 167\n",
      "4Zeta ( ζ) is the 6th letter of the Greek alphabet.\n",
      "Figure 5-13. A smaller weight vector results in a larger margin\n",
      "So we want to minimize ∥ w ∥ to get a large margin. However, if we also want to avoid\n",
      "any margin violation (hard margin), then we need the decision function to be greater\n",
      "than 1 for all positive training instances, and lower than –1 for negative training\n",
      "instances. If we define t(i) = –1 for negative instances (if y(i) = 0) and t(i) = 1 for positive\n",
      "instances (if y(i) = 1), then we can express this constraint as t(i)(wT x(i) + b) ≥ 1 for all\n",
      "instances.\n",
      "We can therefore express the hard margin linear SVM classifier objective as the con‐\n",
      "strained optimization  problem in Equation 5-3 .\n",
      "Equation 5-3. Hard margin linear SVM classifier  objective\n",
      "minimizew,b1\n",
      "2wTw\n",
      "subject to tiwTxi+b≥ 1 for i= 1, 2,⋯,m\n",
      "We are minimizing 1\n",
      "2wT w, which is equal to 1\n",
      "2∥ w ∥2, rather than\n",
      "minimizing ∥ w ∥. Indeed, 1\n",
      "2∥ w ∥2 has a nice and simple derivative\n",
      "(it is just w) while ∥ w ∥ is not differentiable at w = 0. Optimization\n",
      "algorithms work much better on differentiable functions.\n",
      "To get the soft margin objective, we need to introduce a slack variable  ζ(i) ≥ 0 for each\n",
      "instance:4 ζ(i) measures how much the ith instance is allowed to violate the margin. We\n",
      "now have two conflicting objectives: making the slack variables as small as possible to\n",
      "reduce the margin violations, and making 1\n",
      "2wT w as small as possible to increase the\n",
      "margin. This is where the C hyperparameter comes in: it allows us to define the trade‐\n",
      "168 | Chapter 5: Support Vector Machines\n",
      "5To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vanden‐\n",
      "berghe, Convex Optimization  (Cambridge, UK: Cambridge University Press, 2004) or watch Richard Brown’s\n",
      "series of video lectures .off between these two objectives. This gives us the constrained optimization problem\n",
      "in Equation 5-4 .\n",
      "Equation 5-4. Soft margin linear SVM classifier  objective\n",
      "minimizew,b,ζ1\n",
      "2wTw+C∑\n",
      "i= 1m\n",
      "ζi\n",
      "subject to tiwTxi+b≥ 1 − ζiand ζi≥ 0 for i= 1, 2,⋯,m\n",
      "Quadratic Programming\n",
      "The hard margin and soft margin problems are both convex quadratic optimization\n",
      "problems with linear constraints. Such problems are known as Quadratic Program‐\n",
      "ming  (QP) problems. Many off-the-shelf solvers are available to solve QP problems\n",
      "using a variety of techniques that are outside the scope of this book.5 The general\n",
      "problem formulation is given by Equation 5-5 .\n",
      "Equation 5-5. Quadratic Programming problem\n",
      "Minimize\n",
      "p1\n",
      "2pTHp + fTp\n",
      "subject to Ap≤b\n",
      "wherepis an np‐dimensional vector ( np= number of parameters),\n",
      "His an np×npmatrix,\n",
      "fis an np‐dimensional vector,\n",
      "Ais an nc×npmatrix ( nc= number of constraints),\n",
      "bis an nc‐dimensional vector.\n",
      "Note that the expression A p ≤ b actually defines nc constraints: pT a(i) ≤ b(i) for i = 1,\n",
      "2, ⋯, nc, where a(i) is the vector containing the elements of the ith row of A and b(i) is\n",
      "the ith element of b.\n",
      "Y ou can easily verify that if you set the QP parameters in the following way, you get\n",
      "the hard margin linear SVM classifier objective:\n",
      "•np = n + 1, where n is the number of features (the +1 is for the bias term).\n",
      "Under the Hood | 169\n",
      "6The objective function is convex, and the inequality constraints are continuously differentiable and convex\n",
      "functions.•nc = m, where m is the number of training instances.\n",
      "•H is the np × np identity matrix, except with a zero in the top-left cell (to ignore\n",
      "the bias term).\n",
      "•f = 0, an np-dimensional vector full of 0s.\n",
      "•b = –1, an nc-dimensional vector full of –1s.\n",
      "•a(i) = –t(i) x˙ (i), where x˙ (i) is equal to x(i) with an extra bias feature x˙ 0 = 1.\n",
      "So one way to train a hard margin linear SVM classifier is just to use an off-the-shelf\n",
      "QP solver by passing it the preceding parameters. The resulting vector p will contain\n",
      "the bias term b = p0 and the feature weights wi = pi for i = 1, 2, ⋯, n. Similarly, you\n",
      "can use a QP solver to solve the soft margin problem (see the exercises at the end of\n",
      "the chapter).\n",
      "However, to use the kernel trick we are going to look at a different constrained opti‐\n",
      "mization problem.\n",
      "The Dual Problem\n",
      "Given a constrained optimization problem, known as the primal problem , it is possi‐\n",
      "ble to express a different but closely related problem, called its dual problem . The sol‐\n",
      "ution to the dual problem typically gives a lower bound to the solution of the primal\n",
      "problem, but under some conditions it can even have the same solutions as the pri‐\n",
      "mal problem. Luckily, the SVM problem happens to meet these conditions,6 so you\n",
      "can choose to solve the primal problem or the dual problem; both will have the same\n",
      "solution. Equation 5-6  shows the dual form of the linear SVM objective (if you are\n",
      "interested in knowing how to derive the dual problem from the primal problem,\n",
      "see ???).\n",
      "Equation 5-6. Dual form of the linear SVM objective\n",
      "minimizeα1\n",
      "2∑\n",
      "i= 1m\n",
      "∑\n",
      "j= 1m\n",
      "αiαjtitjxiTxj−∑\n",
      "i= 1m\n",
      "αi\n",
      "subject to αi≥ 0 for i= 1, 2,⋯,m\n",
      "170 | Chapter 5: Support Vector Machines\n",
      "7As explained in Chapter 4 , the dot product of two vectors a and b is normally noted a · b. However, in\n",
      "Machine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the\n",
      "dot product is achieved by computing aTb. To remain consistent with the rest of the book, we will use this\n",
      "notation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value.Once you find the vector α that minimizes this equation (using a QP solver), you can\n",
      "compute w and b that minimize the primal problem by using Equation 5-7 .\n",
      "Equation 5-7. From the dual solution to the primal solution\n",
      "w=∑\n",
      "i= 1m\n",
      "αitixi\n",
      "b=1\n",
      "ns∑\n",
      "i= 1\n",
      "αi> 0m\n",
      "ti−wTxi\n",
      "The dual problem is faster to solve than the primal when the number of training\n",
      "instances is smaller than the number of features. More importantly, it makes the ker‐\n",
      "nel trick possible, while the primal does not. So what is this kernel trick anyway?\n",
      "Kernelized SVM\n",
      "Suppose you want to apply a 2nd-degree polynomial transformation to a two-\n",
      "dimensional training set (such as the moons training set), then train a linear SVM\n",
      "classifier on the transformed training set. Equation 5-8  shows the 2nd-degree polyno‐\n",
      "mial mapping function ϕ that you want to apply.\n",
      "Equation 5-8. Second-degree polynomial mapping\n",
      "ϕx=ϕx1\n",
      "x2=x12\n",
      "2x1x2\n",
      "x22\n",
      "Notice that the transformed vector is three-dimensional instead of two-dimensional.\n",
      "Now let’s look at what happens to a couple of two-dimensional vectors, a and b, if we\n",
      "apply this 2nd-degree polynomial mapping and then compute the dot product7 of the\n",
      "transformed vectors (See Equation 5-9 ).\n",
      "Under the Hood | 171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equation 5-9. Kernel trick for a 2nd-degree polynomial mapping\n",
      "ϕaTϕb =a12\n",
      "2a1a2\n",
      "a22Tb12\n",
      "2b1b2\n",
      "b22=a12b12+ 2a1b1a2b2+a22b22\n",
      "=a1b1+a2b22=a1\n",
      "a2Tb1\n",
      "b22\n",
      "=aTb2\n",
      "How about that? The dot product of the transformed vectors is equal to the square of\n",
      "the dot product of the original vectors: ϕ(a)T ϕ(b) = ( aT b)2.\n",
      "Now here is the key insight: if you apply the transformation ϕ to all training instan‐\n",
      "ces, then the dual problem (see Equation 5-6 ) will contain the dot product ϕ(x(i))T\n",
      "ϕ(x(j)). But if ϕ is the 2nd-degree polynomial transformation defined in Equation 5-8 ,\n",
      "then you can replace this dot product of transformed vectors simply by xiTxj2\n",
      ". So\n",
      "you don’t actually need to transform the training instances at all: just replace the dot\n",
      "product by its square in Equation 5-6 . The result will be strictly the same as if you\n",
      "went through the trouble of actually transforming the training set then fitting a linear\n",
      "SVM algorithm, but this trick makes the whole process much more computationally\n",
      "efficient. This is the essence of the kernel trick.\n",
      "The function K(a, b) = ( aT b)2 is called a 2nd-degree polynomial kernel . In Machine\n",
      "Learning, a kernel  is a function capable of computing the dot product ϕ(a)T ϕ(b)\n",
      "based only on the original vectors a and b, without having to compute (or even to\n",
      "know about) the transformation ϕ. Equation 5-10  lists some of the most commonly\n",
      "used kernels.\n",
      "Equation 5-10. Common kernels\n",
      "Linear: Ka,b=aTb\n",
      "Polynomial: Ka,b=γaTb+rd\n",
      "Gaussian RBF: Ka,b= exp −γ∥a−b∥2\n",
      "Sigmoid: Ka,b= tanh γaTb+r\n",
      "172 | Chapter 5: Support Vector Machines\n",
      "Mercer’s Theorem\n",
      "According to Mercer’s theorem , if a function K(a, b) respects a few mathematical con‐\n",
      "ditions called Mercer’s conditions  (K must be continuous, symmetric in its arguments\n",
      "so K(a, b) = K(b, a), etc.), then there exists a function ϕ that maps a and b into\n",
      "another space (possibly with much higher dimensions) such that K(a, b) = ϕ(a)T ϕ(b).\n",
      "So you can use K as a kernel since you know ϕ exists, even if you don’t know what ϕ\n",
      "is. In the case of the Gaussian RBF kernel, it can be shown that ϕ actually maps each\n",
      "training instance to an infinite-dimensional space, so it’s a good thing you don’t need\n",
      "to actually perform the mapping!\n",
      "Note that some frequently used kernels (such as the Sigmoid kernel) don’t respect all\n",
      "of Mercer’s conditions, yet they generally work well in practice.\n",
      "There is still one loose end we must tie. Equation 5-7  shows how to go from the dual\n",
      "solution to the primal solution in the case of a linear SVM classifier, but if you apply\n",
      "the kernel trick you end up with equations that include ϕ(x(i)). In fact, w must have\n",
      "the same number of dimensions as ϕ(x(i)), which may be huge or even infinite, so you\n",
      "can’t compute it. But how can you make predictions without knowing w? Well, the\n",
      "good news is that you can plug in the formula for w from Equation 5-7  into the deci‐\n",
      "sion function for a new instance x(n), and you get an equation with only dot products\n",
      "between input vectors. This makes it possible to use the kernel trick, once again\n",
      "(Equation 5-11 ).\n",
      "Equation 5-11. Making predictions with a kernelized SVM\n",
      "hw,bϕxn=wTϕxn+b=∑\n",
      "i= 1m\n",
      "αitiϕxiT\n",
      "ϕxn+b\n",
      "=∑\n",
      "i= 1m\n",
      "αitiϕxiTϕxn+b\n",
      "=∑\n",
      "i= 1\n",
      "αi> 0m\n",
      "αitiKxi,xn+b\n",
      "Note that since α(i) ≠ 0 only for support vectors, making predictions involves comput‐\n",
      "ing the dot product of the new input vector x(n) with only the support vectors, not all\n",
      "the training instances. Of course, you also need to compute the bias term b, using the\n",
      "same trick ( Equation 5-12 ).\n",
      "Under the Hood | 173\n",
      "Equation 5-12. Computing the bias term using the kernel trick\n",
      "b=1\n",
      "ns∑\n",
      "i= 1\n",
      "αi> 0m\n",
      "ti−wTϕxi=1\n",
      "ns∑\n",
      "i= 1\n",
      "αi> 0m\n",
      "ti−∑\n",
      "j= 1m\n",
      "αjtjϕxjT\n",
      "ϕxi\n",
      "=1\n",
      "ns∑\n",
      "i= 1\n",
      "αi> 0m\n",
      "ti−∑\n",
      "j= 1\n",
      "αj> 0m\n",
      "αjtjKxi,xj\n",
      "If you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side\n",
      "effect of the kernel trick.\n",
      "Online SVMs\n",
      "Before concluding this chapter, let’s take a quick look at online SVM classifiers (recall\n",
      "that online learning means learning incrementally, typically as new instances arrive).\n",
      "For linear SVM classifiers, one method is to use Gradient Descent (e.g., using\n",
      "SGDClassifier ) to minimize the cost function in Equation 5-13 , which is derived\n",
      "from the primal problem. Unfortunately it converges much more slowly than the\n",
      "methods based on QP .\n",
      "Equation 5-13. Linear SVM classifier  cost function\n",
      "Jw,b=1\n",
      "2wTw + C∑\n",
      "i= 1m\n",
      "max 0, 1 − tiwTxi+b\n",
      "The first sum in the cost function will push the model to have a small weight vector\n",
      "w, leading to a larger margin. The second sum computes the total of all margin viola‐\n",
      "tions. An instance’s margin violation is equal to 0 if it is located off the street and on\n",
      "the correct side, or else it is proportional to the distance to the correct side of the\n",
      "street. Minimizing this term ensures that the model makes the margin violations as\n",
      "small and as few as possible\n",
      "Hinge Loss\n",
      "The function max (0, 1 – t) is called the hinge loss  function (represented below). It is\n",
      "equal to 0 when t ≥ 1. Its derivative (slope) is equal to –1 if t < 1 and 0 if t > 1. It is not\n",
      "differentiable at t = 1, but just like for Lasso Regression (see “Lasso Regression”  on\n",
      "page 139) you can still use Gradient Descent using any subderivative  at t = 1 (i.e., any\n",
      "value between –1 and 0).\n",
      "174 | Chapter 5: Support Vector Machines\n",
      "8“Incremental and Decremental Support Vector Machine Learning, ” G. Cauwenberghs, T. Poggio (2001).\n",
      "9“Fast Kernel Classifiers with Online and Active Learning,“ A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005).\n",
      "It is also possible to implement online kernelized SVMs—for example, using “Incre‐\n",
      "mental and Decremental SVM Learning”8 or “Fast Kernel Classifiers with Online and\n",
      "Active Learning. ”9 However, these are implemented in Matlab and C++. For large-\n",
      "scale nonlinear problems, you may want to consider using neural networks instead \n",
      "(see Part II ).\n",
      "Exercises\n",
      "1.What is the fundamental idea behind Support Vector Machines?\n",
      "2.What is a support vector?\n",
      "3.Why is it important to scale the inputs when using SVMs?\n",
      "4.Can an SVM classifier output a confidence score when it classifies an instance?\n",
      "What about a probability?\n",
      "5.Should you use the primal or the dual form of the SVM problem to train a model\n",
      "on a training set with millions of instances and hundreds of features?\n",
      "6.Say you trained an SVM classifier with an RBF kernel. It seems to underfit the\n",
      "training set: should you increase or decrease γ (gamma )? What about C?\n",
      "7.How should you set the QP parameters ( H, f, A, and b) to solve the soft margin\n",
      "linear SVM classifier problem using an off-the-shelf QP solver?\n",
      "8.Train a LinearSVC  on a linearly separable dataset. Then train an SVC and a\n",
      "SGDClassifier  on the same dataset. See if you can get them to produce roughly\n",
      "the same model.\n",
      "9.Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary\n",
      "classifiers, you will need to use one-versus-all to classify all 10 digits. Y ou may\n",
      "Exercises | 175\n",
      "want to tune the hyperparameters using small validation sets to speed up the pro‐\n",
      "cess. What accuracy can you reach?\n",
      "10.Train an SVM regressor on the California housing dataset.\n",
      "Solutions to these exercises are available in ???.\n",
      "176 | Chapter 5: Support Vector Machines\n",
      "CHAPTER 6\n",
      "Decision Trees\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 6 in the final\n",
      "release of the book.\n",
      "Like SVMs, Decision Trees  are versatile Machine Learning algorithms that can per‐\n",
      "form both classification and regression tasks, and even multioutput tasks. They are\n",
      "very powerful algorithms, capable of fitting complex datasets. For example, in Chap‐\n",
      "ter 2  you trained a DecisionTreeRegressor  model on the California housing dataset,\n",
      "fitting it perfectly (actually overfitting it).\n",
      "Decision Trees are also the fundamental components of Random Forests (see Chap‐\n",
      "ter 7 ), which are among the most powerful Machine Learning algorithms available\n",
      "today.\n",
      "In this chapter we will start by discussing how to train, visualize, and make predic‐\n",
      "tions with Decision Trees. Then we will go through the CART training algorithm\n",
      "used by Scikit-Learn, and we will discuss how to regularize trees and use them for\n",
      "regression tasks. Finally, we will discuss some of the limitations of Decision Trees.\n",
      "Training and Visualizing a Decision Tree\n",
      "To understand Decision Trees, let’s just build one and take a look at how it makes pre‐\n",
      "dictions. The following code trains a DecisionTreeClassifier  on the iris dataset\n",
      "(see Chapter 4 ):\n",
      "from sklearn.datasets  import load_iris\n",
      "from sklearn.tree  import DecisionTreeClassifier\n",
      "177\n",
      "1Graphviz is an open source graph visualization software package, available at http://www.graphviz.org/ .iris = load_iris ()\n",
      "X = iris.data[:, 2:] # petal length and width\n",
      "y = iris.target\n",
      "tree_clf  = DecisionTreeClassifier (max_depth =2)\n",
      "tree_clf .fit(X, y)\n",
      "Y ou can visualize the trained Decision Tree by first using the export_graphviz()  \n",
      "method to output a graph definition file called iris_tree.dot :\n",
      "from sklearn.tree  import export_graphviz\n",
      "export_graphviz (\n",
      "        tree_clf ,\n",
      "        out_file =image_path (\"iris_tree.dot\" ),\n",
      "        feature_names =iris.feature_names [2:],\n",
      "        class_names =iris.target_names ,\n",
      "        rounded=True,\n",
      "        filled=True\n",
      "    )\n",
      "Then you can convert this .dot file to a variety of formats such as PDF or PNG using\n",
      "the dot command-line tool from the graphviz  package.1 This command line converts\n",
      "the .dot file to a .png image file:\n",
      "$ dot -Tpng iris_tree.dot -o iris_tree.png\n",
      "Y our first decision tree looks like Figure 6-1 .\n",
      "178 | Chapter 6: Decision Trees\n",
      "Figure 6-1. Iris Decision Tree\n",
      "Making Predictions\n",
      "Let’s see how the tree represented in Figure 6-1  makes predictions. Suppose you find\n",
      "an iris flower and you want to classify it. Y ou start at the root node  (depth 0, at the\n",
      "top): this node asks whether the flower’s petal length is smaller than 2.45 cm. If it is,\n",
      "then you move down to the root’s left child node (depth 1, left). In this case, it is a leaf\n",
      "node  (i.e., it does not have any children nodes), so it does not ask any questions: you\n",
      "can simply look at the predicted class for that node and the Decision Tree predicts\n",
      "that your flower is an Iris-Setosa ( class=setosa ).\n",
      "Now suppose you find another flower, but this time the petal length is greater than\n",
      "2.45 cm. Y ou must move down to the root’s right child node (depth 1, right), which is\n",
      "not a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If\n",
      "it is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely\n",
      "an Iris-Virginica (depth 2, right). It’s really that simple.\n",
      "One of the many qualities of Decision Trees is that they require\n",
      "very little data preparation. In particular, they don’t require feature\n",
      "scaling or centering at all.\n",
      "Making Predictions | 179\n",
      "A node’s samples  attribute counts how many training instances it applies to. For\n",
      "example, 100 training instances have a petal length greater than 2.45 cm (depth 1,\n",
      "right), among which 54 have a petal width smaller than 1.75 cm (depth 2, left). A\n",
      "node’s value  attribute tells you how many training instances of each class this node\n",
      "applies to: for example, the bottom-right node applies to 0 Iris-Setosa, 1 Iris-\n",
      "Versicolor, and 45 Iris-Virginica. Finally, a node’s gini  attribute measures its impur‐\n",
      "ity: a node is “pure” ( gini=0 ) if all training instances it applies to belong to the same\n",
      "class. For example, since the depth-1 left node applies only to Iris-Setosa training\n",
      "instances, it is pure and its gini  score is 0. Equation 6-1  shows how the training algo‐\n",
      "rithm computes the gini score Gi of the ith node. For example, the depth-2 left node\n",
      "has a gini  score equal to 1 – (0/54)2 – (49/54)2 – (5/54)2 ≈ 0.168. Another impurity\n",
      "measure  is discussed shortly.\n",
      "Equation 6-1. Gini impurity\n",
      "Gi= 1 − ∑\n",
      "k= 1n\n",
      "pi,k2\n",
      "•pi,k is the ratio of class k instances among the training instances in the ith node.\n",
      "Scikit-Learn uses the CART algorithm, which produces only binary\n",
      "trees : nonleaf nodes always have two children (i.e., questions only\n",
      "have yes/no answers). However, other algorithms such as ID3 can\n",
      "produce Decision Trees with nodes that have more than two chil‐\n",
      "dren.\n",
      "Figure 6-2  shows this Decision Tree’s decision boundaries. The thick vertical line rep‐\n",
      "resents the decision boundary of the root node (depth 0): petal length = 2.45 cm.\n",
      "Since the left area is pure (only Iris-Setosa), it cannot be split any further. However,\n",
      "the right area is impure, so the depth-1 right node splits it at petal width = 1.75 cm\n",
      "(represented by the dashed line). Since max_depth  was set to 2, the Decision Tree\n",
      "stops right there. However, if you set max_depth  to 3, then the two depth-2 nodes\n",
      "would each add another decision boundary (represented by the dotted lines).\n",
      "180 | Chapter 6: Decision Trees\n",
      "Figure 6-2. Decision Tree decision boundaries\n",
      "Model Interpretation: White Box Versus Black Box\n",
      "As you can see Decision Trees are fairly intuitive and their decisions are easy to inter‐\n",
      "pret. Such models are often called white box models . In contrast, as we will see, Ran‐\n",
      "dom Forests or neural networks are generally considered black box models . They\n",
      "make great predictions, and you can easily check the calculations that they performed\n",
      "to make these predictions; nevertheless, it is usually hard to explain in simple terms\n",
      "why the predictions were made. For example, if a neural network says that a particu‐\n",
      "lar person appears on a picture, it is hard to know what actually contributed to this\n",
      "prediction: did the model recognize that person’s eyes? Her mouth? Her nose? Her\n",
      "shoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide\n",
      "nice and simple classification rules that can even be applied manually if need be (e.g.,\n",
      "for flower classification).\n",
      "Estimating Class Probabilities\n",
      "A Decision Tree can also estimate the probability that an instance belongs to a partic‐\n",
      "ular class k: first it traverses the tree to find the leaf node for this instance, and then it\n",
      "returns the ratio of training instances of class k in this node. For example, suppose\n",
      "you have found a flower whose petals are 5 cm long and 1.5 cm wide. The corre‐\n",
      "sponding leaf node is the depth-2 left node, so the Decision Tree should output the\n",
      "following probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54),\n",
      "and 9.3% for Iris-Virginica (5/54). And of course if you ask it to predict the class, it\n",
      "should output Iris-Versicolor (class 1) since it has the highest probability. Let’s check\n",
      "this:\n",
      ">>> tree_clf .predict_proba ([[5, 1.5]])\n",
      "array([[0.        , 0.90740741, 0.09259259]])\n",
      "Estimating Class Probabilities | 181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> tree_clf .predict([[5, 1.5]])\n",
      "array([1])\n",
      "Perfect! Notice that the estimated probabilities would be identical anywhere else in\n",
      "the bottom-right rectangle of Figure 6-2 —for example, if the petals were 6 cm long\n",
      "and 1.5 cm wide (even though it seems obvious that it would most likely be an Iris-\n",
      "Virginica in this case).\n",
      "The CART Training Algorithm\n",
      "Scikit-Learn uses the Classification  And Regression Tree  (CART) algorithm to train\n",
      "Decision Trees (also called “growing” trees). The idea is really quite simple: the algo‐\n",
      "rithm first splits the training set in two subsets using a single feature k and a thres‐\n",
      "hold tk (e.g., “petal length ≤  2.45 cm”). How does it choose k and tk? It searches for the\n",
      "pair ( k, tk) that produces the purest subsets (weighted by their size). The cost function\n",
      "that the algorithm tries to minimize is given by Equation 6-2 .\n",
      "Equation 6-2. CART cost function for classification\n",
      "Jk,tk=mleft\n",
      "mGleft+mright\n",
      "mGright\n",
      "whereGleft/rightmeasures the impurity of the left/right subset,\n",
      "mleft/rightis the number of instances in the left/right subset.\n",
      "Once it has successfully split the training set in two, it splits the subsets using the\n",
      "same logic, then the sub-subsets and so on, recursively. It stops recursing once it rea‐\n",
      "ches the maximum depth (defined by the max_depth  hyperparameter), or if it cannot\n",
      "find a split that will reduce impurity. A few other hyperparameters (described in a\n",
      "moment) control additional stopping conditions ( min_samples_split , min_sam\n",
      "ples_leaf , min_weight_fraction_leaf , and max_leaf_nodes ).\n",
      "182 | Chapter 6: Decision Trees\n",
      "2P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\n",
      "be verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\n",
      "in polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques‐\n",
      "tion is whether or not P = NP . If P ≠ NP (which seems likely), then no polynomial algorithm will ever be\n",
      "found for any NP-Complete problem (except perhaps on a quantum computer).\n",
      "3log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2).\n",
      "4A reduction of entropy is often called an information gain .\n",
      "As you can see, the CART algorithm is a greedy algorithm : it greed‐\n",
      "ily searches for an optimum split at the top level, then repeats the\n",
      "process at each level. It does not check whether or not the split will\n",
      "lead to the lowest possible impurity several levels down. A greedy\n",
      "algorithm often produces a reasonably good solution, but it is not\n",
      "guaranteed to be the optimal solution.\n",
      "Unfortunately, finding the optimal tree is known to be an NP-\n",
      "Complete  problem:2 it requires O(exp( m)) time, making the prob‐\n",
      "lem intractable even for fairly small training sets. This is why we\n",
      "must settle for a “reasonably good” solution.\n",
      "Computational Complexity\n",
      "Making predictions requires traversing the Decision Tree from the root to a leaf.\n",
      "Decision Trees are generally approximately balanced, so traversing the Decision Tree\n",
      "requires going through roughly O(log2(m)) nodes.3 Since each node only requires\n",
      "checking the value of one feature, the overall prediction complexity is just O(log2(m)),\n",
      "independent of the number of features. So predictions are very fast, even when deal‐\n",
      "ing with large training sets.\n",
      "However, the training algorithm compares all features (or less if max_features  is set)\n",
      "on all samples at each node. This results in a training complexity of O(n × m log(m)).\n",
      "For small training sets (less than a few thousand instances), Scikit-Learn can speed up\n",
      "training by presorting the data (set presort=True ), but this slows down training con‐\n",
      "siderably for larger training sets.\n",
      "Gini Impurity or Entropy?\n",
      "By default, the Gini impurity measure is used, but you can select the entropy  impurity\n",
      "measure instead by setting the criterion  hyperparameter to \"entropy\" . The concept\n",
      "of entropy originated in thermodynamics as a measure of molecular disorder:\n",
      "entropy approaches zero when molecules are still and well ordered. It later spread to a\n",
      "wide variety of domains, including Shannon’s information theory , where it measures\n",
      "the average information content of a message:4 entropy is zero when all messages are\n",
      "identical. In Machine Learning, it is frequently used as an impurity measure: a set’s\n",
      "Computational Complexity | 183\n",
      "5See Sebastian Raschka’s interesting analysis for more details .entropy is zero when it contains instances of only one class. Equation 6-3  shows the\n",
      "definition of the entropy of the ith node. For example, the depth-2 left node in\n",
      "Figure 6-1  has an entropy equal to −49\n",
      "54log249\n",
      "54−5\n",
      "54log25\n",
      "54 ≈ 0.445.\n",
      "Equation 6-3. Entropy\n",
      "Hi= − ∑\n",
      "k= 1\n",
      "pi,k≠ 0n\n",
      "pi,klog2pi,k\n",
      "So should you use Gini impurity or entropy? The truth is, most of the time it does not\n",
      "make a big difference: they lead to similar trees. Gini impurity is slightly faster to\n",
      "compute, so it is a good default. However, when they differ, Gini impurity tends to\n",
      "isolate the most frequent class in its own branch of the tree, while entropy tends to\n",
      "produce slightly more balanced trees.5\n",
      "Regularization Hyperparameters\n",
      "Decision Trees make very few assumptions about the training data (as opposed to lin‐\n",
      "ear models, which obviously assume that the data is linear, for example). If left\n",
      "unconstrained, the tree structure will adapt itself to the training data, fitting it very\n",
      "closely, and most likely overfitting it. Such a model is often called a nonparametric\n",
      "model , not because it does not have any parameters (it often has a lot) but because the\n",
      "number of parameters is not determined prior to training, so the model structure is\n",
      "free to stick closely to the data. In contrast, a parametric model  such as a linear model\n",
      "has a predetermined number of parameters, so its degree of freedom is limited,\n",
      "reducing the risk of overfitting (but increasing the risk of underfitting).\n",
      "To avoid overfitting the training data, you need to restrict the Decision Tree’s freedom\n",
      "during training. As you know by now, this is called regularization. The regularization\n",
      "hyperparameters depend on the algorithm used, but generally you can at least restrict\n",
      "the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\n",
      "max_depth  hyperparameter (the default value is None , which means unlimited).\n",
      "Reducing max_depth  will regularize the model and thus reduce the risk of overfitting.\n",
      "The DecisionTreeClassifier  class has a few other parameters that similarly restrict\n",
      "the shape of the Decision Tree: min_samples_split  (the minimum number of sam‐\n",
      "ples a node must have before it can be split), min_samples_leaf  (the minimum num‐\n",
      "ber of samples a leaf node must have), min_weight_fraction_leaf  (same as\n",
      "min_samples_leaf  but expressed as a fraction of the total number of weighted\n",
      "184 | Chapter 6: Decision Trees\n",
      "instances), max_leaf_nodes  (maximum number of leaf nodes), and max_features\n",
      "(maximum number of features that are evaluated for splitting at each node). Increas‐\n",
      "ing min_*  hyperparameters or reducing max_*  hyperparameters will regularize the\n",
      "model.\n",
      "Other algorithms work by first training the Decision Tree without\n",
      "restrictions, then pruning  (deleting) unnecessary nodes. A node\n",
      "whose children are all leaf nodes is considered unnecessary if the\n",
      "purity improvement it provides is not statistically significant . Stan‐\n",
      "dard statistical tests, such as the χ2 test, are used to estimate the\n",
      "probability that the improvement is purely the result of chance\n",
      "(which is called the null hypothesis ). If this probability, called the p-\n",
      "value , is higher than a given threshold (typically 5%, controlled by\n",
      "a hyperparameter), then the node is considered unnecessary and its\n",
      "children are deleted. The pruning continues until all unnecessary\n",
      "nodes have been pruned.\n",
      "Figure 6-3  shows two Decision Trees trained on the moons dataset (introduced in\n",
      "Chapter 5 ). On the left, the Decision Tree is trained with the default hyperparameters\n",
      "(i.e., no restrictions), and on the right the Decision Tree is trained with min_sam\n",
      "ples_leaf=4 . It is quite obvious that the model on the left is overfitting, and the\n",
      "model on the right will probably generalize better.\n",
      "Figure 6-3. Regularization using min_samples_leaf\n",
      "Regression\n",
      "Decision Trees are also capable of performing regression tasks. Let’s build a regres‐\n",
      "sion tree using Scikit-Learn’s DecisionTreeRegressor  class, training it on a noisy\n",
      "quadratic dataset with max_depth=2 :\n",
      "from sklearn.tree  import DecisionTreeRegressor\n",
      "Regression | 185\n",
      "tree_reg  = DecisionTreeRegressor (max_depth =2)\n",
      "tree_reg .fit(X, y)\n",
      "The resulting tree is represented on Figure 6-4 .\n",
      "Figure 6-4. A Decision Tree for regression\n",
      "This tree looks very similar to the classification tree you built earlier. The main differ‐\n",
      "ence is that instead of predicting a class in each node, it predicts a value. For example,\n",
      "suppose you want to make a prediction for a new instance with x1 = 0.6. Y ou traverse\n",
      "the tree starting at the root, and you eventually reach the leaf node that predicts\n",
      "value=0.1106 . This prediction is simply the average target value of the 110 training\n",
      "instances associated to this leaf node. This prediction results in a Mean Squared Error\n",
      "(MSE) equal to 0.0151 over these 110 instances.\n",
      "This model’s predictions are represented on the left of Figure 6-5 . If you set\n",
      "max_depth=3 , you get the predictions represented on the right. Notice how the pre‐\n",
      "dicted value for each region is always the average target value of the instances in that\n",
      "region. The algorithm splits each region in a way that makes most training instances\n",
      "as close as possible to that predicted value.\n",
      "186 | Chapter 6: Decision Trees\n",
      "Figure 6-5. Predictions of two Decision Tree regression models\n",
      "The CART algorithm works mostly the same way as earlier, except that instead of try‐\n",
      "ing to split the training set in a way that minimizes impurity, it now tries to split the\n",
      "training set in a way that minimizes the MSE. Equation 6-4  shows the cost function\n",
      "that the algorithm tries to minimize.\n",
      "Equation 6-4. CART cost function for regression\n",
      "Jk,tk=mleft\n",
      "mMSEleft+mright\n",
      "mMSErightwhereMSEnode=∑\n",
      "i∈nodeynode−yi2\n",
      "ynode=1\n",
      "mnode∑\n",
      "i∈nodeyi\n",
      "Just like for classification tasks, Decision Trees are prone to overfitting when dealing\n",
      "with regression tasks. Without any regularization (i.e., using the default hyperpara‐\n",
      "meters), you get the predictions on the left of Figure 6-6 . It is obviously overfitting\n",
      "the training set very badly. Just setting min_samples_leaf=10  results in a much more\n",
      "reasonable model, represented on the right of Figure 6-6 .\n",
      "Figure 6-6. Regularizing a Decision Tree regressor\n",
      "Regression | 187\n",
      "6It randomly selects the set of features to evaluate at each node.Instability\n",
      "Hopefully by now you are convinced that Decision Trees have a lot going for them:\n",
      "they are simple to understand and interpret, easy to use, versatile, and powerful.\n",
      "However they do have a few limitations. First, as you may have noticed, Decision\n",
      "Trees love orthogonal decision boundaries (all splits are perpendicular to an axis),\n",
      "which makes them sensitive to training set rotation. For example, Figure 6-7  shows a\n",
      "simple linearly separable dataset: on the left, a Decision Tree can split it easily, while\n",
      "on the right, after the dataset is rotated by 45°, the decision boundary looks unneces‐\n",
      "sarily convoluted. Although both Decision Trees fit the training set perfectly, it is very\n",
      "likely that the model on the right will not generalize well. One way to limit this prob‐\n",
      "lem is to use PCA (see Chapter 8 ), which often results in a better orientation of the\n",
      "training data.\n",
      "Figure 6-7. Sensitivity to training set rotation\n",
      "More generally, the main issue with Decision Trees is that they are very sensitive to\n",
      "small variations in the training data. For example, if you just remove the widest Iris-\n",
      "Versicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\n",
      "and train a new Decision Tree, you may get the model represented in Figure 6-8 . As\n",
      "you can see, it looks very different from the previous Decision Tree ( Figure 6-2 ).\n",
      "Actually, since the training algorithm used by Scikit-Learn is stochastic6 you may\n",
      "get very different models even on the same training data (unless you set the\n",
      "random_state  hyperparameter).\n",
      "188 | Chapter 6: Decision Trees\n",
      "Figure 6-8. Sensitivity to training set details\n",
      "Random Forests can limit this instability by averaging predictions over many trees, as\n",
      "we will see in the next chapter.\n",
      "Exercises\n",
      "1.What is the approximate depth of a Decision Tree trained (without restrictions)\n",
      "on a training set with 1 million instances?\n",
      "2.Is a node’s Gini impurity generally lower or greater than its parent’s? Is it gener‐\n",
      "ally lower/greater, or always  lower/greater?\n",
      "3.If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\n",
      "max_depth ?\n",
      "4.If a Decision Tree is underfitting the training set, is it a good idea to try scaling\n",
      "the input features?\n",
      "5.If it takes one hour to train a Decision Tree on a training set containing 1 million\n",
      "instances, roughly how much time will it take to train another Decision Tree on a\n",
      "training set containing 10 million instances?\n",
      "6.If your training set contains 100,000 instances, will setting presort=True  speed\n",
      "up training?\n",
      "7.Train and fine-tune a Decision Tree for the moons dataset.\n",
      "a.Generate a moons dataset using make_moons(n_samples=10000, noise=0.4) .\n",
      "b.Split it into a training set and a test set using train_test_split() .\n",
      "Exercises | 189\n",
      "c.Use grid search with cross-validation (with the help of the GridSearchCV\n",
      "class) to find good hyperparameter values for a DecisionTreeClassifier . \n",
      "Hint: try various values for max_leaf_nodes .\n",
      "d.Train it on the full training set using these hyperparameters, and measure\n",
      "your model’s performance on the test set. Y ou should get roughly 85% to 87%\n",
      "accuracy.\n",
      "8.Grow a forest.\n",
      "a.Continuing the previous exercise, generate 1,000 subsets of the training set,\n",
      "each containing 100 instances selected randomly. Hint: you can use Scikit-\n",
      "Learn’s ShuffleSplit  class for this.\n",
      "b.Train one Decision Tree on each subset, using the best hyperparameter values\n",
      "found above. Evaluate these 1,000 Decision Trees on the test set. Since they\n",
      "were trained on smaller sets, these Decision Trees will likely perform worse\n",
      "than the first Decision Tree, achieving only about 80% accuracy.\n",
      "c.Now comes the magic. For each test set instance, generate the predictions of\n",
      "the 1,000 Decision Trees, and keep only the most frequent prediction (you can\n",
      "use SciPy’s mode()  function for this). This gives you majority-vote predictions\n",
      "over the test set.\n",
      "d.Evaluate these predictions on the test set: you should obtain a slightly higher\n",
      "accuracy than your first model (about 0.5 to 1.5% higher). Congratulations,\n",
      "you have trained a Random Forest classifier!\n",
      "Solutions to these exercises are available in ???.\n",
      "190 | Chapter 6: Decision Trees\n",
      "CHAPTER 7\n",
      "Ensemble Learning and Random Forests\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 7 in the final\n",
      "release of the book.\n",
      "Suppose you ask a complex question to thousands of random people, then aggregate\n",
      "their answers. In many cases you will find that this aggregated answer is better than\n",
      "an expert’s answer. This is called the wisdom of the crowd . Similarly, if you aggregate\n",
      "the predictions of a group of predictors (such as classifiers or regressors), you will\n",
      "often get better predictions than with the best individual predictor. A group of pre‐\n",
      "dictors is called an ensemble ; thus, this technique is called Ensemble Learning , and an\n",
      "Ensemble Learning algorithm is called an Ensemble method .\n",
      "For example, you can train a group of Decision Tree classifiers, each on a different\n",
      "random subset of the training set. To make predictions, you just obtain the predic‐\n",
      "tions of all individual trees, then predict the class that gets the most votes (see the last\n",
      "exercise in Chapter 6 ). Such an ensemble of Decision Trees is called a Random Forest , \n",
      "and despite its simplicity, this is one of the most powerful Machine Learning algo‐\n",
      "rithms available today.\n",
      "Moreover, as we discussed in Chapter 2 , you will often use Ensemble methods near\n",
      "the end of a project, once you have already built a few good predictors, to combine\n",
      "them into an even better predictor. In fact, the winning solutions in Machine Learn‐\n",
      "ing competitions often involve several Ensemble methods (most famously in the Net‐\n",
      "flix Prize competition ).\n",
      "In this chapter we will discuss the most popular Ensemble methods, including bag‐\n",
      "ging, boosting , stacking , and a few others. We will also explore Random Forests.\n",
      "191\n",
      "Voting Classifiers\n",
      "Suppose you have trained a few classifiers, each one achieving about 80% accuracy.\n",
      "Y ou may have a Logistic Regression classifier, an SVM classifier, a Random Forest\n",
      "classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1 ).\n",
      "Figure 7-1. Training diverse classifiers\n",
      "A very simple way to create an even better classifier is to aggregate the predictions of\n",
      "each classifier and predict the class that gets the most votes. This majority-vote classi‐\n",
      "fier is called a hard voting  classifier (see Figure 7-2 ).\n",
      "Figure 7-2. Hard voting classifier  predictions\n",
      "192 | Chapter 7: Ensemble Learning and Random Forests\n",
      "Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the\n",
      "best classifier in the ensemble. In fact, even if each classifier is a weak learner  (mean‐\n",
      "ing it does only slightly better than random guessing), the ensemble can still be a\n",
      "strong learner  (achieving high accuracy), provided there are a sufficient number of\n",
      "weak learners and they are sufficiently diverse.\n",
      "How is this possible? The following analogy can help shed some light on this mystery.\n",
      "Suppose you have a slightly biased coin that has a 51% chance of coming up heads,\n",
      "and 49% chance of coming up tails. If you toss it 1,000 times, you will generally get\n",
      "more or less 510 heads and 490 tails, and hence a majority of heads. If you do the\n",
      "math, you will find that the probability of obtaining a majority of heads after 1,000\n",
      "tosses is close to 75%. The more you toss the coin, the higher the probability (e.g.,\n",
      "with 10,000 tosses, the probability climbs over 97%). This is due to the law of large\n",
      "numbers : as you keep tossing the coin, the ratio of heads gets closer and closer to the\n",
      "probability of heads (51%). Figure 7-3  shows 10 series of biased coin tosses. Y ou can\n",
      "see that as the number of tosses increases, the ratio of heads approaches 51%. Eventu‐\n",
      "ally all 10 series end up so close to 51% that they are consistently above 50%.\n",
      "Figure 7-3. The law of large numbers\n",
      "Similarly, suppose you build an ensemble containing 1,000 classifiers that are individ‐\n",
      "ually correct only 51% of the time (barely better than random guessing). If you pre‐\n",
      "dict the majority voted class, you can hope for up to 75% accuracy! However, this is\n",
      "only true if all classifiers are perfectly independent, making uncorrelated errors,\n",
      "which is clearly not the case since they are trained on the same data. They are likely to\n",
      "make the same types of errors, so there will be many majority votes for the wrong\n",
      "class, reducing the ensemble’s accuracy.\n",
      "Voting Classifiers  | 193\n",
      "Ensemble methods work best when the predictors are as independ‐\n",
      "ent from one another as possible. One way to get diverse classifiers\n",
      "is to train them using very different algorithms. This increases the\n",
      "chance that they will make very different types of errors, improving\n",
      "the ensemble’s accuracy.\n",
      "The following code creates and trains a voting classifier in Scikit-Learn, composed of\n",
      "three diverse classifiers (the training set is the moons dataset, introduced in Chap‐\n",
      "ter 5 ):\n",
      "from sklearn.ensemble  import RandomForestClassifier\n",
      "from sklearn.ensemble  import VotingClassifier\n",
      "from sklearn.linear_model  import LogisticRegression\n",
      "from sklearn.svm  import SVC\n",
      "log_clf = LogisticRegression ()\n",
      "rnd_clf = RandomForestClassifier ()\n",
      "svm_clf = SVC()\n",
      "voting_clf  = VotingClassifier (\n",
      "    estimators =[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
      "    voting='hard')\n",
      "voting_clf .fit(X_train, y_train)\n",
      "Let’s look at each classifier’s accuracy on the test set:\n",
      ">>> from sklearn.metrics  import accuracy_score\n",
      ">>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf ):\n",
      "...     clf.fit(X_train, y_train)\n",
      "...     y_pred = clf.predict(X_test)\n",
      "...     print(clf.__class__ .__name__ , accuracy_score (y_test, y_pred))\n",
      "...\n",
      "LogisticRegression 0.864\n",
      "RandomForestClassifier 0.896\n",
      "SVC 0.888\n",
      "VotingClassifier 0.904\n",
      "There you have it! The voting classifier slightly outperforms all the individual classifi‐\n",
      "ers.\n",
      "If all classifiers are able to estimate class probabilities (i.e., they have a pre\n",
      "dict_proba()  method), then you can tell Scikit-Learn to predict the class with the\n",
      "highest class probability, averaged over all the individual classifiers. This is called soft\n",
      "voting . It often achieves higher performance than hard voting because it gives more\n",
      "weight to highly confident votes. All you need to do is replace voting=\"hard\"  with\n",
      "voting=\"soft\"  and ensure that all classifiers can estimate class probabilities. This is\n",
      "not the case of the SVC class by default, so you need to set its probability  hyperpara‐\n",
      "meter to True  (this will make the SVC class use cross-validation to estimate class prob‐\n",
      "abilities, slowing down training, and it will add a predict_proba()  method). If you\n",
      "194 | Chapter 7: Ensemble Learning and Random Forests\n",
      "1“Bagging Predictors, ” L. Breiman (1996).\n",
      "2In statistics, resampling with replacement is called bootstrapping .\n",
      "3“Pasting small votes for classification in large databases and on-line, ” L. Breiman (1999).modify the preceding code to use soft voting, you will find that the voting classifier\n",
      "achieves over 91.2% accuracy!\n",
      "Bagging and Pasting\n",
      "One way to get a diverse set of classifiers is to use very different training algorithms,\n",
      "as just discussed. Another approach is to use the same training algorithm for every\n",
      "predictor, but to train them on different random subsets of the training set. When\n",
      "sampling is performed with  replacement, this method is called bagging1 (short for\n",
      "bootstrap aggregating2). When sampling is performed without  replacement, it is called\n",
      "pasting .3\n",
      "In other words, both bagging and pasting allow training instances to be sampled sev‐\n",
      "eral times across multiple predictors, but only bagging allows training instances to be\n",
      "sampled several times for the same predictor. This sampling and training process is\n",
      "represented in Figure 7-4 .\n",
      "Figure 7-4. Pasting/bagging training set sampling and training\n",
      "Once all predictors are trained, the ensemble can make a prediction for a new\n",
      "instance by simply aggregating the predictions of all predictors. The aggregation\n",
      "function is typically the statistical mode  (i.e., the most frequent prediction, just like a\n",
      "hard voting classifier) for classification, or the average for regression. Each individual\n",
      "Bagging and Pasting | 195\n",
      "4Bias and variance were introduced in Chapter 4 .\n",
      "5max_samples  can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances\n",
      "to sample is equal to the size of the training set times max_samples .\n",
      "predictor has a higher bias than if it were trained on the original training set, but\n",
      "aggregation reduces both bias and variance.4 Generally, the net result is that the\n",
      "ensemble has a similar bias but a lower variance than a single predictor trained on the\n",
      "original training set.\n",
      "As you can see in Figure 7-4 , predictors can all be trained in parallel, via different\n",
      "CPU cores or even different servers. Similarly, predictions can be made in parallel.\n",
      "This is one of the reasons why bagging and pasting are such popular methods: they\n",
      "scale very well.\n",
      "Bagging and Pasting in Scikit-Learn\n",
      "Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\n",
      "sifier  class (or BaggingRegressor  for regression). The following code trains an\n",
      "ensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran‐\n",
      "domly sampled from the training set with replacement (this is an example of bagging,\n",
      "but if you want to use pasting instead, just set bootstrap=False ). The n_jobs  param‐\n",
      "eter tells Scikit-Learn the number of CPU cores to use for training and predictions\n",
      "(–1 tells Scikit-Learn to use all available cores):\n",
      "from sklearn.ensemble  import BaggingClassifier\n",
      "from sklearn.tree  import DecisionTreeClassifier\n",
      "bag_clf = BaggingClassifier (\n",
      "    DecisionTreeClassifier (), n_estimators =500,\n",
      "    max_samples =100, bootstrap =True, n_jobs=-1)\n",
      "bag_clf.fit(X_train, y_train)\n",
      "y_pred = bag_clf.predict(X_test)\n",
      "The BaggingClassifier  automatically performs soft voting\n",
      "instead of hard voting if the base classifier can estimate class proba‐\n",
      "bilities (i.e., if it has a predict_proba()  method), which is the case\n",
      "with Decision Trees classifiers.\n",
      "Figure 7-5  compares the decision boundary of a single Decision Tree with the deci‐\n",
      "sion boundary of a bagging ensemble of 500 trees (from the preceding code), both\n",
      "trained on the moons dataset. As you can see, the ensemble’s predictions will likely\n",
      "generalize much better than the single Decision Tree’s predictions: the ensemble has a\n",
      "comparable bias but a smaller variance (it makes roughly the same number of errors\n",
      "on the training set, but the decision boundary is less irregular).\n",
      "196 | Chapter 7: Ensemble Learning and Random Forests\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6As m grows, this ratio approaches 1 – exp(–1) ≈ 63.212%.\n",
      "Figure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees\n",
      "Bootstrapping introduces a bit more diversity in the subsets that each predictor is\n",
      "trained on, so bagging ends up with a slightly higher bias than pasting, but this also\n",
      "means that predictors end up being less correlated so the ensemble’s variance is\n",
      "reduced. Overall, bagging often results in better models, which explains why it is gen‐\n",
      "erally preferred. However, if you have spare time and CPU power you can use cross-\n",
      "validation to evaluate both bagging and pasting and select the one that works best.\n",
      "Out-of-Bag Evaluation\n",
      "With bagging, some instances may be sampled several times for any given predictor,\n",
      "while others may not be sampled at all. By default a BaggingClassifier  samples m\n",
      "training instances with replacement ( bootstrap=True ), where m is the size of the\n",
      "training set. This means that only about 63% of the training instances are sampled on\n",
      "average for each predictor.6 The remaining 37% of the training instances that are not\n",
      "sampled are called out-of-bag  (oob) instances. Note that they are not the same 37%\n",
      "for all predictors.\n",
      "Since a predictor never sees the oob instances during training, it can be evaluated on\n",
      "these instances, without the need for a separate validation set. Y ou can evaluate the\n",
      "ensemble itself by averaging out the oob evaluations of each predictor.\n",
      "In Scikit-Learn, you can set oob_score=True  when creating a BaggingClassifier  to\n",
      "request an automatic oob evaluation after training. The following code demonstrates\n",
      "this. The resulting evaluation score is available through the oob_score_  variable:\n",
      ">>> bag_clf = BaggingClassifier (\n",
      "...     DecisionTreeClassifier (), n_estimators =500,\n",
      "...     bootstrap =True, n_jobs=-1, oob_score =True)\n",
      "...\n",
      ">>> bag_clf.fit(X_train, y_train)\n",
      "Bagging and Pasting | 197\n",
      "7“Ensembles on Random Patches, ” G. Louppe and P . Geurts (2012).\n",
      "8“The random subspace method for constructing decision forests, ” Tin Kam Ho (1998).>>> bag_clf.oob_score_\n",
      "0.90133333333333332\n",
      "According to this oob evaluation, this BaggingClassifier  is likely to achieve about\n",
      "90.1% accuracy on the test set. Let’s verify this:\n",
      ">>> from sklearn.metrics  import accuracy_score\n",
      ">>> y_pred = bag_clf.predict(X_test)\n",
      ">>> accuracy_score (y_test, y_pred)\n",
      "0.91200000000000003\n",
      "We get 91.2% accuracy on the test set—close enough!\n",
      "The oob decision function for each training instance is also available through the\n",
      "oob_decision_function_  variable. In this case (since the base estimator has a pre\n",
      "dict_proba()  method) the decision function returns the class probabilities for each\n",
      "training instance. For example, the oob evaluation estimates that the first training\n",
      "instance has a 68.25% probability of belonging to the positive class (and 31.75% of\n",
      "belonging to the negative class):\n",
      ">>> bag_clf.oob_decision_function_\n",
      "array([[0.31746032, 0.68253968],\n",
      "       [0.34117647, 0.65882353],\n",
      "       [1.        , 0.        ],\n",
      "       ...\n",
      "       [1.        , 0.        ],\n",
      "       [0.03108808, 0.96891192],\n",
      "       [0.57291667, 0.42708333]])\n",
      "Random Patches and Random Subspaces\n",
      "The BaggingClassifier  class supports sampling the features as well. This is con‐\n",
      "trolled by two hyperparameters: max_features  and bootstrap_features . They work\n",
      "the same way as max_samples  and bootstrap , but for feature sampling instead of\n",
      "instance sampling. Thus, each predictor will be trained on a random subset of the\n",
      "input features.\n",
      "This is particularly useful when you are dealing with high-dimensional inputs (such\n",
      "as images). Sampling both training instances and features is called the Random\n",
      "Patches  method .7 Keeping all training instances (i.e., bootstrap=False  and max_sam\n",
      "ples=1.0 ) but sampling features (i.e., bootstrap_features=True  and/or max_fea\n",
      "tures  smaller than 1.0) is called the Random Subspaces  method .8\n",
      "198 | Chapter 7: Ensemble Learning and Random Forests\n",
      "9“Random Decision Forests, ” T. Ho (1995).\n",
      "10The BaggingClassifier  class remains useful if you want a bag of something other than Decision Trees.\n",
      "11There are a few notable exceptions: splitter  is absent (forced to \"random\" ), presort  is absent (forced to\n",
      "False ), max_samples  is absent (forced to 1.0), and base_estimator  is absent (forced to DecisionTreeClassi\n",
      "fier  with the provided hyperparameters).Sampling features results in even more predictor diversity, trading a bit more bias for\n",
      "a lower variance.\n",
      "Random Forests\n",
      "As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\n",
      "trained via the bagging method (or sometimes pasting), typically with max_samples\n",
      "set to the size of the training set. Instead of building a BaggingClassifier  and pass‐\n",
      "ing it a DecisionTreeClassifier , you can instead use the RandomForestClassifier\n",
      "class, which is more convenient and optimized for Decision Trees10 (similarly, there is\n",
      "a RandomForestRegressor  class for regression tasks). The following code trains a\n",
      "Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using\n",
      "all available CPU cores:\n",
      "from sklearn.ensemble  import RandomForestClassifier\n",
      "rnd_clf = RandomForestClassifier (n_estimators =500, max_leaf_nodes =16, n_jobs=-1)\n",
      "rnd_clf.fit(X_train, y_train)\n",
      "y_pred_rf  = rnd_clf.predict(X_test)\n",
      "With a few exceptions, a RandomForestClassifier  has all the hyperparameters of a\n",
      "DecisionTreeClassifier  (to control how trees are grown), plus all the hyperpara‐\n",
      "meters of a BaggingClassifier  to control the ensemble itself.11\n",
      "The Random Forest algorithm introduces extra randomness when growing trees;\n",
      "instead of searching for the very best feature when splitting a node (see Chapter 6 ), it\n",
      "searches for the best feature among a random subset of features. This results in a\n",
      "greater tree diversity, which (once again) trades a higher bias for a lower variance,\n",
      "generally yielding an overall better model. The following BaggingClassifier  is\n",
      "roughly equivalent to the previous RandomForestClassifier :\n",
      "bag_clf = BaggingClassifier (\n",
      "    DecisionTreeClassifier (splitter =\"random\" , max_leaf_nodes =16),\n",
      "    n_estimators =500, max_samples =1.0, bootstrap =True, n_jobs=-1)\n",
      "Random Forests | 199\n",
      "12“Extremely randomized trees, ” P . Geurts, D. Ernst, L. Wehenkel (2005).\n",
      "Extra-Trees\n",
      "When you are growing a tree in a Random Forest, at each node only a random subset\n",
      "of the features is considered for splitting (as discussed earlier). It is possible to make\n",
      "trees even more random by also using random thresholds for each feature rather than\n",
      "searching for the best possible thresholds (like regular Decision Trees do).\n",
      "A forest of such extremely random trees is simply called an Extremely Randomized\n",
      "Trees  ensemble12 (or Extra-Trees  for short). Once again, this trades more bias for a\n",
      "lower variance. It also makes Extra-Trees much faster to train than regular Random\n",
      "Forests since finding the best possible threshold for each feature at every node is one\n",
      "of the most time-consuming tasks of growing a tree.\n",
      "Y ou can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\n",
      "class. Its API is identical to the RandomForestClassifier  class. Similarly, the Extra\n",
      "TreesRegressor  class has the same API as the RandomForestRegressor  class.\n",
      "It is hard to tell in advance whether a RandomForestClassifier\n",
      "will perform better or worse than an ExtraTreesClassifier . Gen‐\n",
      "erally, the only way to know is to try both and compare them using\n",
      "cross-validation (and tuning the hyperparameters using grid\n",
      "search).\n",
      "Feature Importance\n",
      "Y et another great quality of Random Forests is that they make it easy to measure the \n",
      "relative importance of each feature. Scikit-Learn measures a feature’s importance by\n",
      "looking at how much the tree nodes that use that feature reduce impurity on average\n",
      "(across all trees in the forest). More precisely, it is a weighted average, where each\n",
      "node’s weight is equal to the number of training samples that are associated with it\n",
      "(see Chapter 6 ).\n",
      "Scikit-Learn computes this score automatically for each feature after training, then it\n",
      "scales the results so that the sum of all importances is equal to 1. Y ou can access the\n",
      "result using the feature_importances_  variable. For example, the following code\n",
      "trains a RandomForestClassifier  on the iris dataset (introduced in Chapter 4 ) and\n",
      "outputs each feature’s importance. It seems that the most important features are the\n",
      "petal length (44%) and width (42%), while sepal length and width are rather unim‐\n",
      "portant in comparison (11% and 2%, respectively).\n",
      "200 | Chapter 7: Ensemble Learning and Random Forests\n",
      ">>> from sklearn.datasets  import load_iris\n",
      ">>> iris = load_iris ()\n",
      ">>> rnd_clf = RandomForestClassifier (n_estimators =500, n_jobs=-1)\n",
      ">>> rnd_clf.fit(iris[\"data\"], iris[\"target\" ])\n",
      ">>> for name, score in zip(iris[\"feature_names\" ], rnd_clf.feature_importances_ ):\n",
      "...     print(name, score)\n",
      "...\n",
      "sepal length (cm) 0.112492250999\n",
      "sepal width (cm) 0.0231192882825\n",
      "petal length (cm) 0.441030464364\n",
      "petal width (cm) 0.423357996355\n",
      "Similarly, if you train a Random Forest classifier on the MNIST dataset (introduced\n",
      "in Chapter 3 ) and plot each pixel’s importance, you get the image represented in\n",
      "Figure 7-6 .\n",
      "Figure 7-6. MNIST pixel importance (according to a Random Forest classifier)\n",
      "Random Forests are very handy to get a quick understanding of what features\n",
      "actually matter, in particular if you need to perform feature selection.\n",
      "Boosting\n",
      "Boosting  (originally called hypothesis boosting ) refers to any Ensemble method that\n",
      "can combine several weak learners into a strong learner. The general idea of most\n",
      "boosting methods is to train predictors sequentially, each trying to correct its prede‐\n",
      "cessor. There are many boosting methods available, but by far the most popular are\n",
      "Boosting | 201\n",
      "13“ A Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting, ” Y oav Freund,\n",
      "Robert E. Schapire (1997).\n",
      "14This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they\n",
      "are slow and tend to be unstable with AdaBoost.AdaBoost13 (short for Adaptive Boosting ) and Gradient Boosting . Let’s start with Ada‐\n",
      "Boost.\n",
      "AdaBoost\n",
      "One way for a new predictor to correct its predecessor is to pay a bit more attention\n",
      "to the training instances that the predecessor underfitted. This results in new predic‐\n",
      "tors focusing more and more on the hard cases. This is the technique used by Ada‐\n",
      "Boost.\n",
      "For example, to build an AdaBoost classifier, a first base classifier (such as a Decision\n",
      "Tree) is trained and used to make predictions on the training set. The relative weight\n",
      "of misclassified training instances is then increased. A second classifier is trained\n",
      "using the updated weights and again it makes predictions on the training set, weights\n",
      "are updated, and so on (see Figure 7-7 ).\n",
      "Figure 7-7. AdaBoost sequential training with instance weight updates\n",
      "Figure 7-8  shows the decision boundaries of five consecutive predictors on the\n",
      "moons dataset (in this example, each predictor is a highly regularized SVM classifier\n",
      "with an RBF kernel14). The first classifier gets many instances wrong, so their weights\n",
      "202 | Chapter 7: Ensemble Learning and Random Forests\n",
      "get boosted. The second classifier therefore does a better job on these instances, and\n",
      "so on. The plot on the right represents the same sequence of predictors except that\n",
      "the learning rate is halved (i.e., the misclassified instance weights are boosted half as\n",
      "much at every iteration). As you can see, this sequential learning technique has some\n",
      "similarities with Gradient Descent, except that instead of tweaking a single predictor’s\n",
      "parameters to minimize a cost function, AdaBoost adds predictors to the ensemble,\n",
      "gradually making it better.\n",
      "Figure 7-8. Decision boundaries of consecutive predictors\n",
      "Once all predictors are trained, the ensemble makes predictions very much like bag‐\n",
      "ging or pasting, except that predictors have different weights depending on their\n",
      "overall accuracy on the weighted training set.\n",
      "There is one important drawback to this sequential learning techni‐\n",
      "que: it cannot be parallelized (or only partially), since each predic‐\n",
      "tor can only be trained after the previous predictor has been\n",
      "trained and evaluated. As a result, it does not scale as well as bag‐\n",
      "ging or pasting.\n",
      "Let’s take a closer look at the AdaBoost algorithm. Each instance weight w(i) is initially\n",
      "set to 1\n",
      "m. A first predictor is trained and its weighted error rate r1 is computed on the\n",
      "training set; see Equation 7-1 .\n",
      "Equation 7-1. Weighted error rate of the jth predictor\n",
      "rj=∑\n",
      "i= 1\n",
      "yji≠yim\n",
      "wi\n",
      "∑\n",
      "i= 1m\n",
      "wiwhere yjiis the jthpredictor’s prediction for the ithinstance.\n",
      "Boosting | 203\n",
      "15The original AdaBoost algorithm does not use a learning rate hyperparameter.The predictor’s weight αj is then computed using Equation 7-2 , where η is the learn‐\n",
      "ing rate hyperparameter (defaults to 1).15 The more accurate the predictor is, the\n",
      "higher its weight will be. If it is just guessing randomly, then its weight will be close to\n",
      "zero. However, if it is most often wrong (i.e., less accurate than random guessing),\n",
      "then its weight will be negative.\n",
      "Equation 7-2. Predictor weight\n",
      "αj=ηlog1 −rj\n",
      "rj\n",
      "Next the instance weights are updated using Equation 7-3 : the misclassified instances\n",
      "are boosted.\n",
      "Equation 7-3. Weight update rule\n",
      "fori= 1, 2,⋯,m\n",
      "wiwiifyji=yi\n",
      "wiexp αjifyji≠yi\n",
      "Then all the instance weights are normalized (i.e., divided by ∑i= 1mwi).\n",
      "Finally, a new predictor is trained using the updated weights, and the whole process is\n",
      "repeated (the new predictor’s weight is computed, the instance weights are updated,\n",
      "then another predictor is trained, and so on). The algorithm stops when the desired\n",
      "number of predictors is reached, or when a perfect predictor is found.\n",
      "To make predictions, AdaBoost simply computes the predictions of all the predictors\n",
      "and weighs them using the predictor weights αj. The predicted class is the one that\n",
      "receives the majority of weighted votes (see Equation 7-4 ).\n",
      "Equation 7-4. AdaBoost predictions\n",
      "yx= argmax\n",
      "k∑\n",
      "j= 1\n",
      "yjx=kN\n",
      "αjwhere Nis the number of predictors.\n",
      "204 | Chapter 7: Ensemble Learning and Random Forests\n",
      "16For more details, see “Multi-Class AdaBoost, ” J. Zhu et al. (2006).\n",
      "17First introduced in “ Arcing the Edge, ” L. Breiman (1997), and further developed in the paper “Greedy Func‐\n",
      "tion Approximation: A Gradient Boosting Machine, ” Jerome H. Friedman (1999).\n",
      "Scikit-Learn actually uses a multiclass version of AdaBoost called SAMME16 (which\n",
      "stands for Stagewise Additive Modeling using a Multiclass Exponential loss function ).\n",
      "When there are just two classes, SAMME is equivalent to AdaBoost. Moreover, if the\n",
      "predictors can estimate class probabilities (i.e., if they have a predict_proba()\n",
      "method), Scikit-Learn can use a variant of SAMME called SAMME.R  (the R stands\n",
      "for “Real”), which relies on class probabilities rather than predictions and generally\n",
      "performs better.\n",
      "The following code trains an AdaBoost classifier based on 200 Decision Stumps  using\n",
      "Scikit-Learn’s AdaBoostClassifier  class (as you might expect, there is also an Ada\n",
      "BoostRegressor  class). A Decision Stump is a Decision Tree with max_depth=1 —in\n",
      "other words, a tree composed of a single decision node plus two leaf nodes. This is\n",
      "the default base estimator for the AdaBoostClassifier  class:\n",
      "from sklearn.ensemble  import AdaBoostClassifier\n",
      "ada_clf = AdaBoostClassifier (\n",
      "    DecisionTreeClassifier (max_depth =1), n_estimators =200,\n",
      "    algorithm =\"SAMME.R\" , learning_rate =0.5)\n",
      "ada_clf.fit(X_train, y_train)\n",
      "If your AdaBoost ensemble is overfitting the training set, you can\n",
      "try reducing the number of estimators or more strongly regulariz‐\n",
      "ing the base estimator.\n",
      "Gradient Boosting\n",
      "Another very popular Boosting algorithm is Gradient Boosting .17 Just like AdaBoost,\n",
      "Gradient Boosting works by sequentially adding predictors to an ensemble, each one\n",
      "correcting its predecessor. However, instead of tweaking the instance weights at every\n",
      "iteration like AdaBoost does, this method tries to fit the new predictor to the residual\n",
      "errors  made by the previous predictor.\n",
      "Let’s go through a simple regression example using Decision Trees as the base predic‐\n",
      "tors (of course Gradient Boosting also works great with regression tasks). This is\n",
      "called  Gradient Tree Boosting , or Gradient Boosted Regression Trees  (GBRT ). First, let’s\n",
      "fit a DecisionTreeRegressor  to the training set (for example, a noisy quadratic train‐\n",
      "ing set):\n",
      "Boosting | 205\n",
      "from sklearn.tree  import DecisionTreeRegressor\n",
      "tree_reg1  = DecisionTreeRegressor (max_depth =2)\n",
      "tree_reg1 .fit(X, y)\n",
      "Now train a second DecisionTreeRegressor  on the residual errors made by the first\n",
      "predictor:\n",
      "y2 = y - tree_reg1 .predict(X)\n",
      "tree_reg2  = DecisionTreeRegressor (max_depth =2)\n",
      "tree_reg2 .fit(X, y2)\n",
      "Then we train a third regressor on the residual errors made by the second predictor:\n",
      "y3 = y2 - tree_reg2 .predict(X)\n",
      "tree_reg3  = DecisionTreeRegressor (max_depth =2)\n",
      "tree_reg3 .fit(X, y3)\n",
      "Now we have an ensemble containing three trees. It can make predictions on a new\n",
      "instance simply by adding up the predictions of all the trees:\n",
      "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1 , tree_reg2 , tree_reg3 ))\n",
      "Figure 7-9  represents the predictions of these three trees in the left column, and the\n",
      "ensemble’s predictions in the right column. In the first row, the ensemble has just one\n",
      "tree, so its predictions are exactly the same as the first tree’s predictions. In the second\n",
      "row, a new tree is trained on the residual errors of the first tree. On the right you can\n",
      "see that the ensemble’s predictions are equal to the sum of the predictions of the first\n",
      "two trees. Similarly, in the third row another tree is trained on the residual errors of\n",
      "the second tree. Y ou can see that the ensemble’s predictions gradually get better as\n",
      "trees are added to the ensemble.\n",
      "A simpler way to train GBRT ensembles is to use Scikit-Learn’s GradientBoostingRe\n",
      "gressor  class. Much like the RandomForestRegressor  class, it has hyperparameters to\n",
      "control the growth of Decision Trees (e.g., max_depth , min_samples_leaf , and so on),\n",
      "as well as hyperparameters to control the ensemble training, such as the number of\n",
      "trees (n_estimators ). The following code creates the same ensemble as the previous\n",
      "one:\n",
      "from sklearn.ensemble  import GradientBoostingRegressor\n",
      "gbrt = GradientBoostingRegressor (max_depth =2, n_estimators =3, learning_rate =1.0)\n",
      "gbrt.fit(X, y)\n",
      "206 | Chapter 7: Ensemble Learning and Random Forests\n",
      "Figure 7-9. Gradient Boosting\n",
      "The learning_rate  hyperparameter scales the contribution of each tree. If you set it\n",
      "to a low value, such as 0.1, you will need more trees in the ensemble to fit the train‐\n",
      "ing set, but the predictions will usually generalize better. This is a regularization tech‐\n",
      "nique called shrinkage . Figure 7-10  shows two GBRT ensembles trained with a low\n",
      "learning rate: the one on the left does not have enough trees to fit the training set,\n",
      "while the one on the right has too many trees and overfits the training set.\n",
      "Boosting | 207\n",
      "Figure 7-10. GBRT ensembles with not enough predictors (left)  and too many (right)\n",
      "In order to find the optimal number of trees, you can use early stopping (see Chap‐\n",
      "ter 4 ). A simple way to implement this is to use the staged_predict()  method: it\n",
      "returns an iterator over the predictions made by the ensemble at each stage of train‐\n",
      "ing (with one tree, two trees, etc.). The following code trains a GBRT ensemble with\n",
      "120 trees, then measures the validation error at each stage of training to find the opti‐\n",
      "mal number of trees, and finally trains another GBRT ensemble using the optimal\n",
      "number of trees:\n",
      "import numpy as np\n",
      "from sklearn.model_selection  import train_test_split\n",
      "from sklearn.metrics  import mean_squared_error\n",
      "X_train, X_val, y_train, y_val = train_test_split (X, y)\n",
      "gbrt = GradientBoostingRegressor (max_depth =2, n_estimators =120)\n",
      "gbrt.fit(X_train, y_train)\n",
      "errors = [mean_squared_error (y_val, y_pred)\n",
      "          for y_pred in gbrt.staged_predict (X_val)]\n",
      "bst_n_estimators  = np.argmin(errors)\n",
      "gbrt_best  = GradientBoostingRegressor (max_depth =2,n_estimators =bst_n_estimators )\n",
      "gbrt_best .fit(X_train, y_train)\n",
      "The validation errors are represented on the left of Figure 7-11 , and the best model’s\n",
      "predictions are represented on the right.\n",
      "208 | Chapter 7: Ensemble Learning and Random Forests\n",
      "Figure 7-11. Tuning the number of trees using early stopping\n",
      "It is also possible to implement early stopping by actually stopping training early\n",
      "(instead of training a large number of trees first and then looking back to find the\n",
      "optimal number). Y ou can do so by setting warm_start=True , which makes Scikit-\n",
      "Learn keep existing trees when the fit()  method is called, allowing incremental\n",
      "training. The following code stops training when the validation error does not\n",
      "improve for five iterations in a row:\n",
      "gbrt = GradientBoostingRegressor (max_depth =2, warm_start =True)\n",
      "min_val_error  = float(\"inf\")\n",
      "error_going_up  = 0\n",
      "for n_estimators  in range(1, 120):\n",
      "    gbrt.n_estimators  = n_estimators\n",
      "    gbrt.fit(X_train, y_train)\n",
      "    y_pred = gbrt.predict(X_val)\n",
      "    val_error  = mean_squared_error (y_val, y_pred)\n",
      "    if val_error  < min_val_error :\n",
      "        min_val_error  = val_error\n",
      "        error_going_up  = 0\n",
      "    else:\n",
      "        error_going_up  += 1\n",
      "        if error_going_up  == 5:\n",
      "            break  # early stopping\n",
      "The GradientBoostingRegressor  class also supports a subsample  hyperparameter,\n",
      "which specifies the fraction of training instances to be used for training each tree. For\n",
      "example, if subsample=0.25 , then each tree is trained on 25% of the training instan‐\n",
      "ces, selected randomly. As you can probably guess by now, this trades a higher bias\n",
      "for a lower variance. It also speeds up training considerably. This technique is called\n",
      "Stochastic Gradient Boosting .\n",
      "Boosting | 209\n",
      "18“Stacked Generalization, ” D. Wolpert (1992).\n",
      "It is possible to use Gradient Boosting with other cost functions.\n",
      "This is controlled by the loss  hyperparameter (see Scikit-Learn’s\n",
      "documentation for more details).\n",
      "It is worth noting that an optimized implementation of Gradient Boosting is available\n",
      "in the popular python library XGBoost , which stands for Extreme Gradient Boosting.\n",
      "This package was initially developed by Tianqi Chen as part of the Distributed (Deep)\n",
      "Machine Learning Community ( DMLC ), and it aims at being extremely fast, scalable\n",
      "and portable. In fact, XGBoost is often an important component of the winning\n",
      "entries in ML competitions. XGBoost’s API is quite similar to Scikit-Learn’s:\n",
      "import xgboost\n",
      "xgb_reg = xgboost.XGBRegressor ()\n",
      "xgb_reg.fit(X_train, y_train)\n",
      "y_pred = xgb_reg.predict(X_val)\n",
      "XGBoost also offers several nice features, such as automatically taking care of early\n",
      "stopping:\n",
      "xgb_reg.fit(X_train, y_train,\n",
      "            eval_set =[(X_val, y_val)], early_stopping_rounds =2)\n",
      "y_pred = xgb_reg.predict(X_val)\n",
      "Y ou should definitely check it out!\n",
      "Stacking\n",
      "The last Ensemble method we will discuss in this chapter is called stacking  (short for\n",
      "stacked generalization ).18 It is based on a simple idea: instead of using trivial functions\n",
      "(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\n",
      "why don’t we train a model to perform this aggregation? Figure 7-12  shows such an\n",
      "ensemble performing a regression task on a new instance. Each of the bottom three\n",
      "predictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor \n",
      "(called a blender , or a meta learner ) takes these predictions as inputs and makes the\n",
      "final prediction (3.0).\n",
      "210 | Chapter 7: Ensemble Learning and Random Forests\n",
      "19Alternatively, it is possible to use out-of-fold predictions. In some contexts this is called stacking , while using a\n",
      "hold-out set is called blending . However, for many people these terms are synonymous.\n",
      "Figure 7-12. Aggregating predictions using a blending predictor\n",
      "To train the blender, a common approach is to use a hold-out set.19 Let’s see how it\n",
      "works. First, the training set is split in two subsets. The first subset is used to train the\n",
      "predictors in the first layer (see Figure 7-13 ).\n",
      "Figure 7-13. Training the first layer\n",
      "Next, the first layer predictors are used to make predictions on the second (held-out)\n",
      "set (see Figure 7-14 ). This ensures that the predictions are “clean, ” since the predictors\n",
      "never saw these instances during training. Now for each instance in the hold-out set\n",
      "Stacking | 211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are three predicted values. We can create a new training set using these predic‐\n",
      "ted values as input features (which makes this new training set three-dimensional),\n",
      "and keeping the target values. The blender is trained on this new training set, so it\n",
      "learns to predict the target value given the first layer’s predictions.\n",
      "Figure 7-14. Training the blender\n",
      "It is actually possible to train several different blenders this way (e.g., one using Lin‐\n",
      "ear Regression, another using Random Forest Regression, and so on): we get a whole\n",
      "layer of blenders. The trick is to split the training set into three subsets: the first one is\n",
      "used to train the first layer, the second one is used to create the training set used to\n",
      "train the second layer (using predictions made by the predictors of the first layer),\n",
      "and the third one is used to create the training set to train the third layer (using pre‐\n",
      "dictions made by the predictors of the second layer). Once this is done, we can make\n",
      "a prediction for a new instance by going through each layer sequentially, as shown in\n",
      "Figure 7-15 .\n",
      "212 | Chapter 7: Ensemble Learning and Random Forests\n",
      "Figure 7-15. Predictions in a multilayer stacking ensemble\n",
      "Unfortunately, Scikit-Learn does not support stacking directly, but it is not too hard\n",
      "to roll out your own implementation (see the following exercises). Alternatively, you\n",
      "can use an open source implementation such as brew  (available at https://github.com/\n",
      "viisar/brew ).\n",
      "Exercises\n",
      "1.If you have trained five different models on the exact same training data, and\n",
      "they all achieve 95% precision, is there any chance that you can combine these\n",
      "models to get better results? If so, how? If not, why?\n",
      "2.What is the difference between hard and soft voting classifiers?\n",
      "3.Is it possible to speed up training of a bagging ensemble by distributing it across\n",
      "multiple servers? What about pasting ensembles, boosting ensembles, random\n",
      "forests, or stacking ensembles?\n",
      "4.What is the benefit of out-of-bag evaluation?\n",
      "5.What makes Extra-Trees more random than regular Random Forests? How can\n",
      "this extra randomness help? Are Extra-Trees slower or faster than regular Ran‐\n",
      "dom Forests?\n",
      "6.If your AdaBoost ensemble underfits the training data, what hyperparameters\n",
      "should you tweak and how?\n",
      "Exercises | 213\n",
      "7.If your Gradient Boosting ensemble overfits the training set, should you increase\n",
      "or decrease the learning rate?\n",
      "8.Load the MNIST data (introduced in Chapter 3 ), and split it into a training set, a\n",
      "validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for val‐\n",
      "idation, and 10,000 for testing). Then train various classifiers, such as a Random\n",
      "Forest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine\n",
      "them into an ensemble that outperforms them all on the validation set, using a\n",
      "soft or hard voting classifier. Once you have found one, try it on the test set. How\n",
      "much better does it perform compared to the individual classifiers?\n",
      "9.Run the individual classifiers from the previous exercise to make predictions on\n",
      "the validation set, and create a new training set with the resulting predictions:\n",
      "each training instance is a vector containing the set of predictions from all your\n",
      "classifiers for an image, and the target is the image’s class. Train a classifier on\n",
      "this new training set. Congratulations, you have just trained a blender, and\n",
      "together with the classifiers they form a stacking ensemble! Now let’s evaluate the\n",
      "ensemble on the test set. For each image in the test set, make predictions with all\n",
      "your classifiers, then feed the predictions to the blender to get the ensemble’s pre‐\n",
      "dictions. How does it compare to the voting classifier you trained earlier?\n",
      "Solutions to these exercises are available in ???.\n",
      "214 | Chapter 7: Ensemble Learning and Random Forests\n",
      "CHAPTER 8\n",
      "Dimensionality Reduction\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 8 in the final\n",
      "release of the book.\n",
      "Many Machine Learning problems involve thousands or even millions of features for\n",
      "each training instance. Not only does this make training extremely slow, it can also\n",
      "make it much harder to find a good solution, as we will see. This problem is often\n",
      "referred to as the curse of dimensionality .\n",
      "Fortunately, in real-world problems, it is often possible to reduce the number of fea‐\n",
      "tures considerably, turning an intractable problem into a tractable one. For example,\n",
      "consider the MNIST images (introduced in Chapter 3 ): the pixels on the image bor‐\n",
      "ders are almost always white, so you could completely drop these pixels from the\n",
      "training set without losing much information. Figure 7-6  confirms that these pixels\n",
      "are utterly unimportant for the classification task. Moreover, two neighboring pixels\n",
      "are often highly correlated: if you merge them into a single pixel (e.g., by taking the\n",
      "mean of the two pixel intensities), you will not lose much information.\n",
      "215\n",
      "1Well, four dimensions if you count time, and a few more if you are a string theorist.\n",
      "Reducing dimensionality does lose some information (just like\n",
      "compressing an image to JPEG can degrade its quality), so even\n",
      "though it will speed up training, it may also make your system per‐\n",
      "form slightly worse. It also makes your pipelines a bit more com‐\n",
      "plex and thus harder to maintain. So you should first try to train\n",
      "your system with the original data before considering using dimen‐\n",
      "sionality reduction if training is too slow. In some cases, however,\n",
      "reducing the dimensionality of the training data may filter out\n",
      "some noise and unnecessary details and thus result in higher per‐\n",
      "formance (but in general it won’t; it will just speed up training).\n",
      "Apart from speeding up training, dimensionality reduction is also extremely useful\n",
      "for data visualization (or DataViz ). Reducing the number of dimensions down to two\n",
      "(or three) makes it possible to plot a condensed view of a high-dimensional training\n",
      "set on a graph and often gain some important insights by visually detecting patterns,\n",
      "such as clusters. Moreover, DataViz is essential to communicate your conclusions to\n",
      "people who are not data scientists, in particular decision makers who will use your\n",
      "results.\n",
      "In this chapter we will discuss the curse of dimensionality and get a sense of what\n",
      "goes on in high-dimensional space. Then, we will present the two main approaches to\n",
      "dimensionality reduction (projection and Manifold Learning), and we will go\n",
      "through three of the most popular dimensionality reduction techniques: PCA, Kernel\n",
      "PCA, and LLE.\n",
      "The Curse of Dimensionality\n",
      "We are so used to living in three dimensions1 that our intuition fails us when we try\n",
      "to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\n",
      "picture in our mind (see Figure 8-1 ), let alone a 200-dimensional ellipsoid bent in a\n",
      "1,000-dimensional space.\n",
      "216 | Chapter 8: Dimensionality Reduction\n",
      "2Watch a rotating tesseract projected into 3D space at https://homl.info/30 . Image by Wikipedia user Nerd‐\n",
      "Boy1392 ( Creative Commons BY-SA 3.0 ). Reproduced from https://en.wikipedia.org/wiki/Tesseract .\n",
      "3Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much sugar they put\n",
      "in their coffee), if you consider enough dimensions.\n",
      "Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2\n",
      "It turns out that many things behave very differently in high-dimensional space. For\n",
      "example, if you pick a random point in a unit square (a 1 × 1 square), it will have only\n",
      "about a 0.4% chance of being located less than 0.001 from a border (in other words, it\n",
      "is very unlikely that a random point will be “extreme” along any dimension). But in a\n",
      "10,000-dimensional unit hypercube (a 1 × 1 × ⋯ × 1 cube, with ten thousand 1s), this\n",
      "probability is greater than 99.999999%. Most points in a high-dimensional hypercube\n",
      "are very close to the border.3\n",
      "Here is a more troublesome difference: if you pick two points randomly in a unit\n",
      "square, the distance between these two points will be, on average, roughly 0.52. If you\n",
      "pick two random points in a unit 3D cube, the average distance will be roughly 0.66.\n",
      "But what about two points picked randomly in a 1,000,000-dimensional hypercube?\n",
      "Well, the average distance, believe it or not, will be about 408.25 (roughly\n",
      "1, 000, 000/6 )! This is quite counterintuitive: how can two points be so far apart\n",
      "when they both lie within the same unit hypercube? This fact implies that high-\n",
      "dimensional datasets are at risk of being very sparse: most training instances are\n",
      "likely to be far away from each other. Of course, this also means that a new instance\n",
      "will likely be far away from any training instance, making predictions much less relia‐\n",
      "ble than in lower dimensions, since they will be based on much larger extrapolations.\n",
      "In short, the more dimensions the training set has, the greater the risk of overfitting\n",
      "it.\n",
      "In theory, one solution to the curse of dimensionality could be to increase the size of\n",
      "the training set to reach a sufficient density of training instances. Unfortunately, in\n",
      "practice, the number of training instances required to reach a given density grows\n",
      "exponentially with the number of dimensions. With just 100 features (much less than\n",
      "The Curse of Dimensionality | 217\n",
      "in the MNIST problem), you would need more training instances than atoms in the\n",
      "observable universe in order for training instances to be within 0.1 of each other on\n",
      "average, assuming they were spread out uniformly across all dimensions.\n",
      "Main Approaches for Dimensionality Reduction\n",
      "Before we dive into specific dimensionality reduction algorithms, let’s take a look at\n",
      "the two main approaches to reducing dimensionality: projection and Manifold\n",
      "Learning.\n",
      "Projection\n",
      "In most real-world problems, training instances are not spread out uniformly across\n",
      "all dimensions. Many features are almost constant, while others are highly correlated\n",
      "(as discussed earlier for MNIST). As a result, all training instances actually lie within\n",
      "(or close to) a much lower-dimensional subspace  of the high-dimensional space. This\n",
      "sounds very abstract, so let’s look at an example. In Figure 8-2  you can see a 3D data‐\n",
      "set represented by the circles.\n",
      "Figure 8-2. A 3D dataset lying close to a 2D subspace\n",
      "Notice that all training instances lie close to a plane: this is a lower-dimensional (2D)\n",
      "subspace of the high-dimensional (3D) space. Now if we project every training\n",
      "instance perpendicularly onto this subspace (as represented by the short lines con‐\n",
      "necting the instances to the plane), we get the new 2D dataset shown in Figure 8-3 .\n",
      "Ta-da! We have just reduced the dataset’s dimensionality from 3D to 2D. Note that\n",
      "the axes correspond to new features z1 and z2 (the coordinates of the projections on\n",
      "the plane).\n",
      "218 | Chapter 8: Dimensionality Reduction\n",
      "Figure 8-3. The new 2D dataset after  projection\n",
      "However, projection is not always the best approach to dimensionality reduction. In\n",
      "many cases the subspace may twist and turn, such as in the famous Swiss roll  toy data‐\n",
      "set represented in Figure 8-4 .\n",
      "Figure 8-4. Swiss roll dataset\n",
      "Main Approaches for Dimensionality Reduction | 219\n",
      "Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of\n",
      "the Swiss roll together, as shown on the left of Figure 8-5 . However, what you really\n",
      "want is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5 .\n",
      "Figure 8-5. Squashing by projecting onto a plane (left)  versus unrolling the Swiss roll\n",
      "(right)\n",
      "Manifold Learning\n",
      "The Swiss roll is an example of a 2D manifold . Put simply, a 2D manifold is a 2D\n",
      "shape that can be bent and twisted in a higher-dimensional space. More generally, a\n",
      "d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\n",
      "resembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\n",
      "locally resembles a 2D plane, but it is rolled in the third dimension.\n",
      "Many dimensionality reduction algorithms work by modeling the manifold  on which\n",
      "the training instances lie; this is called Manifold Learning . It relies on the manifold\n",
      "assumption , also called the manifold hypothesis , which holds that most real-world\n",
      "high-dimensional datasets lie close to a much lower-dimensional manifold. This\n",
      "assumption is very often empirically observed.\n",
      "Once again, think about the MNIST dataset: all handwritten digit images have some\n",
      "similarities. They are made of connected lines, the borders are white, they are more\n",
      "or less centered, and so on. If you randomly generated images, only a ridiculously\n",
      "tiny fraction of them would look like handwritten digits. In other words, the degrees\n",
      "of freedom available to you if you try to create a digit image are dramatically lower\n",
      "than the degrees of freedom you would have if you were allowed to generate any\n",
      "image you wanted. These constraints tend to squeeze the dataset into a lower-\n",
      "dimensional manifold.\n",
      "The manifold assumption is often accompanied by another implicit assumption: that\n",
      "the task at hand (e.g., classification or regression) will be simpler if expressed in the\n",
      "lower-dimensional space of the manifold. For example, in the top row of Figure 8-6\n",
      "the Swiss roll is split into two classes: in the 3D space (on the left), the decision\n",
      "220 | Chapter 8: Dimensionality Reduction\n",
      "boundary would be fairly complex, but in the 2D unrolled manifold space (on the\n",
      "right), the decision boundary is a simple straight line.\n",
      "However, this assumption does not always hold. For example, in the bottom row of\n",
      "Figure 8-6 , the decision boundary is located at x1 = 5. This decision boundary looks\n",
      "very simple in the original 3D space (a vertical plane), but it looks more complex in\n",
      "the unrolled manifold (a collection of four independent line segments).\n",
      "In short, if you reduce the dimensionality of your training set before training a\n",
      "model, it will usually speed up training, but it may not always lead to a better or sim‐\n",
      "pler solution; it all depends on the dataset.\n",
      "Hopefully you now have a good sense of what the curse of dimensionality is and how\n",
      "dimensionality reduction algorithms can fight it, especially when the manifold\n",
      "assumption holds. The rest of this chapter will go through some of the most popular\n",
      "algorithms.\n",
      "Figure 8-6. The decision boundary may not always be simpler with lower dimensions\n",
      "Main Approaches for Dimensionality Reduction | 221\n",
      "4“On Lines and Planes of Closest Fit to Systems of Points in Space, ” K. Pearson (1901).PCA\n",
      "Principal Component Analysis  (PCA) is by far the most popular dimensionality reduc‐\n",
      "tion algorithm. First it identifies the hyperplane that lies closest to the data, and then\n",
      "it projects the data onto it, just like in Figure 8-2 .\n",
      "Preserving the Variance\n",
      "Before you can project the training set onto a lower-dimensional hyperplane, you\n",
      "first need to choose the right hyperplane. For example, a simple 2D dataset is repre‐\n",
      "sented on the left of Figure 8-7 , along with three different axes (i.e., one-dimensional\n",
      "hyperplanes). On the right is the result of the projection of the dataset onto each of\n",
      "these axes. As you can see, the projection onto the solid line preserves the maximum\n",
      "variance, while the projection onto the dotted line preserves very little variance, and\n",
      "the projection onto the dashed line preserves an intermediate amount of variance.\n",
      "Figure 8-7. Selecting the subspace onto which to project\n",
      "It seems reasonable to select the axis that preserves the maximum amount of var‐\n",
      "iance, as it will most likely lose less information than the other projections. Another\n",
      "way to justify this choice is that it is the axis that minimizes the mean squared dis‐\n",
      "tance between the original dataset and its projection onto that axis. This is the rather\n",
      "simple idea behind PCA .4\n",
      "222 | Chapter 8: Dimensionality Reduction\n",
      "Principal Components\n",
      "PCA identifies the axis that accounts for the largest amount of variance in the train‐\n",
      "ing set. In Figure 8-7 , it is the solid line. It also finds a second axis, orthogonal to the\n",
      "first one, that accounts for the largest amount of remaining variance. In this 2D\n",
      "example there is no choice: it is the dotted line. If it were a higher-dimensional data‐\n",
      "set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\n",
      "a fifth, and so on—as many axes as the number of dimensions in the dataset.\n",
      "The unit vector that defines the ith axis is called the ith principal component  (PC). In\n",
      "Figure 8-7 , the 1st PC is c1 and the 2nd PC is c2. In Figure 8-2  the first two PCs are\n",
      "represented by the orthogonal arrows in the plane, and the third PC would be\n",
      "orthogonal to the plane (pointing up or down).\n",
      "The direction of the principal components is not stable: if you per‐\n",
      "turb the training set slightly and run PCA again, some of the new\n",
      "PCs may point in the opposite direction of the original PCs. How‐\n",
      "ever, they will generally still lie on the same axes. In some cases, a\n",
      "pair of PCs may even rotate or swap, but the plane they define will\n",
      "generally remain the same.\n",
      "So how can you find the principal components of a training set? Luckily, there is a\n",
      "standard matrix factorization technique called Singular Value Decomposition  (SVD)\n",
      "that can decompose the training set matrix X into the matrix multiplication of three\n",
      "matrices U Σ VT, where V contains all the principal components that we are looking\n",
      "for, as shown in Equation 8-1 .\n",
      "Equation 8-1. Principal components matrix\n",
      "V=∣ ∣ ∣\n",
      "c1c2⋯cn\n",
      "∣ ∣ ∣\n",
      "The following Python code uses NumPy’s svd()  function to obtain all the principal\n",
      "components of the training set, then extracts the first two PCs:\n",
      "X_centered  = X - X.mean(axis=0)\n",
      "U, s, Vt = np.linalg.svd(X_centered )\n",
      "c1 = Vt.T[:, 0]\n",
      "c2 = Vt.T[:, 1]\n",
      "PCA | 223\n",
      "PCA assumes that the dataset is centered around the origin. As we\n",
      "will see, Scikit-Learn’s PCA classes take care of centering the data\n",
      "for you. However, if you implement PCA yourself (as in the pre‐\n",
      "ceding example), or if you use other libraries, don’t forget to center\n",
      "the data first.\n",
      "Projecting Down to d Dimensions\n",
      "Once you have identified all the principal components, you can reduce the dimen‐\n",
      "sionality of the dataset down to d dimensions by projecting it onto the hyperplane\n",
      "defined by the first d principal components. Selecting this hyperplane ensures that the\n",
      "projection will preserve as much variance as possible. For example, in Figure 8-2  the\n",
      "3D dataset is projected down to the 2D plane defined by the first two principal com‐\n",
      "ponents, preserving a large part of the dataset’s variance. As a result, the 2D projec‐\n",
      "tion looks very much like the original 3D dataset.\n",
      "To project the training set onto the hyperplane, you can simply compute the matrix\n",
      "multiplication of the training set matrix X by the matrix Wd, defined as the matrix\n",
      "containing the first d principal components (i.e., the matrix composed of the first d\n",
      "columns of V), as shown in Equation 8-2 .\n",
      "Equation 8-2. Projecting the training set down to d dimensions\n",
      "Xd‐proj=XWd\n",
      "The following Python code projects the training set onto the plane defined by the first\n",
      "two principal components:\n",
      "W2 = Vt.T[:, :2]\n",
      "X2D = X_centered .dot(W2)\n",
      "There you have it! Y ou now know how to reduce the dimensionality of any dataset\n",
      "down to any number of dimensions, while preserving as much variance as possible.\n",
      "Using Scikit-Learn\n",
      "Scikit-Learn’s PCA class implements PCA using SVD decomposition just like we did\n",
      "before. The following code applies PCA to reduce the dimensionality of the dataset\n",
      "down to two dimensions (note that it automatically takes care of centering the data):\n",
      "from sklearn.decomposition  import PCA\n",
      "pca = PCA(n_components  = 2)\n",
      "X2D = pca.fit_transform (X)\n",
      "After fitting the PCA transformer to the dataset, you can access the principal compo‐\n",
      "nents using the components_  variable (note that it contains the PCs as horizontal vec‐\n",
      "224 | Chapter 8: Dimensionality Reduction\n",
      "tors, so, for example, the first principal component is equal to pca.components_.T[:,\n",
      "0]).\n",
      "Explained Variance Ratio\n",
      "Another very useful piece of information is the explained variance ratio  of each prin‐\n",
      "cipal component, available via the explained_variance_ratio_  variable. It indicates\n",
      "the proportion of the dataset’s variance that lies along the axis of each principal com‐\n",
      "ponent. For example, let’s look at the explained variance ratios of the first two compo‐\n",
      "nents of the 3D dataset represented in Figure 8-2 :\n",
      ">>> pca.explained_variance_ratio_\n",
      "array([0.84248607, 0.14631839])\n",
      "This tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6%\n",
      "lies along the second axis. This leaves less than 1.2% for the third axis, so it is reason‐\n",
      "able to assume that it probably carries little information.\n",
      "Choosing the Right Number of Dimensions\n",
      "Instead of arbitrarily choosing the number of dimensions to reduce down to, it is\n",
      "generally preferable to choose the number of dimensions that add up to a sufficiently\n",
      "large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen‐\n",
      "sionality for data visualization—in that case you will generally want to reduce the\n",
      "dimensionality down to 2 or 3.\n",
      "The following code computes PCA without reducing dimensionality, then computes\n",
      "the minimum number of dimensions required to preserve 95% of the training set’s\n",
      "variance:\n",
      "pca = PCA()\n",
      "pca.fit(X_train)\n",
      "cumsum = np.cumsum(pca.explained_variance_ratio_ )\n",
      "d = np.argmax(cumsum >= 0.95) + 1\n",
      "Y ou could then set n_components=d  and run PCA again. However, there is a much\n",
      "better option: instead of specifying the number of principal components you want to\n",
      "preserve, you can set n_components  to be a float between 0.0 and 1.0, indicating the\n",
      "ratio of variance you wish to preserve:\n",
      "pca = PCA(n_components =0.95)\n",
      "X_reduced  = pca.fit_transform (X_train)\n",
      "Y et another option is to plot the explained variance as a function of the number of\n",
      "dimensions (simply plot cumsum ; see Figure 8-8 ). There will usually be an elbow in the\n",
      "curve, where the explained variance stops growing fast. Y ou can think of this as the\n",
      "intrinsic dimensionality of the dataset. In this case, you can see that reducing the\n",
      "PCA | 225\n",
      "dimensionality down to about 100 dimensions wouldn’t lose too much explained var‐\n",
      "iance.\n",
      "Figure 8-8. Explained variance as a function of the number of dimensions\n",
      "PCA for Compression\n",
      "Obviously after dimensionality reduction, the training set takes up much less space.\n",
      "For example, try applying PCA to the MNIST dataset while preserving 95% of its var‐\n",
      "iance. Y ou should find that each instance will have just over 150 features, instead of\n",
      "the original 784 features. So while most of the variance is preserved, the dataset is\n",
      "now less than 20% of its original size! This is a reasonable compression ratio, and you\n",
      "can see how this can speed up a classification algorithm (such as an SVM classifier)\n",
      "tremendously.\n",
      "It is also possible to decompress the reduced dataset back to 784 dimensions by\n",
      "applying the inverse transformation of the PCA projection. Of course this won’t give\n",
      "you back the original data, since the projection lost a bit of information (within the\n",
      "5% variance that was dropped), but it will likely be quite close to the original data.\n",
      "The mean squared distance between the original data and the reconstructed data\n",
      "(compressed and then decompressed) is called the reconstruction error . For example,\n",
      "the following code compresses the MNIST dataset down to 154 dimensions, then uses\n",
      "the inverse_transform()  method to decompress it back to 784 dimensions.\n",
      "Figure 8-9  shows a few digits from the original training set (on the left), and the cor‐\n",
      "responding digits after compression and decompression. Y ou can see that there is a\n",
      "slight image quality loss, but the digits are still mostly intact.\n",
      "pca = PCA(n_components  = 154)\n",
      "X_reduced  = pca.fit_transform (X_train)\n",
      "X_recovered  = pca.inverse_transform (X_reduced )\n",
      "226 | Chapter 8: Dimensionality Reduction\n",
      "Figure 8-9. MNIST compression preserving 95% of the variance\n",
      "The equation of the inverse transformation is shown in Equation 8-3 .\n",
      "Equation 8-3. PCA inverse transformation, back to the original number of\n",
      "dimensions\n",
      "Xrecovered=Xd‐projWdT\n",
      "Randomized PCA\n",
      "If you set the svd_solver  hyperparameter to \"randomized\" , Scikit-Learn uses a sto‐\n",
      "chastic algorithm called Randomized PCA  that quickly finds an approximation of the\n",
      "first d principal components. Its computational complexity is O(m × d2) + O(d3),\n",
      "instead of O(m × n2) + O(n3) for the full SVD approach, so it is dramatically faster\n",
      "than full SVD when d is much smaller than n:\n",
      "rnd_pca = PCA(n_components =154, svd_solver =\"randomized\" )\n",
      "X_reduced  = rnd_pca.fit_transform (X_train)\n",
      "By default, svd_solver  is actually set to \"auto\" : Scikit-Learn automatically uses the\n",
      "randomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\n",
      "or n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\n",
      "SVD, you can set the svd_solver  hyperparameter to \"full\" .\n",
      "Incremental PCA\n",
      "One problem with the preceding implementations of PCA is that they require the\n",
      "whole training set to fit in memory in order for the algorithm to run. Fortunately,\n",
      "Incremental PCA  (IPCA) algorithms have been developed: you can split the training\n",
      "set into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is\n",
      "PCA | 227\n",
      "5Scikit-Learn uses the algorithm described in “Incremental Learning for Robust Visual Tracking, ” D. Ross et al.\n",
      "(2007).useful for large training sets, and also to apply PCA online (i.e., on the fly, as new\n",
      "instances arrive).\n",
      "The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s\n",
      "array_split()  function) and feeds them to Scikit-Learn’s IncrementalPCA  class5 to \n",
      "reduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\n",
      "before). Note that you must call the partial_fit()  method with each mini-batch\n",
      "rather than the fit()  method with the whole training set:\n",
      "from sklearn.decomposition  import IncrementalPCA\n",
      "n_batches  = 100\n",
      "inc_pca = IncrementalPCA (n_components =154)\n",
      "for X_batch in np.array_split (X_train, n_batches ):\n",
      "    inc_pca.partial_fit (X_batch)\n",
      "X_reduced  = inc_pca.transform (X_train)\n",
      "Alternatively, you can use NumPy’s memmap  class, which allows you to manipulate a\n",
      "large array stored in a binary file on disk as if it were entirely in memory; the class\n",
      "loads only the data it needs in memory, when it needs it. Since the IncrementalPCA\n",
      "class uses only a small part of the array at any given time, the memory usage remains\n",
      "under control. This makes it possible to call the usual fit()  method, as you can see\n",
      "in the following code:\n",
      "X_mm = np.memmap(filename , dtype=\"float32\" , mode=\"readonly\" , shape=(m, n))\n",
      "batch_size  = m // n_batches\n",
      "inc_pca = IncrementalPCA (n_components =154, batch_size =batch_size )\n",
      "inc_pca.fit(X_mm)\n",
      "Kernel PCA\n",
      "In Chapter 5  we discussed the kernel trick, a mathematical technique that implicitly\n",
      "maps instances into a very high-dimensional space (called the feature space ), enabling\n",
      "nonlinear classification and regression with Support Vector Machines. Recall that a\n",
      "linear decision boundary in the high-dimensional feature space corresponds to a\n",
      "complex nonlinear decision boundary in the original space .\n",
      "It turns out that the same trick can be applied to PCA, making it possible to perform\n",
      "complex nonlinear projections for dimensionality reduction. This is called Kernel\n",
      "228 | Chapter 8: Dimensionality Reduction\n",
      "6“Kernel Principal Component Analysis, ” B. Schölkopf, A. Smola, K. Müller (1999).PCA  (kPCA) .6 It is often good at preserving clusters of instances after projection, or\n",
      "sometimes even unrolling datasets that lie close to a twisted manifold.\n",
      "For example, the following code uses Scikit-Learn’s KernelPCA  class to perform kPCA\n",
      "with an RBF kernel (see Chapter 5  for more details about the RBF kernel and the\n",
      "other kernels):\n",
      "from sklearn.decomposition  import KernelPCA\n",
      "rbf_pca = KernelPCA (n_components  = 2, kernel=\"rbf\", gamma=0.04)\n",
      "X_reduced  = rbf_pca.fit_transform (X)\n",
      "Figure 8-10  shows the Swiss roll, reduced to two dimensions using a linear kernel\n",
      "(equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel\n",
      "(Logistic).\n",
      "Figure 8-10. Swiss roll reduced to 2D using kPCA with various kernels\n",
      "Selecting a Kernel and Tuning Hyperparameters\n",
      "As kPCA is an unsupervised learning algorithm, there is no obvious performance\n",
      "measure to help you select the best kernel and hyperparameter values. However,\n",
      "dimensionality reduction is often a preparation step for a supervised learning task\n",
      "(e.g., classification), so you can simply use grid search to select the kernel and hyper‐\n",
      "parameters that lead to the best performance on that task. For example, the following\n",
      "code creates a two-step pipeline, first reducing dimensionality to two dimensions\n",
      "using kPCA, then applying Logistic Regression for classification. Then it uses Grid\n",
      "SearchCV  to find the best kernel and gamma value for kPCA in order to get the best\n",
      "classification accuracy at the end of the pipeline:\n",
      "from sklearn.model_selection  import GridSearchCV\n",
      "from sklearn.linear_model  import LogisticRegression\n",
      "from sklearn.pipeline  import Pipeline\n",
      "Kernel PCA | 229\n",
      "clf = Pipeline ([\n",
      "        (\"kpca\", KernelPCA (n_components =2)),\n",
      "        (\"log_reg\" , LogisticRegression ())\n",
      "    ])\n",
      "param_grid  = [{\n",
      "        \"kpca__gamma\" : np.linspace (0.03, 0.05, 10),\n",
      "        \"kpca__kernel\" : [\"rbf\", \"sigmoid\" ]\n",
      "    }]\n",
      "grid_search  = GridSearchCV (clf, param_grid , cv=3)\n",
      "grid_search .fit(X, y)\n",
      "The best kernel and hyperparameters are then available through the best_params_\n",
      "variable:\n",
      ">>> print(grid_search .best_params_ )\n",
      "{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}\n",
      "Another approach, this time entirely unsupervised, is to select the kernel and hyper‐\n",
      "parameters that yield the lowest reconstruction error. However, reconstruction is not\n",
      "as easy as with linear PCA. Here’s why. Figure 8-11  shows the original Swiss roll 3D\n",
      "dataset (top left), and the resulting 2D dataset after kPCA is applied using an RBF\n",
      "kernel (top right). Thanks to the kernel trick, this is mathematically equivalent to\n",
      "mapping the training set to an infinite-dimensional feature space (bottom right)\n",
      "using the feature map  φ, then projecting the transformed training set down to 2D\n",
      "using linear PCA. Notice that if we could invert the linear PCA step for a given\n",
      "instance in the reduced space, the reconstructed point would lie in feature space, not\n",
      "in the original space (e.g., like the one represented by an x in the diagram). Since the\n",
      "feature space is infinite-dimensional, we cannot compute the reconstructed point,\n",
      "and therefore we cannot compute the true reconstruction error. Fortunately, it is pos‐\n",
      "sible to find a point in the original space that would map close to the reconstructed\n",
      "point. This is called the reconstruction pre-image . Once you have this pre-image, you\n",
      "can measure its squared distance to the original instance. Y ou can then select the ker‐\n",
      "nel and hyperparameters that minimize this reconstruction pre-image error.\n",
      "230 | Chapter 8: Dimensionality Reduction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7Scikit-Learn uses the algorithm based on Kernel Ridge Regression described in Gokhan H. Bakır, Jason\n",
      "Weston, and Bernhard Scholkopf, “Learning to Find Pre-images”  (Tubingen, Germany: Max Planck Institute\n",
      "for Biological Cybernetics, 2004).\n",
      "Figure 8-11. Kernel PCA and the reconstruction pre-image error\n",
      "Y ou may be wondering how to perform this reconstruction. One solution is to train a\n",
      "supervised regression model, with the projected instances as the training set and the\n",
      "original instances as the targets. Scikit-Learn will do this automatically if you set\n",
      "fit_inverse_transform=True , as shown in the following code:7\n",
      "rbf_pca = KernelPCA (n_components  = 2, kernel=\"rbf\", gamma=0.0433,\n",
      "                    fit_inverse_transform =True)\n",
      "X_reduced  = rbf_pca.fit_transform (X)\n",
      "X_preimage  = rbf_pca.inverse_transform (X_reduced )\n",
      "By default, fit_inverse_transform=False  and KernelPCA  has no\n",
      "inverse_transform()  method. This method only gets created\n",
      "when you set fit_inverse_transform=True .\n",
      "Kernel PCA | 231\n",
      "8“Nonlinear Dimensionality Reduction by Locally Linear Embedding, ” S. Roweis, L. Saul (2000).Y ou can then compute the reconstruction pre-image error:\n",
      ">>> from sklearn.metrics  import mean_squared_error\n",
      ">>> mean_squared_error (X, X_preimage )\n",
      "32.786308795766132\n",
      "Now you can use grid search with cross-validation to find the kernel and hyperpara‐\n",
      "meters that minimize this pre-image reconstruction error.\n",
      "LLE\n",
      "Locally Linear Embedding  (LLE)8 is another very powerful nonlinear dimensionality\n",
      "reduction  (NLDR) technique. It is a Manifold Learning technique that does not rely\n",
      "on projections like the previous algorithms. In a nutshell, LLE works by first measur‐\n",
      "ing how each training instance linearly relates to its closest neighbors (c.n.), and then\n",
      "looking for a low-dimensional representation of the training set where these local\n",
      "relationships are best preserved (more details shortly). This makes it particularly\n",
      "good at unrolling twisted manifolds, especially when there is not too much noise.\n",
      "For example, the following code uses Scikit-Learn’s LocallyLinearEmbedding  class to\n",
      "unroll the Swiss roll. The resulting 2D dataset is shown in Figure 8-12 . As you can\n",
      "see, the Swiss roll is completely unrolled and the distances between instances are\n",
      "locally well preserved. However, distances are not preserved on a larger scale: the left\n",
      "part of the unrolled Swiss roll is stretched, while the right part is squeezed. Neverthe‐\n",
      "less, LLE did a pretty good job at modeling the manifold.\n",
      "from sklearn.manifold  import LocallyLinearEmbedding\n",
      "lle = LocallyLinearEmbedding (n_components =2, n_neighbors =10)\n",
      "X_reduced  = lle.fit_transform (X)\n",
      "232 | Chapter 8: Dimensionality Reduction\n",
      "Figure 8-12. Unrolled Swiss roll using LLE\n",
      "Here’s how LLE works: first, for each training instance x(i), the algorithm identifies its\n",
      "k closest neighbors (in the preceding code k = 10), then tries to reconstruct x(i) as a\n",
      "linear function of these neighbors. More specifically, it finds the weights wi,j such that\n",
      "the squared distance between x(i) and ∑j= 1mwi,jxj is as small as possible, assuming wi,j\n",
      "= 0 if x(j) is not one of the k closest neighbors of x(i). Thus the first step of LLE is the\n",
      "constrained optimization problem described in Equation 8-4 , where W is the weight\n",
      "matrix containing all the weights wi,j. The second constraint simply normalizes the\n",
      "weights for each training instance x(i).\n",
      "LLE | 233\n",
      "Equation 8-4. LLE step 1: linearly modeling local relationships\n",
      "W= argmin\n",
      "W∑\n",
      "i= 1m\n",
      "xi−∑\n",
      "j= 1m\n",
      "wi,jxj2\n",
      "subject towi,j= 0 if xjis not one of the kc.n. of xi\n",
      "∑\n",
      "j= 1m\n",
      "wi,j= 1 for i= 1, 2,⋯,m\n",
      "After this step, the weight matrix W (containing the weights wi,j) encodes the local\n",
      "linear relationships between the training instances. Now the second step is to map the\n",
      "training instances into a d-dimensional space (where d < n) while preserving these\n",
      "local relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\n",
      "space, then we want the squared distance between z(i) and ∑j= 1mwi,jzj to be as small\n",
      "as possible. This idea leads to the unconstrained optimization problem described in\n",
      "Equation 8-5 . It looks very similar to the first step, but instead of keeping the instan‐\n",
      "ces fixed and finding the optimal weights, we are doing the reverse: keeping the\n",
      "weights fixed and finding the optimal position of the instances’ images in the low-\n",
      "dimensional space. Note that Z is the matrix containing all z(i).\n",
      "Equation 8-5. LLE step 2: reducing dimensionality while preserving relationships\n",
      "Z= argmin\n",
      "Z∑\n",
      "i= 1m\n",
      "zi−∑\n",
      "j= 1m\n",
      "wi,jzj2\n",
      "Scikit-Learn’s LLE implementation has the following computational complexity:\n",
      "O(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the\n",
      "weights, and O(dm2) for constructing the low-dimensional representations. Unfortu‐\n",
      "nately, the m2 in the last term makes this algorithm scale poorly to very large datasets.\n",
      "Other Dimensionality Reduction Techniques\n",
      "There are many other dimensionality reduction techniques, several of which are\n",
      "available in Scikit-Learn. Here are some of the most popular:\n",
      "•Multidimensional Scaling  (MDS) reduces dimensionality while trying to preserve\n",
      "the distances between the instances (see Figure 8-13 ).\n",
      "234 | Chapter 8: Dimensionality Reduction\n",
      "9The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\n",
      "these nodes.•Isomap  creates a graph by connecting each instance to its nearest neighbors, then\n",
      "reduces dimensionality while trying to preserve the geodesic distances9 between\n",
      "the instances.\n",
      "•t-Distributed Stochastic Neighbor Embedding  (t-SNE) reduces dimensionality\n",
      "while trying to keep similar instances close and dissimilar instances apart. It is\n",
      "mostly used for visualization, in particular to visualize clusters of instances in\n",
      "high-dimensional space (e.g., to visualize the MNIST images in 2D).\n",
      "•Linear Discriminant Analysis  (LDA) is actually a classification algorithm, but dur‐\n",
      "ing training it learns the most discriminative axes between the classes, and these\n",
      "axes can then be used to define a hyperplane onto which to project the data. The\n",
      "benefit is that the projection will keep classes as far apart as possible, so LDA is a\n",
      "good technique to reduce dimensionality before running another classification\n",
      "algorithm such as an SVM classifier.\n",
      "Figure 8-13. Reducing the Swiss roll to 2D using various techniques\n",
      "Exercises\n",
      "1.What are the main motivations for reducing a dataset’s dimensionality? What are\n",
      "the main drawbacks?\n",
      "2.What is the curse of dimensionality?\n",
      "3.Once a dataset’s dimensionality has been reduced, is it possible to reverse the\n",
      "operation? If so, how? If not, why?\n",
      "4.Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\n",
      "5.Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\n",
      "variance ratio to 95%. How many dimensions will the resulting dataset have?\n",
      "Exercises | 235\n",
      "6.In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA,\n",
      "or Kernel PCA?\n",
      "7.How can you evaluate the performance of a dimensionality reduction algorithm\n",
      "on your dataset?\n",
      "8.Does it make any sense to chain two different dimensionality reduction algo‐\n",
      "rithms?\n",
      "9.Load the MNIST dataset (introduced in Chapter 3 ) and split it into a training set\n",
      "and a test set (take the first 60,000 instances for training, and the remaining\n",
      "10,000 for testing). Train a Random Forest classifier on the dataset and time how\n",
      "long it takes, then evaluate the resulting model on the test set. Next, use PCA to\n",
      "reduce the dataset’s dimensionality, with an explained variance ratio of 95%.\n",
      "Train a new Random Forest classifier on the reduced dataset and see how long it\n",
      "takes. Was training much faster? Next evaluate the classifier on the test set: how\n",
      "does it compare to the previous classifier?\n",
      "10.Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the\n",
      "result using Matplotlib. Y ou can use a scatterplot using 10 different colors to rep‐\n",
      "resent each image’s target class. Alternatively, you can write colored digits at the\n",
      "location of each instance, or even plot scaled-down versions of the digit images\n",
      "themselves (if you plot all digits, the visualization will be too cluttered, so you\n",
      "should either draw a random sample or plot an instance only if no other instance\n",
      "has already been plotted at a close distance). Y ou should get a nice visualization\n",
      "with well-separated clusters of digits. Try using other dimensionality reduction\n",
      "algorithms such as PCA, LLE, or MDS and compare the resulting visualizations.\n",
      "Solutions to these exercises are available in ???.\n",
      "236 | Chapter 8: Dimensionality Reduction\n",
      "CHAPTER 9\n",
      "Unsupervised Learning Techniques\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 9 in the final\n",
      "release of the book.\n",
      "Although most of the applications of Machine Learning today are based on super‐\n",
      "vised learning (and as a result, this is where most of the investments go to), the vast\n",
      "majority of the available data is actually unlabeled: we have the input features X, but\n",
      "we do not have the labels y. Y ann LeCun famously said that “if intelligence was a cake,\n",
      "unsupervised learning would be the cake, supervised learning would be the icing on\n",
      "the cake, and reinforcement learning would be the cherry on the cake” . In other\n",
      "words, there is a huge potential in unsupervised learning that we have only barely\n",
      "started to sink our teeth into.\n",
      "For example, say you want to create a system that will take a few pictures of each item\n",
      "on a manufacturing production line and detect which items are defective. Y ou can\n",
      "fairly easily create a system that will take pictures automatically, and this might give\n",
      "you thousands of pictures every day. Y ou can then build a reasonably large dataset in\n",
      "just a few weeks. But wait, there are no labels! If you want to train a regular binary\n",
      "classifier that will predict whether an item is defective or not, you will need to label\n",
      "every single picture as “defective” or “normal” . This will generally require human\n",
      "experts to sit down and manually go through all the pictures. This is a long, costly\n",
      "and tedious task, so it will usually only be done on a small subset of the available pic‐\n",
      "tures. As a result, the labeled dataset will be quite small, and the classifier’s perfor‐\n",
      "mance will be disappointing. Moreover, every time the company makes any change to\n",
      "its products, the whole process will need to be started over from scratch. Wouldn’t it\n",
      "237\n",
      "be great if the algorithm could just exploit the unlabeled data without needing\n",
      "humans to label every picture? Enter unsupervised learning.\n",
      "In Chapter 8 , we looked at the most common unsupervised learning task: dimension‐\n",
      "ality reduction. In this chapter, we will look at a few more unsupervised learning tasks\n",
      "and algorithms:\n",
      "•Clustering : the goal is to group similar instances together into clusters . This is a\n",
      "great tool for data analysis, customer segmentation, recommender systems,\n",
      "search engines, image segmentation, semi-supervised learning, dimensionality\n",
      "reduction, and more.\n",
      "•Anomaly detection : the objective is to learn what “normal” data looks like, and\n",
      "use this to detect abnormal instances, such as defective items on a production\n",
      "line or a new trend in a time series.\n",
      "•Density estimation : this is the task of estimating the probability density function\n",
      "(PDF) of the random process that generated the dataset. This is commonly used\n",
      "for anomaly detection: instances located in very low-density regions are likely to\n",
      "be anomalies. It is also useful for data analysis and visualization.\n",
      "Ready for some cake? We will start with clustering, using K-Means and DBSCAN,\n",
      "and then we will discuss Gaussian mixture models and see how they can be used for\n",
      "density estimation, clustering, and anomaly detection.\n",
      "Clustering\n",
      "As you enjoy a hike in the mountains, you stumble upon a plant you have never seen\n",
      "before. Y ou look around and you notice a few more. They are not perfectly identical,\n",
      "yet they are sufficiently similar for you to know that they most likely belong to the\n",
      "same species (or at least the same genus). Y ou may need a botanist to tell you what\n",
      "species that is, but you certainly don’t need an expert to identify groups of similar-\n",
      "looking objects. This is called clustering : it is the task of identifying similar instances\n",
      "and assigning them to clusters , i.e., groups of similar instances.\n",
      "Just like in classification, each instance gets assigned to a group. However, this is an\n",
      "unsupervised task. Consider Figure 9-1 : on the left is the iris dataset (introduced in\n",
      "Chapter 4 ), where each instance’s species (i.e., its class) is represented with a different\n",
      "marker. It is a labeled dataset, for which classification algorithms such as Logistic\n",
      "Regression, SVMs or Random Forest classifiers are well suited. On the right is the\n",
      "same dataset, but without the labels, so you cannot use a classification algorithm any‐\n",
      "more. This is where clustering algorithms step in: many of them can easily detect the\n",
      "top left cluster. It is also quite easy to see with our own eyes, but it is not so obvious\n",
      "that the lower right cluster is actually composed of two distinct sub-clusters. That\n",
      "said, the dataset actually has two additional features (sepal length and width), not\n",
      "238 | Chapter 9: Unsupervised Learning Techniques\n",
      "represented here, and clustering algorithms can make good use of all features, so in\n",
      "fact they identify the three clusters fairly well (e.g., using a Gaussian mixture model,\n",
      "only 5 instances out of 150 are assigned to the wrong cluster).\n",
      "Figure 9-1. Classification  (left)  versus clustering (right)\n",
      "Clustering is used in a wide variety of applications, including:\n",
      "•For customer segmentation: you can cluster your customers based on their pur‐\n",
      "chases, their activity on your website, and so on. This is useful to understand who\n",
      "your customers are and what they need, so you can adapt your products and\n",
      "marketing campaigns to each segment. For example, this can be useful in recom‐\n",
      "mender systems  to suggest content that other users in the same cluster enjoyed.\n",
      "•For data analysis: when analyzing a new dataset, it is often useful to first discover\n",
      "clusters of similar instances, as it is often easier to analyze clusters separately.\n",
      "•As a dimensionality reduction technique: once a dataset has been clustered, it is\n",
      "usually possible to measure each instance’s affinity  with each cluster (affinity is\n",
      "any measure of how well an instance fits into a cluster). Each instance’s feature\n",
      "vector x can then be replaced with the vector of its cluster affinities. If there are k\n",
      "clusters, then this vector is k dimensional. This is typically much lower dimen‐\n",
      "sional than the original feature vector, but it can preserve enough information for\n",
      "further processing.\n",
      "•For anomaly detection  (also called outlier detection ): any instance that has a low\n",
      "affinity to all the clusters is likely to be an anomaly. For example, if you have clus‐\n",
      "tered the users of your website based on their behavior, you can detect users with\n",
      "unusual behavior, such as an unusual number of requests per second, and so on.\n",
      "Anomaly detection is particularly useful in detecting defects in manufacturing, or\n",
      "for fraud detection .\n",
      "•For semi-supervised learning: if you only have a few labels, you could perform\n",
      "clustering and propagate the labels to all the instances in the same cluster. This\n",
      "can greatly increase the amount of labels available for a subsequent supervised\n",
      "learning algorithm, and thus improve its performance.\n",
      "Clustering | 239\n",
      "1“Least square quantization in PCM, ” Stuart P . Lloyd. (1982).•For search engines: for example, some search engines let you search for images\n",
      "that are similar to a reference image. To build such a system, you would first\n",
      "apply a clustering algorithm to all the images in your database: similar images\n",
      "would end up in the same cluster. Then when a user provides a reference image,\n",
      "all you need to do is to find this image’s cluster using the trained clustering\n",
      "model, and you can then simply return all the images from this cluster.\n",
      "•To segment an image: by clustering pixels according to their color, then replacing\n",
      "each pixel’s color with the mean color of its cluster, it is possible to reduce the\n",
      "number of different colors in the image considerably. This technique is used in\n",
      "many object detection and tracking systems, as it makes it easier to detect the\n",
      "contour of each object.\n",
      "There is no universal definition of what a cluster is: it really depends on the context,\n",
      "and different algorithms will capture different kinds of clusters. For example, some\n",
      "algorithms look for instances centered around a particular point, called a centroid .\n",
      "Others look for continuous regions of densely packed instances: these clusters can\n",
      "take on any shape. Some algorithms are hierarchical, looking for clusters of clusters.\n",
      "And the list goes on.\n",
      "In this section, we will look at two popular clustering algorithms: K-Means and\n",
      "DBSCAN, and we will show some of their applications, such as non-linear dimen‐\n",
      "sionality reduction, semi-supervised learning and anomaly detection.\n",
      "K-Means\n",
      "Consider the unlabeled dataset represented in Figure 9-2 : you can clearly see 5 blobs\n",
      "of instances. The K-Means algorithm is a simple algorithm capable of clustering this\n",
      "kind of dataset very quickly and efficiently, often in just a few iterations. It was pro‐\n",
      "posed by Stuart Lloyd at the Bell Labs in 1957 as a technique for pulse-code modula‐\n",
      "tion, but it was only published outside of the company in 1982, in a paper titled\n",
      "“Least square quantization in PCM” .1 By then, in 1965, Edward W . Forgy had pub‐\n",
      "lished virtually the same algorithm, so K-Means is sometimes referred to as Lloyd-\n",
      "Forgy.\n",
      "240 | Chapter 9: Unsupervised Learning Techniques\n",
      "Figure 9-2. An unlabeled dataset composed of five blobs of instances\n",
      "Let’s train a K-Means clusterer on this dataset. It will try to find each blob’s center and\n",
      "assign each instance to the closest blob:\n",
      "from sklearn.cluster  import KMeans\n",
      "k = 5\n",
      "kmeans = KMeans(n_clusters =k)\n",
      "y_pred = kmeans.fit_predict (X)\n",
      "Note that you have to specify the number of clusters k that the algorithm must find.\n",
      "In this example, it is pretty obvious from looking at the data that k should be set to 5,\n",
      "but in general it is not that easy. We will discuss this shortly.\n",
      "Each instance was assigned to one of the 5 clusters. In the context of clustering, an\n",
      "instance’s label  is the index of the cluster that this instance gets assigned to by the\n",
      "algorithm: this is not to be confused with the class labels in classification (remember\n",
      "that clustering is an unsupervised learning task). The KMeans  instance preserves a\n",
      "copy of the labels of the instances it was trained on, available via the labels_  instance\n",
      "variable:\n",
      ">>> y_pred\n",
      "array([4, 0, 1, ..., 2, 1, 0], dtype=int32)\n",
      ">>> y_pred is kmeans.labels_\n",
      "True\n",
      "We can also take a look at the 5 centroids that the algorithm found:\n",
      ">>> kmeans.cluster_centers_\n",
      "array([[-2.80389616,  1.80117999],\n",
      "       [ 0.20876306,  2.25551336],\n",
      "       [-2.79290307,  2.79641063],\n",
      "       [-1.46679593,  2.28585348],\n",
      "       [-2.80037642,  1.30082566]])\n",
      "Of course, you can easily assign new instances to the cluster whose centroid is closest:\n",
      "Clustering | 241\n",
      ">>> X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
      ">>> kmeans.predict(X_new)\n",
      "array([1, 1, 2, 2], dtype=int32)\n",
      "If you plot the cluster’s decision boundaries, you get a Voronoi tessellation (see\n",
      "Figure 9-3 , where each centroid is represented with an X):\n",
      "Figure 9-3. K-Means decision boundaries (Voronoi tessellation)\n",
      "The vast majority of the instances were clearly assigned to the appropriate cluster, but\n",
      "a few instances were probably mislabeled (especially near the boundary between the\n",
      "top left cluster and the central cluster). Indeed, the K-Means algorithm does not\n",
      "behave very well when the blobs have very different diameters since all it cares about\n",
      "when assigning an instance to a cluster is the distance to the centroid.\n",
      "Instead of assigning each instance to a single cluster, which is called hard clustering , it\n",
      "can be useful to just give each instance a score per cluster: this is called soft clustering .\n",
      "For example, the score can be the distance between the instance and the centroid, or\n",
      "conversely it can be a similarity score (or affinity) such as the Gaussian Radial Basis\n",
      "Function (introduced in Chapter 5 ). In the KMeans  class, the transform()  method\n",
      "measures the distance from each instance to every centroid:\n",
      ">>> kmeans.transform (X_new)\n",
      "array([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],\n",
      "       [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],\n",
      "       [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],\n",
      "       [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])\n",
      "In this example, the first instance in X_new  is located at a distance of 2.81 from the\n",
      "first centroid, 0.33 from the second centroid, 2.90 from the third centroid, 1.49 from\n",
      "the fourth centroid and 2.87 from the fifth centroid. If you have a high-dimensional\n",
      "dataset and you transform it this way, you end up with a k-dimensional dataset: this\n",
      "can be a very efficient non-linear dimensionality reduction technique.\n",
      "242 | Chapter 9: Unsupervised Learning Techniques\n",
      "2This can be proven by pointing out that the mean squared distance between the instances and their closest\n",
      "centroid can only go down at each step.The K-Means Algorithm\n",
      "So how does the algorithm work? Well it is really quite simple. Suppose you were\n",
      "given the centroids: you could easily label all the instances in the dataset by assigning\n",
      "each of them to the cluster whose centroid is closest. Conversely, if you were given all\n",
      "the instance labels, you could easily locate all the centroids by computing the mean of\n",
      "the instances for each cluster. But you are given neither the labels nor the centroids,\n",
      "so how can you proceed? Well, just start by placing the centroids randomly (e.g., by\n",
      "picking k instances at random and using their locations as centroids). Then label the\n",
      "instances, update the centroids, label the instances, update the centroids, and so on\n",
      "until the centroids stop moving. The algorithm is guaranteed to converge in a finite\n",
      "number of steps (usually quite small), it will not oscillate forever2. Y ou can see the\n",
      "algorithm in action in Figure 9-4 : the centroids are initialized randomly (top left),\n",
      "then the instances are labeled (top right), then the centroids are updated (center left),\n",
      "the instances are relabeled (center right), and so on. As you can see, in just 3 itera‐\n",
      "tions the algorithm has reached a clustering that seems close to optimal.\n",
      "Figure 9-4. The K-Means algorithm\n",
      "Clustering | 243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7Note that this solution is generally not unique: in general when the data are linearly separable, there is an\n",
      "infinity of hyperplanes that can separate them.(or Hebbian learning ); that is, the connection weight between two neurons is\n",
      "increased whenever they have the same output. Perceptrons are trained using a var‐\n",
      "iant of this rule that takes into account the error made by the network; it reinforces\n",
      "connections that help reduce the error. More specifically, the Perceptron is fed one\n",
      "training instance at a time, and for each instance it makes its predictions. For every\n",
      "output neuron that produced a wrong prediction, it reinforces the connection\n",
      "weights from the inputs that would have contributed to the correct prediction. The\n",
      "rule is shown in Equation 10-3 .\n",
      "Equation 10-3. Perceptron learning rule (weight update)\n",
      "wi,jnext step=wi,j+ηyj−yjxi\n",
      "•wi, j is the connection weight between the ith input neuron and the jth output neu‐\n",
      "ron.\n",
      "•xi is the ith input value of the current training instance.\n",
      "•yj is the output of the jth output neuron for the current training instance.\n",
      "•yj is the target output of the jth output neuron for the current training instance.\n",
      "•η is the learning rate.\n",
      "The decision boundary of each output neuron is linear, so Perceptrons are incapable\n",
      "of learning complex patterns (just like Logistic Regression classifiers). However, if the\n",
      "training instances are linearly separable, Rosenblatt demonstrated that this algorithm\n",
      "would converge to a solution.7 This is called the Perceptron convergence theorem .\n",
      "Scikit-Learn provides a Perceptron  class that implements a single TLU network. It\n",
      "can be used pretty much as you would expect—for example, on the iris dataset (intro‐\n",
      "duced in Chapter 4 ):\n",
      "import numpy as np\n",
      "from sklearn.datasets  import load_iris\n",
      "from sklearn.linear_model  import Perceptron\n",
      "iris = load_iris ()\n",
      "X = iris.data[:, (2, 3)]  # petal length, petal width\n",
      "y = (iris.target == 0).astype(np.int)  # Iris Setosa?\n",
      "per_clf = Perceptron ()\n",
      "per_clf.fit(X, y)\n",
      "284 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "y_pred = per_clf.predict([[2, 0.5]])\n",
      "Y ou may have noticed the fact that the Perceptron learning algorithm strongly resem‐\n",
      "bles Stochastic Gradient Descent. In fact, Scikit-Learn’s Perceptron  class is equivalent\n",
      "to using an SGDClassifier  with the following hyperparameters: loss=\"perceptron\" ,\n",
      "learning_rate=\"constant\" , eta0=1  (the learning rate), and penalty=None  (no regu‐\n",
      "larization).\n",
      "Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class\n",
      "probability; rather, they just make predictions based on a hard threshold. This is one\n",
      "of the good reasons to prefer Logistic Regression over Perceptrons.\n",
      "In their 1969 monograph titled Perceptrons , Marvin Minsky and Seymour Papert\n",
      "highlighted a number of serious weaknesses of Perceptrons, in particular the fact that\n",
      "they are incapable of solving some trivial problems (e.g., the Exclusive OR  (XOR)\n",
      "classification problem; see the left side of Figure 10-6 ). Of course this is true of any\n",
      "other linear classification model as well (such as Logistic Regression classifiers), but\n",
      "researchers had expected much more from Perceptrons, and their disappointment\n",
      "was great, and many researchers dropped neural networks altogether in favor of\n",
      "higher-level problems such as logic, problem solving, and search.\n",
      "However, it turns out that some of the limitations of Perceptrons can be eliminated by\n",
      "stacking multiple Perceptrons. The resulting ANN is called a Multi-Layer Perceptron\n",
      "(MLP). In particular, an MLP can solve the XOR problem, as you can verify by com‐\n",
      "puting the output of the MLP represented on the right of Figure 10-6 : with inputs (0,\n",
      "0) or (1, 1) the network outputs 0, and with inputs (0, 1) or (1, 0) it outputs 1. All\n",
      "connections have a weight equal to 1, except the four connections where the weight is\n",
      "shown. Try verifying that this network indeed solves the XOR problem!\n",
      "Figure 10-6. XOR classification  problem and an MLP that solves it\n",
      "From Biological to Artificial  Neurons | 285\n",
      "8In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\n",
      "ANNs with dozens of layers, or even hundreds, so the definition of “deep” is quite fuzzy.\n",
      "9“Learning Internal Representations by Error Propagation, ” D. Rumelhart, G. Hinton, R. Williams (1986).\n",
      "Multi-Layer Perceptron and Backpropagation\n",
      "An MLP is composed of one (passthrough) input layer , one or more layers of TLUs,\n",
      "called hidden layers , and one final layer of TLUs called the output layer  (see\n",
      "Figure 10-7 ). The layers close to the input layer are usually called the lower layers,\n",
      "and the ones close to the outputs are usually called the upper layers. Every layer\n",
      "except the output layer includes a bias neuron and is fully connected to the next layer.\n",
      "Figure 10-7. Multi-Layer Perceptron\n",
      "The signal flows only in one direction (from the inputs to the out‐\n",
      "puts), so this architecture is an example of a feedforward neural net‐\n",
      "work  (FNN).\n",
      "When an ANN contains a deep stack of hidden layers8, it is called a deep neural net‐\n",
      "work  (DNN). The field of Deep Learning studies DNNs, and more generally models\n",
      "containing deep stacks of computations. However, many people talk about Deep\n",
      "Learning whenever neural networks are involved (even shallow ones).\n",
      "For many years researchers struggled to find a way to train MLPs, without success.\n",
      "But in 1986, David Rumelhart, Geoffrey Hinton and Ronald Williams published a\n",
      "groundbreaking paper9 introducing the backpropagation  training algorithm, which is\n",
      "still used today. In short, it is simply Gradient Descent (introduced in Chapter 4 )\n",
      "286 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "10This technique was actually independently invented several times by various researchers in different fields,\n",
      "starting with P . Werbos in 1974.\n",
      "using an efficient technique for computing the gradients automatically10: in just two\n",
      "passes through the network (one forward, one backward), the backpropagation algo‐\n",
      "rithm is able to compute the gradient of the network’s error with regards to every sin‐\n",
      "gle model parameter. In other words, it can find out how each connection weight and\n",
      "each bias term should be tweaked in order to reduce the error. Once it has these gra‐\n",
      "dients, it just performs a regular Gradient Descent step, and the whole process is\n",
      "repeated until the network converges to the solution.\n",
      "Automatically computing gradients is called automatic differentia‐\n",
      "tion, or autodiff . There are various autodiff techniques, with differ‐\n",
      "ent pros and cons. The one used by backpropagation is called\n",
      "reverse-mode autodiff . It is fast and precise, and is well suited when\n",
      "the function to differentiate has many variables (e.g., connection\n",
      "weights) and few outputs (e.g., one loss). If you want to learn more\n",
      "about autodiff, check out ???.\n",
      "Let’s run through this algorithm in a bit more detail:\n",
      "•It handles one mini-batch at a time (for example containing 32 instances each),\n",
      "and it goes through the full training set multiple times. Each pass is called an\n",
      "epoch , as we saw in Chapter 4 .\n",
      "•Each mini-batch is passed to the network’s input layer, which just sends it to the\n",
      "first hidden layer. The algorithm then computes the output of all the neurons in\n",
      "this layer (for every instance in the mini-batch). The result is passed on to the\n",
      "next layer, its output is computed and passed to the next layer, and so on until we\n",
      "get the output of the last layer, the output layer. This is the forward pass : it is\n",
      "exactly like making predictions, except all intermediate results are preserved\n",
      "since they are needed for the backward pass.\n",
      "•Next, the algorithm measures the network’s output error (i.e., it uses a loss func‐\n",
      "tion that compares the desired output and the actual output of the network, and\n",
      "returns some measure of the error).\n",
      "•Then it computes how much each output connection contributed to the error.\n",
      "This is done analytically by simply applying  the chain rule  (perhaps the most fun‐\n",
      "damental rule in calculus), which makes this step fast and precise.\n",
      "•The algorithm then measures how much of these error contributions came from\n",
      "each connection in the layer below, again using the chain rule—and so on until\n",
      "the algorithm reaches the input layer. As we explained earlier, this reverse pass\n",
      "efficiently measures the error gradient across all the connection weights in the\n",
      "From Biological to Artificial  Neurons | 287\n",
      "network by propagating the error gradient backward through the network (hence\n",
      "the name of the algorithm).\n",
      "•Finally, the algorithm performs a Gradient Descent step to tweak all the connec‐\n",
      "tion weights in the network, using the error gradients it just computed.\n",
      "This algorithm is so important, it’s worth summarizing it again: for each training\n",
      "instance the backpropagation algorithm first makes a prediction (forward pass),\n",
      "measures the error, then goes through each layer in reverse to measure the error con‐\n",
      "tribution from each connection (reverse pass), and finally slightly tweaks the connec‐\n",
      "tion weights to reduce the error (Gradient Descent step).\n",
      "It is important to initialize all the hidden layers’ connection weights\n",
      "randomly, or else training will fail. For example, if you initialize all\n",
      "weights and biases to zero, then all neurons in a given layer will be\n",
      "perfectly identical, and thus backpropagation will affect them in\n",
      "exactly the same way, so they will remain identical. In other words,\n",
      "despite having hundreds of neurons per layer, your model will act\n",
      "as if it had only one neuron per layer: it won’t be too smart. If\n",
      "instead you randomly initialize the weights, you break the symme‐\n",
      "try and allow backpropagation to train a diverse team of neurons.\n",
      "In order for this algorithm to work properly, the authors made a key change to the\n",
      "MLP’s architecture: they replaced the step function with the logistic function, σ(z) =\n",
      "1 / (1 + exp(– z)). This was essential because the step function contains only flat seg‐\n",
      "ments, so there is no gradient to work with (Gradient Descent cannot move on a flat\n",
      "surface), while the logistic function has a well-defined nonzero derivative every‐\n",
      "where, allowing Gradient Descent to make some progress at every step. In fact, the\n",
      "backpropagation algorithm works well with many other activation functions , not just\n",
      "the logistic function. Two other popular activation functions are:\n",
      "The hyperbolic tangent function tanh(z) = 2σ(2z) – 1\n",
      "Just like the logistic function it is S-shaped, continuous, and differentiable, but its\n",
      "output value ranges from –1 to 1 (instead of 0 to 1 in the case of the logistic func‐\n",
      "tion), which tends to make each layer’s output more or less centered around 0 at\n",
      "the beginning of training. This often helps speed up convergence.\n",
      "The Rectified  Linear Unit function: ReLU(z) = max(0, z)\n",
      "It is continuous but unfortunately not differentiable at z = 0 (the slope changes\n",
      "abruptly, which can make Gradient Descent bounce around), and its derivative is\n",
      "0 for z < 0. However, in practice it works very well and has the advantage of being\n",
      "288 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "11Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck\n",
      "to sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is\n",
      "one of the cases where the biological analogy was misleading.fast to compute11. Most importantly, the fact that it does not have a maximum\n",
      "output value also helps reduce some issues during Gradient Descent (we will\n",
      "come back to this in Chapter 11 ).\n",
      "These popular activation functions and their derivatives are represented in\n",
      "Figure 10-8 . But wait! Why do we need activation functions in the first place? Well, if\n",
      "you chain several linear transformations, all you get is a linear transformation. For\n",
      "example, say f( x) = 2 x + 3 and g( x) = 5 x - 1, then chaining these two linear functions\n",
      "gives you another linear function: f(g( x)) = 2(5 x - 1) + 3 = 10 x + 1. So if you don’t\n",
      "have some non-linearity between layers, then even a deep stack of layers is equivalent\n",
      "to a single layer: you cannot solve very complex problems with that.\n",
      "Figure 10-8. Activation functions and their derivatives\n",
      "Okay! So now you know where neural nets came from, what their architecture is and\n",
      "how to compute their outputs, and you also learned about the backpropagation algo‐\n",
      "rithm. But what exactly can you do with them?\n",
      "Regression MLPs\n",
      "First, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,\n",
      "the price of a house given many of its features), then you just need a single output\n",
      "neuron: its output is the predicted value. For multivariate regression (i.e., to predict\n",
      "multiple values at once), you need one output neuron per output dimension. For\n",
      "example, to locate the center of an object on an image, you need to predict 2D coordi‐\n",
      "nates, so you need two output neurons. If you also want to place a bounding box\n",
      "around the object, then you need two more numbers: the width and the height of the\n",
      "object. So you end up with 4 output neurons.\n",
      "From Biological to Artificial  Neurons | 289\n",
      "In general, when building an MLP for regression, you do not want to use any activa‐\n",
      "tion function for the output neurons, so they are free to output any range of values.\n",
      "However, if you want to guarantee that the output will always be positive, then you\n",
      "can use the ReLU activation function, or the softplus  activation function in the output\n",
      "layer. Finally, if you want to guarantee that the predictions will fall within a given\n",
      "range of values, then you can use the logistic function or the hyperbolic tangent, and\n",
      "scale the labels to the appropriate range: 0 to 1 for the logistic function, or –1 to 1 for\n",
      "the hyperbolic tangent.\n",
      "The loss function to use during training is typically the mean squared error, but if you\n",
      "have a lot of outliers in the training set, you may prefer to use the mean absolute\n",
      "error instead. Alternatively, you can use the Huber loss, which is a combination of\n",
      "both.\n",
      "The Huber loss is quadratic when the error is smaller than a thres‐\n",
      "hold δ (typically 1), but linear when the error is larger than δ. This\n",
      "makes it less sensitive to outliers than the mean squared error, and\n",
      "it is often more precise and converges faster than the mean abso‐\n",
      "lute error.\n",
      "Table 10-1  summarizes the typical architecture of a regression MLP .\n",
      "Table 10-1. Typical Regression MLP Architecture\n",
      "Hyperparameter Typical Value\n",
      "# input neurons One per input feature (e.g., 28 x 28 = 784 for MNIST)\n",
      "# hidden layers Depends on the problem. Typically 1 to 5.\n",
      "# neurons per hidden layer Depends on the problem. Typically 10 to 100.\n",
      "# output neurons 1 per prediction dimension\n",
      "Hidden activation ReLU (or SELU, see Chapter 11 )\n",
      "Output activation None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs)\n",
      "Loss function MSE or MAE/Huber (if outliers)\n",
      "Classification  MLPs\n",
      "MLPs can also be used for classification tasks. For a binary classification problem,\n",
      "you just need a single output neuron using the logistic activation function: the output\n",
      "will be a number between 0 and 1, which you can interpret as the estimated probabil‐\n",
      "ity of the positive class. Obviously, the estimated probability of the negative class is\n",
      "equal to one minus that number.\n",
      "MLPs can also easily handle multilabel binary classification tasks (see Chapter 3 ). For\n",
      "example, you could have an email classification system that predicts whether each\n",
      "incoming email is ham or spam, and simultaneously predicts whether it is an urgent\n",
      "290 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "or non-urgent email. In this case, you would need two output neurons, both using\n",
      "the logistic activation function: the first would output the probability that the email is\n",
      "spam and the second would output the probability that it is urgent. More generally,\n",
      "you would dedicate one output neuron for each positive class. Note that the output\n",
      "probabilities do not necessarily add up to one. This lets the model output any combi‐\n",
      "nation of labels: you can have non-urgent ham, urgent ham, non-urgent spam, and\n",
      "perhaps even urgent spam (although that would probably be an error).\n",
      "If each instance can belong only to a single class, out of 3 or more possible classes\n",
      "(e.g., classes 0 through 9 for digit image classification), then you need to have one\n",
      "output neuron per class, and you should use the softmax  activation function for the\n",
      "whole output layer (see Figure 10-9 ). The softmax function (introduced in Chapter 4 )\n",
      "will ensure that all the estimated probabilities are between 0 and 1 and that they add\n",
      "up to one (which is required if the classes are exclusive). This is called multiclass clas‐\n",
      "sification.\n",
      "Figure 10-9. A modern MLP (including ReLU and softmax)  for classification\n",
      "Regarding the loss function, since we are predicting probability distributions, the\n",
      "cross-entropy (also called the log loss, see Chapter 4 ) is generally a good choice.\n",
      "Table 10-2  summarizes the typical architecture of a classification MLP .\n",
      "Table 10-2. Typical Classification  MLP Architecture\n",
      "Hyperparameter Binary classification Multilabel binary classification Multiclass classification\n",
      "Input and hidden layers Same as regression Same as regression Same as regression\n",
      "# output neurons 1 1 per label 1 per class\n",
      "Output layer activation Logistic Logistic Softmax\n",
      "From Biological to Artificial  Neurons | 291\n",
      "12Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\n",
      "Hyperparameter Binary classification Multilabel binary classification Multiclass classification\n",
      "Loss function Cross-Entropy Cross-Entropy Cross-Entropy\n",
      "Before we go on, I recommend you go through exercise 1, at the\n",
      "end of this chapter. Y ou will play with various neural network\n",
      "architectures and visualize their outputs using the TensorFlow Play‐\n",
      "ground . This will be very useful to better understand MLPs, for\n",
      "example the effects of all the hyperparameters (number of layers\n",
      "and neurons, activation functions, and more).\n",
      "Now you have all the concepts you need to start implementing MLPs with Keras!\n",
      "Implementing MLPs with Keras\n",
      "Keras is a high-level Deep Learning API that allows you to easily build, train, evaluate\n",
      "and execute all sorts of neural networks. Its documentation (or specification) is avail‐\n",
      "able at https://keras.io . The reference implementation is simply called Keras as well, so\n",
      "to avoid any confusion we will call it keras-team (since it is available at https://\n",
      "github.com/keras-team/keras ). It was developed by François Chollet as part of a\n",
      "research project12 and released as an open source project in March 2015. It quickly\n",
      "gained popularity owing to its ease-of-use, flexibility and beautiful design. To per‐\n",
      "form the heavy computations required by neural networks, keras-team relies on a\n",
      "computation backend. At the present, you can choose from three popular open\n",
      "source deep learning libraries: TensorFlow, Microsoft Cognitive Toolkit (CNTK) or\n",
      "Theano.\n",
      "Moreover, since late 2016, other implementations have been released. Y ou can now\n",
      "run Keras on Apache MXNet, Apple’s Core ML, Javascript or Typescript (to run Keras\n",
      "code in a web browser), or PlaidML (which can run on all sorts of GPU devices, not\n",
      "just Nvidia). Moreover, TensorFlow itself now comes bundled with its own Keras\n",
      "implementation called tf.keras. It only supports TensorFlow as the backend, but it has\n",
      "the advantage of offering some very useful extra features (see Figure 10-10 ): for\n",
      "example, it supports TensorFlow’s Data API which makes it quite easy to load and\n",
      "preprocess data efficiently. For this reason, we will use tf.keras in this book. However,\n",
      "in this chapter we will not use any of the TensorFlow-specific features, so the code\n",
      "should run fine on other Keras implementations as well (at least in Python), with only\n",
      "minor modifications, such as changing the imports.\n",
      "292 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "Figure 10-10. Two Keras implementations: keras-team (left)  and tf.keras (right)\n",
      "As tf.keras is bundled with TensorFlow, let’s install TensorFlow!\n",
      "Installing TensorFlow 2\n",
      "Assuming you installed Jupyter and Scikit-Learn by following the installation instruc‐\n",
      "tions in Chapter 2 , you can simply use pip to install TensorFlow. If you created an\n",
      "isolated environment using virtualenv, you first need to activate it:\n",
      "$ cd $ML_PATH              # Your ML working directory (e.g., $HOME/ml)\n",
      "$ source env/bin/activate  # on Linux or MacOSX\n",
      "$ .\\env\\Scripts\\activate   # on Windows\n",
      "Next, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis‐\n",
      "trator rights, or to add the --user  option):\n",
      "$ python3 -m pip install --upgrade tensorflow\n",
      "For GPU support, you need to install tensorflow-gpu  instead of\n",
      "tensorflow , and there are other libraries to install. See https://\n",
      "tensorflow.org/install/gpu  for more details.\n",
      "To test your installation, open a Python shell or a Jupyter notebook, then import Ten‐\n",
      "sorFlow and tf.keras, and print their versions:\n",
      ">>> import tensorflow  as tf\n",
      ">>> from tensorflow  import keras\n",
      ">>> tf.__version__\n",
      "'2.0.0'\n",
      ">>> keras.__version__\n",
      "'2.2.4-tf'\n",
      "Implementing MLPs with Keras | 293\n",
      "The second version is the version of the Keras API implemented by tf.keras. Note that\n",
      "it ends with -tf, highlighting the fact that tf.keras implements the Keras API, plus\n",
      "some extra TensorFlow-specific features.\n",
      "Now let’s use tf.keras! Let’s start by building a simple image classifier.\n",
      "Building an Image Classifier  Using the Sequential API\n",
      "First, we need to load a dataset. We will tackle Fashion MNIST , which is a drop-in\n",
      "replacement of MNIST (introduced in Chapter 3 ). It has the exact same format as\n",
      "MNIST (70,000 grayscale images of 28×28 pixels each, with 10 classes), but the\n",
      "images represent fashion items rather than handwritten digits, so each class is more\n",
      "diverse and the problem turns out to be significantly more challenging than MNIST.\n",
      "For example, a simple linear model reaches about 92% accuracy on MNIST, but only\n",
      "about 83% on Fashion MNIST.\n",
      "Using Keras to Load the Dataset\n",
      "Keras provides some utility functions to fetch and load common datasets, including\n",
      "MNIST, Fashion MNIST, the original California housing dataset, and more. Let’s load\n",
      "Fashion MNIST:\n",
      "fashion_mnist  = keras.datasets .fashion_mnist\n",
      "(X_train_full , y_train_full ), (X_test, y_test) = fashion_mnist .load_data ()\n",
      "When loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one\n",
      "important difference is that every image is represented as a 28×28 array rather than a\n",
      "1D array of size 784. Moreover, the pixel intensities are represented as integers (from\n",
      "0 to 255) rather than floats (from 0.0 to 255.0). Here is the shape and data type of the\n",
      "training set:\n",
      ">>> X_train_full .shape\n",
      "(60000, 28, 28)\n",
      ">>> X_train_full .dtype\n",
      "dtype('uint8')\n",
      "Note that the dataset is already split into a training set and a test set, but there is no\n",
      "validation set, so let’s create one. Moreover, since we are going to train the neural net‐\n",
      "work using Gradient Descent, we must scale the input features. For simplicity, we just\n",
      "scale the pixel intensities down to the 0-1 range by dividing them by 255.0 (this also\n",
      "converts them to floats):\n",
      "X_valid, X_train = X_train_full [:5000] / 255.0, X_train_full [5000:] / 255.0\n",
      "y_valid, y_train = y_train_full [:5000], y_train_full [5000:]\n",
      "With MNIST, when the label is equal to 5, it means that the image represents the\n",
      "handwritten digit 5. Easy. However, for Fashion MNIST, we need the list of class\n",
      "names to know what we are dealing with:\n",
      "294 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "class_names  = [\"T-shirt/top\" , \"Trouser\" , \"Pullover\" , \"Dress\", \"Coat\",\n",
      "               \"Sandal\" , \"Shirt\", \"Sneaker\" , \"Bag\", \"Ankle boot\" ]\n",
      "For example, the first image in the training set represents a coat:\n",
      ">>> class_names [y_train[0]]\n",
      "'Coat'\n",
      "Figure 10-11  shows a few samples from the Fashion MNIST dataset:\n",
      "Figure 10-11. Samples from Fashion MNIST\n",
      "Creating the Model Using the Sequential API\n",
      "Now let’s build the neural network! Here is a classification MLP with two hidden lay‐\n",
      "ers:\n",
      "model = keras.models.Sequential ()\n",
      "model.add(keras.layers.Flatten(input_shape =[28, 28]))\n",
      "model.add(keras.layers.Dense(300, activation =\"relu\"))\n",
      "model.add(keras.layers.Dense(100, activation =\"relu\"))\n",
      "model.add(keras.layers.Dense(10, activation =\"softmax\" ))\n",
      "Let’s go through this code line by line:\n",
      "•The first line creates a Sequential  model. This is the simplest kind of Keras\n",
      "model, for neural networks that are just composed of a single stack of layers, con‐\n",
      "nected sequentially. This is called the sequential API.\n",
      "•Next, we build the first layer and add it to the model. It is a Flatten  layer whose\n",
      "role is simply to convert each input image into a 1D array: if it receives input data\n",
      "X, it computes X.reshape(-1, 1) . This layer does not have any parameters, it is\n",
      "just there to do some simple preprocessing. Since it is the first layer in the model,\n",
      "you should specify the input_shape : this does not include the batch size, only the\n",
      "shape of the instances. Alternatively, you could add a keras.layers.InputLayer\n",
      "as the first layer, setting shape=[28,28] .\n",
      "•Next we add a Dense  hidden layer with 300 neurons. It will use the ReLU activa‐\n",
      "tion function. Each Dense  layer manages its own weight matrix, containing all the\n",
      "connection weights between the neurons and their inputs. It also manages a vec‐\n",
      "Implementing MLPs with Keras | 295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tor of bias terms (one per neuron). When it receives some input data, it computes\n",
      "Equation 10-2 .\n",
      "•Next we add a second Dense  hidden layer with 100 neurons, also using the ReLU\n",
      "activation function.\n",
      "•Finally, we add a Dense  output layer with 10 neurons (one per class), using the\n",
      "softmax activation function (because the classes are exclusive).\n",
      "Specifying activation=\"relu\"  is equivalent to activa\n",
      "tion=keras.activations.relu . Other activation functions are\n",
      "available in the keras.activations  package, we will use many of\n",
      "them in this book. See https://keras.io/activations/  for the full list.\n",
      "Instead of adding the layers one by one as we just did, you can pass a list of layers\n",
      "when creating the Sequential  model:\n",
      "model = keras.models.Sequential ([\n",
      "    keras.layers.Flatten(input_shape =[28, 28]),\n",
      "    keras.layers.Dense(300, activation =\"relu\"),\n",
      "    keras.layers.Dense(100, activation =\"relu\"),\n",
      "    keras.layers.Dense(10, activation =\"softmax\" )\n",
      "])\n",
      "Using Code Examples From keras.io\n",
      "Code examples documented on keras.io will work fine with tf.keras, but you need to\n",
      "change the imports. For example, consider this keras.io code:\n",
      "from keras.layers  import Dense\n",
      "output_layer  = Dense(10)\n",
      "Y ou must change the imports like this:\n",
      "from tensorflow.keras.layers  import Dense\n",
      "output_layer  = Dense(10)\n",
      "Or simply use full paths, if you prefer:\n",
      "from tensorflow  import keras\n",
      "output_layer  = keras.layers.Dense(10)\n",
      "This is more verbose, but I use this approach in this book so you can easily see which\n",
      "packages to use, and to avoid confusion between standard classes and custom classes.\n",
      "In production code, I use the previous approach, as do most people.\n",
      "296 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "13Y ou can also generate an image of your model using keras.utils.plot_model() .The model’s summary()  method displays all the model’s layers13, including each layer’s\n",
      "name (which is automatically generated unless you set it when creating the layer), its\n",
      "output shape ( None  means the batch size can be anything), and its number of parame‐\n",
      "ters. The summary ends with the total number of parameters, including trainable and\n",
      "non-trainable parameters. Here we only have trainable parameters (we will see exam‐\n",
      "ples of non-trainable parameters in Chapter 11 ):\n",
      ">>> model.summary()\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #\n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0\n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500\n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100\n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010\n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "Note that Dense  layers often have a lot of parameters. For example, the first hidden\n",
      "layer has 784 × 300 connection weights, plus 300 bias terms, which adds up to\n",
      "235,500 parameters! This gives the model quite a lot of flexibility to fit the training\n",
      "data, but it also means that the model runs the risk of overfitting, especially when you\n",
      "do not have a lot of training data. We will come back to this later.\n",
      "Y ou can easily get a model’s list of layers, to fetch a layer by its index, or you can fetch\n",
      "it by name:\n",
      ">>> model.layers\n",
      "[<tensorflow.python.keras.layers.core.Flatten at 0x132414e48>,\n",
      " <tensorflow.python.keras.layers.core.Dense at 0x1324149b0>,\n",
      " <tensorflow.python.keras.layers.core.Dense at 0x1356ba8d0>,\n",
      " <tensorflow.python.keras.layers.core.Dense at 0x13240d240>]\n",
      ">>> model.layers[1].name\n",
      "'dense_3'\n",
      ">>> model.get_layer ('dense_3' ).name\n",
      "'dense_3'\n",
      "All the parameters of a layer can be accessed using its get_weights()  and\n",
      "set_weights()  method. For a Dense  layer, this includes both the connection weights\n",
      "and the bias terms:\n",
      "Implementing MLPs with Keras | 297\n",
      ">>> weights, biases = hidden1.get_weights ()\n",
      ">>> weights\n",
      "array([[ 0.03854964, -0.04054524,  0.00599282, ...,  0.02566582,\n",
      "         0.01032123,  0.06914985],\n",
      "       ...,\n",
      "       [ 0.02632413, -0.05105981, -0.00332005, ...,  0.04175945,\n",
      "         0.0443138 , -0.05558084]], dtype=float32)\n",
      ">>> weights.shape\n",
      "(784, 300)\n",
      ">>> biases\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., ...,  0., 0., 0.], dtype=float32)\n",
      ">>> biases.shape\n",
      "(300,)\n",
      "Notice that the Dense  layer initialized the connection weights randomly (which is\n",
      "needed to break symmetry, as we discussed earlier), and the biases were just initial‐\n",
      "ized to zeros, which is fine. If you ever want to use a different initialization method,\n",
      "you can set kernel_initializer  (kernel  is another name for the matrix of connec‐\n",
      "tion weights) or bias_initializer  when creating the layer. We will discuss initializ‐\n",
      "ers further in Chapter 11 , but if you want the full list, see https://keras.io/initializers/ .\n",
      "The shape of the weight matrix depends on the number of inputs.\n",
      "This is why it is recommended to specify the input_shape  when\n",
      "creating the first layer in a Sequential  model. However, if you do\n",
      "not specify the input shape, it’s okay: Keras will simply wait until it\n",
      "knows the input shape before it actually builds the model. This will\n",
      "happen either when you feed it actual data (e.g., during training),\n",
      "or when you call its build()  method. Until the model is really\n",
      "built, the layers will not have any weights, and you will not be able\n",
      "to do certain things (such as print the model summary or save the\n",
      "model), so if you know the input shape when creating the model, it\n",
      "is best to specify it.\n",
      "Compiling the Model\n",
      "After a model is created, you must call its compile()  method to specify the loss func‐\n",
      "tion and the optimizer to use. Optionally, you can also specify a list of extra metrics to\n",
      "compute during training and evaluation:\n",
      "model.compile(loss=\"sparse_categorical_crossentropy\" ,\n",
      "              optimizer =\"sgd\",\n",
      "              metrics=[\"accuracy\" ])\n",
      "298 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "Using loss=\"sparse_categorical_crossentropy\"  is equivalent to\n",
      "loss=keras.losses.sparse_categorical_crossentropy . Simi‐\n",
      "larly, optimizer=\"sgd\"  is equivalent to optimizer=keras.optimiz\n",
      "ers.SGD()  and metrics=[\"accuracy\"]  is equivalent to\n",
      "metrics=[keras.metrics.sparse_categorical_accuracy]  (when\n",
      "using this loss). We will use many other losses, optimizers and met‐\n",
      "rics in this book, but for the full lists see https://keras.io/losses/ ,\n",
      "https://keras.io/optimizers/  and https://keras.io/metrics/ .\n",
      "This requires some explanation. First, we use the \"sparse_categorical_crossen\n",
      "tropy\"  loss because we have sparse labels (i.e., for each instance there is just a target\n",
      "class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had\n",
      "one target probability per class for each instance (such as one-hot vectors, e.g. [0.,\n",
      "0., 0., 1., 0., 0., 0., 0., 0., 0.]  to represent class 3), then we would need\n",
      "to use the \"categorical_crossentropy\"  loss instead. If we were doing binary classi‐\n",
      "fication (with one or more binary labels), then we would use the \"sigmoid\"  (i.e.,\n",
      "logistic) activation function in the output layer instead of the \"softmax\"  activation\n",
      "function, and we would use the \"binary_crossentropy\"  loss.\n",
      "If you want to convert sparse labels (i.e., class indices) to one-hot\n",
      "vector labels, you can use the keras.utils.to_categorical()\n",
      "function. To go the other way round, you can just use the np.arg\n",
      "max()  function with axis=1 .\n",
      "Secondly, regarding the optimizer, \"sgd\"  simply means that we will train the model\n",
      "using simple Stochastic Gradient Descent. In other words, Keras will perform the\n",
      "backpropagation algorithm described earlier (i.e., reverse-mode autodiff + Gradient\n",
      "Descent). We will discuss more efficient optimizers in Chapter 11  (they improve the\n",
      "Gradient Descent part, not the autodiff).\n",
      "Finally, since this is a classifier, it’s useful to measure its \"accuracy\"  during training\n",
      "and evaluation.\n",
      "Training and Evaluating the Model\n",
      "Now the model is ready to be trained. For this we simply need to call its fit()\n",
      "method. We pass it the input features ( X_train ) and the target classes ( y_train ), as\n",
      "well as the number of epochs to train (or else it would default to just 1, which would\n",
      "definitely not be enough to converge to a good solution). We also pass a validation set\n",
      "(this is optional): Keras will measure the loss and the extra metrics on this set at the\n",
      "end of each epoch, which is very useful to see how well the model really performs: if\n",
      "the performance on the training set is much better than on the validation set, your\n",
      "Implementing MLPs with Keras | 299\n",
      "model is probably overfitting the training set (or there is a bug, such as a data mis‐\n",
      "match between the training set and the validation set):\n",
      ">>> history = model.fit(X_train, y_train, epochs=30,\n",
      "...                     validation_data =(X_valid, y_valid))\n",
      "...\n",
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==========] - 3s 55us/sample - loss: 1.4948     - acc: 0.5757\n",
      "                                          - val_loss: 1.0042 - val_acc: 0.7166\n",
      "Epoch 2/30\n",
      "55000/55000 [==========] - 3s 55us/sample - loss: 0.8690     - acc: 0.7318\n",
      "                                          - val_loss: 0.7549 - val_acc: 0.7616\n",
      "[...]\n",
      "Epoch 50/50\n",
      "55000/55000 [==========] - 4s 72us/sample - loss: 0.3607     - acc: 0.8752\n",
      "                                          - val_loss: 0.3706 - val_acc: 0.8728\n",
      "And that’s it! The neural network is trained. At each epoch during training, Keras dis‐\n",
      "plays the number of instances processed so far (along with a progress bar), the mean\n",
      "training time per sample, the loss and accuracy (or any other extra metrics you asked\n",
      "for), both on the training set and the validation set. Y ou can see that the training loss\n",
      "went down, which is a good sign, and the validation accuracy reached 87.28% after 50\n",
      "epochs, not too far from the training accuracy, so there does not seem to be much\n",
      "overfitting going on.\n",
      "Instead of passing a validation set using the validation_data\n",
      "argument, you could instead set validation_split  to the ratio of\n",
      "the training set that you want Keras to use for validation (e.g., 0.1).\n",
      "If the training set was very skewed, with some classes being overrepresented and oth‐\n",
      "ers underrepresented, it would be useful to set the class_weight  argument when\n",
      "calling the fit()  method, giving a larger weight to underrepresented classes, and a\n",
      "lower weight to overrepresented classes. These weights would be used by Keras when\n",
      "computing the loss. If you need per-instance weights instead, you can set the sam\n",
      "ple_weight  argument (it supersedes class_weight ). This could be useful for exam‐\n",
      "ple if some instances were labeled by experts while others were labeled using a\n",
      "crowdsourcing platform: you might want to give more weight to the former. Y ou can\n",
      "also provide sample weights (but not class weights) for the validation set by adding\n",
      "them as a third item in the validation_data  tuple.\n",
      "The fit()  method returns a History  object containing the training parameters ( his\n",
      "tory.params ), the list of epochs it went through ( history.epoch ), and most impor‐\n",
      "tantly a dictionary ( history.history ) containing the loss and extra metrics it\n",
      "measured at the end of each epoch on the training set and on the validation set (if\n",
      "300 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "any). If you create a Pandas DataFrame using this dictionary and call its plot()\n",
      "method, you get the learning curves shown in Figure 10-12 :\n",
      "import pandas as pd\n",
      "pd.DataFrame (history.history).plot(figsize=(8, 5))\n",
      "plt.grid(True)\n",
      "plt.gca().set_ylim (0, 1) # set the vertical range to [0-1]\n",
      "plt.show()\n",
      "Figure 10-12. Learning Curves\n",
      "Y ou can see that both the training and validation accuracy steadily increase during\n",
      "training, while the training and validation loss decrease. Good! Moreover, the valida‐\n",
      "tion curves are quite close to the training curves, which means that there is not too\n",
      "much overfitting. In this particular case, the model performed better on the valida‐\n",
      "tion set than on the training set at the beginning of training: this sometimes happens\n",
      "by chance (especially when the validation set is fairly small). However, the training set\n",
      "performance ends up beating the validation performance, as is generally the case\n",
      "when you train for long enough. Y ou can tell that the model has not quite converged\n",
      "yet, as the validation loss is still going down, so you should probably continue train‐\n",
      "ing. It’s as simple as calling the fit()  method again, since Keras just continues train‐\n",
      "ing where it left off (you should be able to reach close to 89% validation accuracy).\n",
      "If you are not satisfied with the performance of your model, you should go back and\n",
      "tune the model’s hyperparameters, for example the number of layers, the number of\n",
      "neurons per layer, the types of activation functions we use for each hidden layer, the\n",
      "Implementing MLPs with Keras | 301\n",
      "number of training epochs, the batch size (it can be set in the fit()  method using the\n",
      "batch_size  argument, which defaults to 32). We will get back to hyperparameter\n",
      "tuning at the end of this chapter. Once you are satisfied with your model’s validation\n",
      "accuracy, you should evaluate it on the test set to estimate the generalization error\n",
      "before you deploy the model to production. Y ou can easily do this using the evalu\n",
      "ate()  method (it also supports several other arguments, such as batch_size  or sam\n",
      "ple_weight , please check the documentation for more details):\n",
      ">>> model.evaluate (X_test, y_test)\n",
      "8832/10000 [==========================] - ETA: 0s - loss: 0.4074 - acc: 0.8540\n",
      "[0.40738476498126985, 0.854]\n",
      "As we saw in Chapter 2 , it is common to get slightly lower performance on the test set\n",
      "than on the validation set, because the hyperparameters are tuned on the validation\n",
      "set, not the test set (however, in this example, we did not do any hyperparameter tun‐\n",
      "ing, so the lower accuracy is just bad luck). Remember to resist the temptation to\n",
      "tweak the hyperparameters on the test set, or else your estimate of the generalization\n",
      "error will be too optimistic.\n",
      "Using the Model to Make Predictions\n",
      "Next, we can use the model’s predict()  method to make predictions on new instan‐\n",
      "ces. Since we don’t have actual new instances, we will just use the first 3 instances of\n",
      "the test set:\n",
      ">>> X_new = X_test[:3]\n",
      ">>> y_proba = model.predict(X_new)\n",
      ">>> y_proba.round(2)\n",
      "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.09, 0.  , 0.12, 0.  , 0.79],\n",
      "       [0.  , 0.  , 0.94, 0.  , 0.02, 0.  , 0.04, 0.  , 0.  , 0.  ],\n",
      "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
      "      dtype=float32)\n",
      "As you can see, for each instance the model estimates one probability per class, from\n",
      "class 0 to class 9. For example, for the first image it estimates that the probability of\n",
      "class 9 (ankle boot) is 79%, the probability of class 7 (sneaker) is 12%, the probability\n",
      "of class 5 (sandal) is 9%, and the other classes are negligible. In other words, it\n",
      "“believes” it’s footwear, probably ankle boots, but it’s not entirely sure, it might be\n",
      "sneakers or sandals instead. If you only care about the class with the highest estima‐\n",
      "ted probability (even if that probability is quite low) then you can use the pre\n",
      "dict_classes()  method instead:\n",
      ">>> y_pred = model.predict_classes (X_new)\n",
      ">>> y_pred\n",
      "array([9, 2, 1])\n",
      ">>> np.array(class_names )[y_pred]\n",
      "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')\n",
      "And the classifier actually classified all three images correctly:\n",
      "302 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      ">>> y_new = y_test[:3]\n",
      ">>> y_new\n",
      "array([9, 2, 1])\n",
      "Now you know how to build, train, evaluate and use a classification MLP using the\n",
      "Sequential API. But what about regression?\n",
      "Building a Regression MLP Using the Sequential API\n",
      "Let’s switch to the California housing problem and tackle it using a regression neural\n",
      "network. For simplicity, we will use Scikit-Learn’s fetch_california_housing()\n",
      "function to load the data: this dataset is simpler than the one we used in Chapter 2 ,\n",
      "since it contains only numerical features (there is no ocean_proximity  feature), and\n",
      "there is no missing value. After loading the data, we split it into a training set, a vali‐\n",
      "dation set and a test set, and we scale all the features:\n",
      "from sklearn.datasets  import fetch_california_housing\n",
      "from sklearn.model_selection  import train_test_split\n",
      "from sklearn.preprocessing  import StandardScaler\n",
      "housing = fetch_california_housing ()\n",
      "X_train_full , X_test, y_train_full , y_test = train_test_split (\n",
      "    housing.data, housing.target)\n",
      "X_train, X_valid, y_train, y_valid = train_test_split (\n",
      "    X_train_full , y_train_full )\n",
      "scaler = StandardScaler ()\n",
      "X_train_scaled  = scaler.fit_transform (X_train)\n",
      "X_valid_scaled  = scaler.transform (X_valid)\n",
      "X_test_scaled  = scaler.transform (X_test)\n",
      "Building, training, evaluating and using a regression MLP using the Sequential API to\n",
      "make predictions is quite similar to what we did for classification. The main differ‐\n",
      "ences are the fact that the output layer has a single neuron (since we only want to\n",
      "predict a single value) and uses no activation function, and the loss function is the\n",
      "mean squared error. Since the dataset is quite noisy, we just use a single hidden layer\n",
      "with fewer neurons than before, to avoid overfitting:\n",
      "model = keras.models.Sequential ([\n",
      "    keras.layers.Dense(30, activation =\"relu\", input_shape =X_train.shape[1:]),\n",
      "    keras.layers.Dense(1)\n",
      "])\n",
      "model.compile(loss=\"mean_squared_error\" , optimizer =\"sgd\")\n",
      "history = model.fit(X_train, y_train, epochs=20,\n",
      "                    validation_data =(X_valid, y_valid))\n",
      "mse_test  = model.evaluate (X_test, y_test)\n",
      "X_new = X_test[:3] # pretend these are new instances\n",
      "y_pred = model.predict(X_new)\n",
      "Implementing MLPs with Keras | 303\n",
      "14“Wide & Deep Learning for Recommender Systems, ” Heng-Tze Cheng et al. (2016).As you can see, the Sequential API is quite easy to use. However, although sequential\n",
      "models are extremely common, it is sometimes useful to build neural networks with\n",
      "more complex topologies, or with multiple inputs or outputs. For this purpose, Keras\n",
      "offers the Functional API.\n",
      "Building Complex Models Using the Functional API\n",
      "One example of a non-sequential neural network is a Wide & Deep  neural network.\n",
      "This neural network architecture was introduced in a 2016 paper  by Heng-Tze Cheng\n",
      "et al.14. It connects all or part of the inputs directly to the output layer, as shown in\n",
      "Figure 10-13 . This architecture makes it possible for the neural network to learn both\n",
      "deep patterns (using the deep path) and simple rules (through the short path). In\n",
      "contrast, a regular MLP forces all the data to flow through the full stack of layers, thus\n",
      "simple patterns in the data may end up being distorted by this sequence of transfor‐\n",
      "mations.\n",
      "304 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "Figure 10-13. Wide and Deep Neural Network\n",
      "Let’s build such a neural network to tackle the California housing problem:\n",
      "input = keras.layers.Input(shape=X_train.shape[1:])\n",
      "hidden1 = keras.layers.Dense(30, activation =\"relu\")(input)\n",
      "hidden2 = keras.layers.Dense(30, activation =\"relu\")(hidden1)\n",
      "concat = keras.layers.Concatenate ()[input, hidden2])\n",
      "output = keras.layers.Dense(1)(concat)\n",
      "model = keras.models.Model(inputs=[input], outputs=[output])\n",
      "Let’s go through each line of this code:\n",
      "•First, we need to create an Input  object. This is needed because we may have\n",
      "multiple inputs, as we will see later.\n",
      "•Next, we create a Dense  layer with 30 neurons and using the ReLU activation\n",
      "function. As soon as it is created, notice that we call it like a function, passing it\n",
      "the input. This is why this is called the Functional API. Note that we are just tell‐\n",
      "Implementing MLPs with Keras | 305\n",
      "ing Keras how it should connect the layers together, no actual data is being pro‐\n",
      "cessed yet.\n",
      "•We then create a second hidden layer, and again we use it as a function. Note\n",
      "however that we pass it the output of the first hidden layer.\n",
      "•Next, we create a Concatenate()  layer, and once again we immediately use it like\n",
      "a function, to concatenate the input and the output of the second hidden layer\n",
      "(you may prefer the keras.layers.concatenate()  function, which creates a Con\n",
      "catenate  layer and immediately calls it with the given inputs).\n",
      "•Then we create the output layer, with a single neuron and no activation function,\n",
      "and we call it like a function, passing it the result of the concatenation.\n",
      "•Lastly, we create a Keras Model , specifying which inputs and outputs to use.\n",
      "Once you have built the Keras model, everything is exactly like earlier, so no need to\n",
      "repeat it here: you must compile the model, train it, evaluate it and use it to make\n",
      "predictions.\n",
      "But what if you want to send a subset of the features through the wide path, and a\n",
      "different subset (possibly overlapping) through the deep path (see Figure 10-14 )? In\n",
      "this case, one solution is to use multiple inputs. For example, suppose we want to\n",
      "send 5 features through the deep path (features 0 to 4), and 6 features through the\n",
      "wide path (features 2 to 7):\n",
      "input_A = keras.layers.Input(shape=[5])\n",
      "input_B = keras.layers.Input(shape=[6])\n",
      "hidden1 = keras.layers.Dense(30, activation =\"relu\")(input_B)\n",
      "hidden2 = keras.layers.Dense(30, activation =\"relu\")(hidden1)\n",
      "concat = keras.layers.concatenate ([input_A, hidden2])\n",
      "output = keras.layers.Dense(1)(concat)\n",
      "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])\n",
      "306 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "Figure 10-14. Handling Multiple Inputs\n",
      "The code is self-explanatory. Note that we specified inputs=[input_A, input_B]\n",
      "when creating the model. Now we can compile the model as usual, but when we call\n",
      "the fit()  method, instead of passing a single input matrix X_train , we must pass a\n",
      "pair of matrices (X_train_A, X_train_B) : one per input. The same is true for\n",
      "X_valid , and also for X_test  and X_new  when you call evaluate()  or predict() :\n",
      "model.compile(loss=\"mse\", optimizer =\"sgd\")\n",
      "X_train_A , X_train_B  = X_train[:, :5], X_train[:, 2:]\n",
      "X_valid_A , X_valid_B  = X_valid[:, :5], X_valid[:, 2:]\n",
      "X_test_A , X_test_B  = X_test[:, :5], X_test[:, 2:]\n",
      "X_new_A, X_new_B = X_test_A [:3], X_test_B [:3]\n",
      "history = model.fit((X_train_A , X_train_B ), y_train, epochs=20,\n",
      "                    validation_data =((X_valid_A , X_valid_B ), y_valid))\n",
      "mse_test  = model.evaluate ((X_test_A , X_test_B ), y_test)\n",
      "y_pred = model.predict((X_new_A, X_new_B))\n",
      "Implementing MLPs with Keras | 307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are also many use cases in which you may want to have multiple outputs:\n",
      "•The task may demand it, for example you may want to locate and classify the\n",
      "main object in a picture. This is both a regression task (finding the coordinates of\n",
      "the object’s center, as well as its width and height) and a classification task.\n",
      "•Similarly, you may have multiple independent tasks to perform based on the\n",
      "same data. Sure, you could train one neural network per task, but in many cases\n",
      "you will get better results on all tasks by training a single neural network with\n",
      "one output per task. This is because the neural network can learn features in the\n",
      "data that are useful across tasks.\n",
      "•Another use case is as a regularization technique (i.e., a training constraint whose\n",
      "objective is to reduce overfitting and thus improve the model’s ability to general‐\n",
      "ize). For example, you may want to add some auxiliary outputs in a neural net‐\n",
      "work architecture (see Figure 10-15 ) to ensure that the underlying part of the\n",
      "network learns something useful on its own, without relying on the rest of the\n",
      "network.\n",
      "Figure 10-15. Handling Multiple Outputs – Auxiliary Output for Regularization\n",
      "Adding extra outputs is quite easy: just connect them to the appropriate layers and\n",
      "add them to your model’s list of outputs. For example, the following code builds the\n",
      "network represented in Figure 10-15 :\n",
      "308 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "[...] # Same as above, up to the main output layer\n",
      "output = keras.layers.Dense(1)(concat)\n",
      "aux_output  = keras.layers.Dense(1)(hidden2)\n",
      "model = keras.models.Model(inputs=[input_A, input_B],\n",
      "                           outputs=[output, aux_output ])\n",
      "Each output will need its own loss function, so when we compile the model we\n",
      "should pass a list of losses (if we pass a single loss, Keras will assume that the same\n",
      "loss must be used for all outputs). By default, Keras will compute all these losses and\n",
      "simply add them up to get the final loss used for training. However, we care much\n",
      "more about the main output than about the auxiliary output (as it is just used for reg‐\n",
      "ularization), so we want to give the main output’s loss a much greater weight. Fortu‐\n",
      "nately, it is possible to set all the loss weights when compiling the model:\n",
      "model.compile(loss=[\"mse\", \"mse\"], loss_weights =[0.9, 0.1], optimizer =\"sgd\")\n",
      "Now when we train the model, we need to provide some labels for each output. In\n",
      "this example, the main output and the auxiliary output should try to predict the same\n",
      "thing, so they should use the same labels. So instead of passing y_train , we just need\n",
      "to pass (y_train, y_train)  (and the same goes for y_valid  and y_test ):\n",
      "history = model.fit(\n",
      "    [X_train_A , X_train_B ], [y_train, y_train], epochs=20,\n",
      "    validation_data =([X_valid_A , X_valid_B ], [y_valid, y_valid]))\n",
      "When we evaluate the model, Keras will return the total loss, as well as all the individ‐\n",
      "ual losses:\n",
      "total_loss , main_loss , aux_loss  = model.evaluate (\n",
      "    [X_test_A , X_test_B ], [y_test, y_test])\n",
      "Similarly, the predict()  method will return predictions for each output:\n",
      "y_pred_main , y_pred_aux  = model.predict([X_new_A, X_new_B])\n",
      "As you can see, you can build any sort of architecture you want quite easily with the\n",
      "Functional API. Let’s look at one last way you can build Keras models.\n",
      "Building Dynamic Models Using the Subclassing API\n",
      "Both the Sequential API and the Functional API are declarative: you start by declar‐\n",
      "ing which layers you want to use and how they should be connected, and only then\n",
      "can you start feeding the model some data for training or inference. This has many\n",
      "advantages: the model can easily be saved, cloned, shared, its structure can be dis‐\n",
      "played and analyzed, the framework can infer shapes and check types, so errors can\n",
      "be caught early (i.e., before any data ever goes through the model). It’s also fairly easy\n",
      "to debug, since the whole model is just a static graph of layers. But the flip side is just\n",
      "that: it’s static. Some models involve loops, varying shapes, conditional branching,\n",
      "and other dynamic behaviors. For such cases, or simply if you prefer a more impera‐\n",
      "tive programming style, the Subclassing API is for you.\n",
      "Implementing MLPs with Keras | 309\n",
      "15Keras models have an output  attribute, so we cannot use that name for the main output layer, which is why\n",
      "we renamed it to main_output .Simply subclass the Model  class, create the layers you need in the constructor, and use\n",
      "them to perform the computations you want in the call()  method. For example, cre‐\n",
      "ating an instance of the following WideAndDeepModel  class gives us an equivalent\n",
      "model to the one we just built with the Functional API. Y ou can then compile it, eval‐\n",
      "uate it and use it to make predictions, exactly like we just did.\n",
      "class WideAndDeepModel (keras.models.Model):\n",
      "    def __init__ (self, units=30, activation =\"relu\", **kwargs):\n",
      "        super().__init__ (**kwargs) # handles standard args (e.g., name)\n",
      "        self.hidden1 = keras.layers.Dense(units, activation =activation )\n",
      "        self.hidden2 = keras.layers.Dense(units, activation =activation )\n",
      "        self.main_output  = keras.layers.Dense(1)\n",
      "        self.aux_output  = keras.layers.Dense(1)\n",
      "    def call(self, inputs):\n",
      "        input_A, input_B = inputs\n",
      "        hidden1 = self.hidden1(input_B)\n",
      "        hidden2 = self.hidden2(hidden1)\n",
      "        concat = keras.layers.concatenate ([input_A, hidden2])\n",
      "        main_output  = self.main_output (concat)\n",
      "        aux_output  = self.aux_output (hidden2)\n",
      "        return main_output , aux_output\n",
      "model = WideAndDeepModel ()\n",
      "This example looks very much like the Functional API, except we do not need to cre‐\n",
      "ate the inputs, we just use the input  argument to the call()  method, and we separate\n",
      "the creation of the layers15 in the constructor from their usage in the call()  method.\n",
      "However, the big difference is that you can do pretty much anything you want in the\n",
      "call()  method: for loops, if statements, low-level TensorFlow operations, your\n",
      "imagination is the limit (see Chapter 12 )! This makes it a great API for researchers\n",
      "experimenting with new ideas.\n",
      "However, this extra flexibility comes at a cost: your model’s architecture is hidden\n",
      "within the call()  method, so Keras cannot easily inspect it, it cannot save or clone it,\n",
      "and when you call the summary()  method, you only get a list of layers, without any\n",
      "information on how they are connected to each other. Moreover, Keras cannot check\n",
      "types and shapes ahead of time, and it is easier to make mistakes. So unless you really\n",
      "need that extra flexibility, you should probably stick to the Sequential API or the\n",
      "Functional API.\n",
      "310 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "Keras models can be used just like regular layers, so you can easily\n",
      "compose them to build complex architectures.\n",
      "Now that you know how to build and train neural nets using Keras, you will want to\n",
      "save them!\n",
      "Saving and Restoring a Model\n",
      "Saving a trained Keras model is as simple as it gets:\n",
      "model.save(\"my_keras_model.h5\" )\n",
      "Keras will save both the model’s architecture (including every layer’s hyperparame‐\n",
      "ters) and the value of all the model parameters for every layer (e.g., connection\n",
      "weights and biases), using the HDF5 format. It also saves the optimizer (including its\n",
      "hyperparameters and any state it may have).\n",
      "Y ou will typically have a script that trains a model and saves it, and one or more\n",
      "scripts (or web services) that load the model and use it to make predictions. Loading\n",
      "the model is just as easy:\n",
      "model = keras.models.load_model (\"my_keras_model.h5\" )\n",
      "This will work when using the Sequential API or the Functional\n",
      "API, but unfortunately not when using Model subclassing. How‐\n",
      "ever, you can use save_weights()  and load_weights()  to at least\n",
      "save and restore the model parameters (but you will need to save\n",
      "and restore everything else yourself).\n",
      "But what if training lasts several hours? This is quite common, especially when train‐\n",
      "ing on large datasets. In this case, you should not only save your model at the end of\n",
      "training, but also save checkpoints at regular intervals during training. But how can\n",
      "you tell the fit()  method to save checkpoints? The answer is: using callbacks.\n",
      "Using Callbacks\n",
      "The fit()  method accepts a callbacks  argument that lets you specify a list of objects\n",
      "that Keras will call during training at the start and end of training, at the start and end\n",
      "of each epoch and even before and after processing each batch. For example, the Mod\n",
      "elCheckpoint  callback saves checkpoints of your model at regular intervals during\n",
      "training, by default at the end of each epoch:\n",
      "Implementing MLPs with Keras | 311\n",
      "[...] # build and compile the model\n",
      "checkpoint_cb  = keras.callbacks .ModelCheckpoint (\"my_keras_model.h5\" )\n",
      "history = model.fit(X_train, y_train, epochs=10, callbacks =[checkpoint_cb ])\n",
      "Moreover, if you use a validation set during training, you can set\n",
      "save_best_only=True  when creating the ModelCheckpoint . In this case, it will only\n",
      "save your model when its performance on the validation set is the best so far. This\n",
      "way, you do not need to worry about training for too long and overfitting the training\n",
      "set: simply restore the last model saved after training, and this will be the best model\n",
      "on the validation set. This is a simple way to implement early stopping (introduced in\n",
      "Chapter 4 ):\n",
      "checkpoint_cb  = keras.callbacks .ModelCheckpoint (\"my_keras_model.h5\" ,\n",
      "                                                save_best_only =True)\n",
      "history = model.fit(X_train, y_train, epochs=10,\n",
      "                    validation_data =(X_valid, y_valid),\n",
      "                    callbacks =[checkpoint_cb ])\n",
      "model = keras.models.load_model (\"my_keras_model.h5\" ) # rollback to best model\n",
      "Another way to implement early stopping is to simply use the EarlyStopping  call‐\n",
      "back. It will interrupt training when it measures no progress on the validation set for\n",
      "a number of epochs (defined by the patience  argument), and it will optionally roll\n",
      "back to the best model. Y ou can combine both callbacks to both save checkpoints of\n",
      "your model (in case your computer crashes), and actually interrupt training early\n",
      "when there is no more progress (to avoid wasting time and resources):\n",
      "early_stopping_cb  = keras.callbacks .EarlyStopping (patience =10,\n",
      "                                                  restore_best_weights =True)\n",
      "history = model.fit(X_train, y_train, epochs=100,\n",
      "                    validation_data =(X_valid, y_valid),\n",
      "                    callbacks =[checkpoint_cb , early_stopping_cb ])\n",
      "The number of epochs can be set to a large value since training will stop automati‐\n",
      "cally when there is no more progress. Moreover, there is no need to restore the best\n",
      "model saved in this case since the EarlyStopping  callback will keep track of the best\n",
      "weights and restore them for us at the end of training.\n",
      "There are many other callbacks available in the keras.callbacks\n",
      "package. See https://keras.io/callbacks/ .\n",
      "If you need extra control, you can easily write your own custom callbacks. For exam‐\n",
      "ple, the following custom callback will display the ratio between the validation loss\n",
      "and the training loss during training (e.g., to detect overfitting):\n",
      "312 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "class PrintValTrainRatioCallback (keras.callbacks .Callback ):\n",
      "    def on_epoch_end (self, epoch, logs):\n",
      "        print(\"\\nval/train: {:.2f}\" .format(logs[\"val_loss\" ] / logs[\"loss\"]))\n",
      "As you might expect, you can implement on_train_begin() , on_train_end() ,\n",
      "on_epoch_begin() , on_epoch_begin() , on_batch_end()  and on_batch_end() .\n",
      "Moreover, callbacks can also be used during evaluation and predictions, should you\n",
      "ever need them (e.g., for debugging). In this case, you should implement\n",
      "on_test_begin() , on_test_end() , on_test_batch_begin() , or\n",
      "on_test_batch_end()  (called by evaluate() ), or on_predict_begin() , on_pre\n",
      "dict_end() , on_predict_batch_begin() , or on_predict_batch_end()  (called by\n",
      "predict() ).\n",
      "Now let’s take a look at one more tool you should definitely have in your toolbox\n",
      "when using tf.keras: TensorBoard.\n",
      "Visualization Using TensorBoard\n",
      "TensorBoard is a great interactive visualization tool that you can use to view the\n",
      "learning curves during training, compare learning curves between multiple runs, vis‐\n",
      "ualize the computation graph, analyze training statistics, view images generated by\n",
      "your model, visualize complex multidimensional data projected down to 3D and\n",
      "automatically clustered for you, and more! This tool is installed automatically when\n",
      "you install TensorFlow, so you already have it!\n",
      "To use it, you must modify your program so that it outputs the data you want to visu‐\n",
      "alize to special binary log files called event files. Each binary data record is called a\n",
      "summary . The TensorBoard server will monitor the log directory, and it will automat‐\n",
      "ically pick up the changes and update the visualizations: this allows you to visualize\n",
      "live data (with a short delay), such as the learning curves during training. In general,\n",
      "you want to point the TensorBoard server to a root log directory, and configure your\n",
      "program so that it writes to a different subdirectory every time it runs. This way, the\n",
      "same TensorBoard server instance will allow you to visualize and compare data from\n",
      "multiple runs of your program, without getting everything mixed up.\n",
      "So let’s start by defining the root log directory we will use for our TensorBoard logs,\n",
      "plus a small function that will generate a subdirectory path based on the current date\n",
      "and time, so that it is different at every run. Y ou may want to include extra informa‐\n",
      "tion in the log directory name, such as hyperparameter values that you are testing, to\n",
      "make it easier to know what you are looking at in TensorBoard:\n",
      "root_logdir  = os.path.join(os.curdir, \"my_logs\" )\n",
      "def get_run_logdir ():\n",
      "    import time\n",
      "    run_id = time.strftime (\"run_%Y_%m_%d-%H_%M_%S\")\n",
      "    return os.path.join(root_logdir , run_id)\n",
      "Implementing MLPs with Keras | 313\n",
      "run_logdir  = get_run_logdir () # e.g., './my_logs/run_2019_01_16-11_28_43'\n",
      "Next, the good news is that Keras provides a nice TensorBoard  callback:\n",
      "[...] # Build and compile your model\n",
      "tensorboard_cb  = keras.callbacks .TensorBoard (run_logdir )\n",
      "history = model.fit(X_train, y_train, epochs=30,\n",
      "                    validation_data =(X_valid, y_valid),\n",
      "                    callbacks =[tensorboard_cb ])\n",
      "And that’s all there is to it! It could hardly be easier to use. If you run this code, the\n",
      "TensorBoard  callback will take care of creating the log directory for you (along with\n",
      "its parent directories if needed), and during training it will create event files and write\n",
      "summaries to them. After running the program a second time (perhaps changing\n",
      "some hyperparameter value), you will end up with a directory structure similar to\n",
      "this one:\n",
      "my_logs\n",
      "├── run_2019_01_16-16_51_02\n",
      "│   └── events.out.tfevents.1547628669.mycomputer.local.v2\n",
      "└── run_2019_01_16-16_56_50\n",
      "    └── events.out.tfevents.1547629020.mycomputer.local.v2\n",
      "Next you need to start the TensorBoard server. If you installed TensorFlow within a\n",
      "virtualenv, you should activate it. Next, run the following command at the root of the\n",
      "project (or from anywhere else as long as you point to the appropriate log directory).\n",
      "If your shell cannot find the tensorboard  script, then you must update your PATH\n",
      "environment variable so that it contains the directory in which the script was\n",
      "installed (alternatively, you can just replace tensorboard  with python3 -m tensor\n",
      "board.main ).\n",
      "$ tensorboard --logdir =./my_logs --port =6006\n",
      "TensorBoard 2.0.0 at http://mycomputer.local:6006 (Press CTRL+C to quit )\n",
      "Finally, open up a web browser to http://localhost:6006 . Y ou should see TensorBoard’s\n",
      "web interface. Click on the SCALARS tab to view the learning curves (see\n",
      "Figure 10-16 ). Notice that the training loss went down nicely during both runs, but\n",
      "the second run went down much faster. Indeed, we used a larger learning rate by set‐\n",
      "ting optimizer=keras.optimizers.SGD(lr=0.05)  instead of optimizer=\"sgd\" ,\n",
      "which defaults to a learning rate of 0.001.\n",
      "314 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "Figure 10-16. Visualizing Learning Curves with TensorBoard\n",
      "Unfortunately, at the time of writing, no other data is exported by the TensorBoard\n",
      "callback, but this issue will probably be fixed by the time you read these lines. In Ten‐\n",
      "sorFlow 1, this callback exported the computation graph and many useful statistics:\n",
      "type help(keras.callbacks.TensorBoard)  to see all the options.\n",
      "Let’s summarize what you learned so far in this chapter: we saw where neural nets\n",
      "came from, what an MLP is and how you can use it for classification and regression,\n",
      "how to build MLPs using tf.keras’s Sequential API, or more complex architectures\n",
      "using the Functional API or Model  Subclassing, you learned how to save and restore a\n",
      "model, use callbacks for checkpointing, early stopping, and more, and finally how to\n",
      "use TensorBoard for visualization. Y ou can already go ahead and use neural networks\n",
      "to tackle many problems! However, you may wonder how to choose the number of\n",
      "hidden layers, the number of neurons in the network, and all the other hyperparame‐\n",
      "ters. Let’s look at this now.\n",
      "Fine-Tuning Neural Network Hyperparameters\n",
      "The flexibility of neural networks is also one of their main drawbacks: there are many\n",
      "hyperparameters to tweak. Not only can you use any imaginable network architec‐\n",
      "ture, but even in a simple MLP you can change the number of layers, the number of\n",
      "neurons per layer, the type of activation function to use in each layer, the weight initi‐\n",
      "Fine-Tuning Neural Network Hyperparameters | 315\n",
      "alization logic, and much more. How do you know what combination of hyperpara‐\n",
      "meters is the best for your task?\n",
      "One option is to simply try many combinations of hyperparameters and see which\n",
      "one works best on the validation set (or using K-fold cross-validation). For this, one\n",
      "approach is simply use GridSearchCV  or RandomizedSearchCV  to explore the hyper‐\n",
      "parameter space, as we did in Chapter 2 . For this, we need to wrap our Keras models\n",
      "in objects that mimic regular Scikit-Learn regressors. The first step is to create a func‐\n",
      "tion that will build and compile a Keras model, given a set of hyperparameters:\n",
      "def build_model (n_hidden =1, n_neurons =30, learning_rate =3e-3, input_shape =[8]):\n",
      "    model = keras.models.Sequential ()\n",
      "    options = {\"input_shape\" : input_shape }\n",
      "    for layer in range(n_hidden ):\n",
      "        model.add(keras.layers.Dense(n_neurons , activation =\"relu\", **options))\n",
      "        options = {}\n",
      "    model.add(keras.layers.Dense(1, **options))\n",
      "    optimizer  = keras.optimizers .SGD(learning_rate )\n",
      "    model.compile(loss=\"mse\", optimizer =optimizer )\n",
      "    return model\n",
      "This function creates a simple Sequential  model for univariate regression (only one\n",
      "output neuron), with the given input shape and the given number of hidden layers\n",
      "and neurons, and it compiles it using an SGD optimizer configured with the given\n",
      "learning rate. The options  dict is used to ensure that the first layer is properly given\n",
      "the input shape (note that if n_hidden=0 , the first layer will be the output layer). It is\n",
      "good practice to provide reasonable defaults to as many hyperparameters as you can,\n",
      "as Scikit-Learn does.\n",
      "Next, let’s create a KerasRegressor  based on this build_model()  function:\n",
      "keras_reg  = keras.wrappers .scikit_learn .KerasRegressor (build_model )\n",
      "The KerasRegressor  object is a thin wrapper around the Keras model built using\n",
      "build_model() . Since we did not specify any hyperparameter when creating it, it will\n",
      "just use the default hyperparameters we defined in build_model() . Now we can use\n",
      "this object like a regular Scikit-Learn regressor: we can train it using its fit()\n",
      "method, then evaluate it using its score()  method, and use it to make predictions\n",
      "using its predict()  method. Note that any extra parameter you pass to the fit()\n",
      "method will simply get passed to the underlying Keras model. Also note that the\n",
      "score will be the opposite of the MSE because Scikit-Learn wants scores, not losses\n",
      "(i.e., higher should be better).\n",
      "keras_reg .fit(X_train, y_train, epochs=100,\n",
      "              validation_data =(X_valid, y_valid),\n",
      "              callbacks =[keras.callbacks .EarlyStopping (patience =10)])\n",
      "mse_test  = keras_reg .score(X_test, y_test)\n",
      "y_pred = keras_reg .predict(X_new)\n",
      "316 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "However, we do not actually want to train and evaluate a single model like this, we\n",
      "want to train hundreds of variants and see which one performs best on the validation\n",
      "set. Since there are many hyperparameters, it is preferable to use a randomized search\n",
      "rather than grid search (as we discussed in Chapter 2 ). Let’s try to explore the number\n",
      "of hidden layers, the number of neurons and the learning rate:\n",
      "from scipy.stats  import reciprocal\n",
      "from sklearn.model_selection  import RandomizedSearchCV\n",
      "param_distribs  = {\n",
      "    \"n_hidden\" : [0, 1, 2, 3],\n",
      "    \"n_neurons\" : np.arange(1, 100),\n",
      "    \"learning_rate\" : reciprocal (3e-4, 3e-2),\n",
      "}\n",
      "rnd_search_cv  = RandomizedSearchCV (keras_reg , param_distribs , n_iter=10, cv=3)\n",
      "rnd_search_cv .fit(X_train, y_train, epochs=100,\n",
      "                  validation_data =(X_valid, y_valid),\n",
      "                  callbacks =[keras.callbacks .EarlyStopping (patience =10)])\n",
      "As you can see, this is identical to what we did in Chapter 2 , with the exception that\n",
      "we pass extra parameters to the fit()  method: they simply get relayed to the under‐\n",
      "lying Keras models. Note that RandomizedSearchCV  uses K-fold cross-validation, so it\n",
      "does not use X_valid  and y_valid . These are just used for early stopping.\n",
      "The exploration may last many hours depending on the hardware, the size of the\n",
      "dataset, the complexity of the model and the value of n_iter  and cv. When it is over,\n",
      "you can access the best parameters found, the best score, and the trained Keras model\n",
      "like this:\n",
      ">>> rnd_search_cv .best_params_\n",
      "{'learning_rate': 0.0033625641252688094, 'n_hidden': 2, 'n_neurons': 42}\n",
      ">>> rnd_search_cv .best_score_\n",
      "-0.3189529188278931\n",
      ">>> model = rnd_search_cv .best_estimator_ .model\n",
      "Y ou can now save this model, evaluate it on the test set, and if you are satisfied with\n",
      "its performance, deploy it to production. Using randomized search is not too hard,\n",
      "and it works well for many fairly simple problems. However, when training is slow\n",
      "(e.g., for more complex problems with larger datasets), this approach will only\n",
      "explore a tiny portion of the hyperparameter space. Y ou can partially alleviate this\n",
      "problem by assisting the search process manually: first run a quick random search\n",
      "using wide ranges of hyperparameter values, then run another search using smaller\n",
      "ranges of values centered on the best ones found during the first run, and so on. This\n",
      "will hopefully zoom in to a good set of hyperparameters. However, this is very time\n",
      "consuming, and probably not the best use of your time.\n",
      "Fortunately, there are many techniques to explore a search space much more effi‐\n",
      "ciently than randomly. Their core idea is simple: when a region of the space turns out\n",
      "Fine-Tuning Neural Network Hyperparameters | 317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16“Population Based Training of Neural Networks, ” Max Jaderberg et al. (2017).to be good, it should be explored more. This takes care of the “zooming” process for\n",
      "you and leads to much better solutions in much less time. Here are a few Python\n",
      "libraries you can use to optimize hyperparameters:\n",
      "•Hyperopt : a popular Python library for optimizing over all sorts of complex\n",
      "search spaces (including real values such as the learning rate, or discrete values\n",
      "such as the number of layers).\n",
      "•Hyperas , kopt  or Talos : optimizing hyperparameters for Keras model (the first\n",
      "two are based on Hyperopt).\n",
      "•Scikit-Optimize  (skopt): a general-purpose optimization library. The Bayes\n",
      "SearchCV  class performs Bayesian optimization using an interface similar to Grid\n",
      "SearchCV .\n",
      "•Spearmint : a Bayesian optimization library.\n",
      "•Sklearn-Deap : a hyperparameter optimization library based on evolutionary\n",
      "algorithms, also with a GridSearchCV -like interface.\n",
      "•And many more!\n",
      "Moreover, many companies offer services for hyperparameter optimization. For\n",
      "example Google Cloud ML Engine has a hyperparameter tuning service . Other com‐\n",
      "panies provide APIs for hyperparameter optimization, such as Arimo , SigOpt , Oscar\n",
      "and many more.\n",
      "Hyperparameter tuning is still an active area of research. Evolutionary algorithms are\n",
      "making a comeback lately. For example, check out DeepMind’s excellent 2017 paper16,\n",
      "where they jointly optimize a population of models and their hyperparameters. Goo‐\n",
      "gle also used an evolutionary approach, not just to search for hyperparameters, but\n",
      "also to look for the best neural network architecture for the problem. They call this\n",
      "AutoML , and it is already available as a cloud service . Perhaps the days of building\n",
      "neural networks manually will soon be over? Check out Google’s post  on this topic. In\n",
      "fact, evolutionary algorithms have also been used successfully to train individual neu‐\n",
      "ral networks, replacing the ubiquitous Gradient Descent! See this 2017 post  by Uber\n",
      "where they introduce their Deep Neuroevolution  technique.\n",
      "Despite all this exciting progress, and all these tools and services, it still helps to have\n",
      "an idea of what values are reasonable for each hyperparameter, so you can build a\n",
      "quick prototype, and restrict the search space. Here are a few guidelines for choosing\n",
      "the number of hidden layers and neurons in an MLP , and selecting good values for\n",
      "some of the main hyperparameters.\n",
      "318 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "Number of Hidden Layers\n",
      "For many problems, you can just begin with a single hidden layer and you will get\n",
      "reasonable results. It has actually been shown that an MLP with just one hidden layer\n",
      "can model even the most complex functions provided it has enough neurons. For a\n",
      "long time, these facts convinced researchers that there was no need to investigate any\n",
      "deeper neural networks. But they overlooked the fact that deep networks have a much\n",
      "higher parameter efficiency  than shallow ones: they can model complex functions\n",
      "using exponentially fewer neurons than shallow nets, allowing them to reach much\n",
      "better performance with the same amount of training data.\n",
      "To understand why, suppose you are asked to draw a forest using some drawing soft‐\n",
      "ware, but you are forbidden to use copy/paste. Y ou would have to draw each tree\n",
      "individually, branch per branch, leaf per leaf. If you could instead draw one leaf,\n",
      "copy/paste it to draw a branch, then copy/paste that branch to create a tree, and\n",
      "finally copy/paste this tree to make a forest, you would be finished in no time. Real-\n",
      "world data is often structured in such a hierarchical way and Deep Neural Networks\n",
      "automatically take advantage of this fact: lower hidden layers model low-level struc‐\n",
      "tures (e.g., line segments of various shapes and orientations), intermediate hidden\n",
      "layers combine these low-level structures to model intermediate-level structures (e.g.,\n",
      "squares, circles), and the highest hidden layers and the output layer combine these\n",
      "intermediate structures to model high-level structures (e.g., faces).\n",
      "Not only does this hierarchical architecture help DNNs converge faster to a good sol‐\n",
      "ution, it also improves their ability to generalize to new datasets. For example, if you\n",
      "have already trained a model to recognize faces in pictures, and you now want to\n",
      "train a new neural network to recognize hairstyles, then you can kickstart training by\n",
      "reusing the lower layers of the first network. Instead of randomly initializing the\n",
      "weights and biases of the first few layers of the new neural network, you can initialize\n",
      "them to the value of the weights and biases of the lower layers of the first network.\n",
      "This way the network will not have to learn from scratch all the low-level structures\n",
      "that occur in most pictures; it will only have to learn the higher-level structures (e.g.,\n",
      "hairstyles). This is called transfer learning .\n",
      "In summary, for many problems you can start with just one or two hidden layers and\n",
      "it will work just fine (e.g., you can easily reach above 97% accuracy on the MNIST\n",
      "dataset using just one hidden layer with a few hundred neurons, and above 98% accu‐\n",
      "racy using two hidden layers with the same total amount of neurons, in roughly the\n",
      "same amount of training time). For more complex problems, you can gradually ramp\n",
      "up the number of hidden layers, until you start overfitting the training set. Very com‐\n",
      "plex tasks, such as large image classification or speech recognition, typically require\n",
      "networks with dozens of layers (or even hundreds, but not fully connected ones, as\n",
      "we will see in Chapter 14 ), and they need a huge amount of training data. However,\n",
      "you will rarely have to train such networks from scratch: it is much more common to\n",
      "Fine-Tuning Neural Network Hyperparameters | 319\n",
      "17By Vincent Vanhoucke in his Deep Learning class  on Udacity.com.reuse parts of a pretrained state-of-the-art network that performs a similar task.\n",
      "Training will be a lot faster and require much less data (we will discuss this in Chap‐\n",
      "ter 11 ).\n",
      "Number of Neurons per Hidden Layer\n",
      "Obviously the number of neurons in the input and output layers is determined by the\n",
      "type of input and output your task requires. For example, the MNIST task requires 28\n",
      "x 28 = 784 input neurons and 10 output neurons.\n",
      "As for the hidden layers, it used to be a common practice to size them to form a pyra‐\n",
      "mid, with fewer and fewer neurons at each layer—the rationale being that many low-\n",
      "level features can coalesce into far fewer high-level features. For example, a typical\n",
      "neural network for MNIST may have three hidden layers, the first with 300 neurons,\n",
      "the second with 200, and the third with 100. However, this practice has been largely\n",
      "abandoned now, as it seems that simply using the same number of neurons in all hid‐\n",
      "den layers performs just as well in most cases, or even better, and there is just one\n",
      "hyperparameter to tune instead of one per layer—for example, all hidden layers could\n",
      "simply have 150 neurons. However, depending on the dataset, it can sometimes help\n",
      "to make the first hidden layer bigger than the others.\n",
      "Just like for the number of layers, you can try increasing the number of neurons grad‐\n",
      "ually until the network starts overfitting. In general you will get more bang for the\n",
      "buck by increasing the number of layers than the number of neurons per layer.\n",
      "Unfortunately, as you can see, finding the perfect amount of neurons is still somewhat\n",
      "of a dark art.\n",
      "A simpler approach is to pick a model with more layers and neurons than you\n",
      "actually need, then use early stopping to prevent it from overfitting (and other regu‐\n",
      "larization techniques, such as dropout , as we will see in Chapter 11 ). This has been\n",
      "dubbed the “stretch pants” approach:17 instead of wasting time looking for pants that\n",
      "perfectly match your size, just use large stretch pants that will shrink down to the\n",
      "right size.\n",
      "Learning Rate, Batch Size and Other Hyperparameters\n",
      "The number of hidden layers and neurons are not the only hyperparameters you can\n",
      "tweak in an MLP . Here are some of the most important ones, and some tips on how\n",
      "to set them:\n",
      "•The learning rate is arguably the most important hyperparameter. In general, the\n",
      "optimal learning rate is about half of the maximum learning rate (i.e., the learn‐\n",
      "320 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "18“Practical recommendations for gradient-based training of deep architectures, ” Y oshua Bengio (2012).ing rate above which the training algorithm diverges, as we saw in Chapter 4 ). So\n",
      "a simple approach for tuning the learning rate is to start with a large value that\n",
      "makes the training algorithm diverge, then divide this value by 3 and try again,\n",
      "and repeat until the training algorithm stops diverging. At that point, you gener‐\n",
      "ally won’t be too far from the optimal learning rate. That said, it is sometimes\n",
      "useful to reduce the learning rate during training: we will discuss this in Chap‐\n",
      "ter 11 .\n",
      "•Choosing a better optimizer than plain old Mini-batch Gradient Descent (and\n",
      "tuning its hyperparameters) is also quite important. We will discuss this in Chap‐\n",
      "ter 11 .\n",
      "•The batch size can also have a significant impact on your model’s performance\n",
      "and the training time. In general the optimal batch size will be lower than 32 (in\n",
      "April 2018, Y ann Lecun even tweeted \" Friends don’t let friends use mini-batches\n",
      "larger than 32 “). A small batch size ensures that each training iteration is very\n",
      "fast, and although a large batch size will give a more precise estimate of the gradi‐\n",
      "ents, in practice this does not matter much since the optimization landscape is\n",
      "quite complex and the direction of the true gradients do not point precisely in\n",
      "the direction of the optimum. However, having a batch size greater than 10 helps\n",
      "take advantage of hardware and software optimizations, in particular for matrix\n",
      "multiplications, so it will speed up training. Moreover, if you use Batch Normal‐\n",
      "ization  (see Chapter 11 ), the batch size should not be too small (in general no less\n",
      "than 20).\n",
      "•We discussed the choice of the activation function earlier in this chapter: in gen‐\n",
      "eral, the ReLU activation function will be a good default for all hidden layers. For\n",
      "the output layer, it really depends on your task.\n",
      "•In most cases, the number of training iterations does not actually need to be\n",
      "tweaked: just use early stopping instead.\n",
      "For more best practices, make sure to read Y oshua Bengio’s great 2012 paper18, which\n",
      "presents many practical recommendations for deep networks.\n",
      "This concludes this introduction to artificial neural networks and their implementa‐\n",
      "tion with Keras. In the next few chapters, we will discuss techniques to train very\n",
      "deep nets, we will see how to customize your models using TensorFlow’s lower-level\n",
      "API and how to load and preprocess data efficiently using the Data API, and we will\n",
      "dive into other popular neural network architectures: convolutional neural networks\n",
      "for image processing, recurrent neural networks for sequential data, autoencoders for\n",
      "Fine-Tuning Neural Network Hyperparameters | 321\n",
      "19A few extra ANN architectures are presented in ???.representation learning, and generative adversarial networks to model and generate\n",
      "data.19\n",
      "Exercises\n",
      "1.Visit the TensorFlow Playground at https://playground.tensorflow.org/\n",
      "•Layers and patterns: try training the default neural network by clicking the run\n",
      "button (top left). Notice how it quickly finds a good solution for the classifica‐\n",
      "tion task. Notice that the neurons in the first hidden layer have learned simple\n",
      "patterns, while the neurons in the second hidden layer have learned to com‐\n",
      "bine the simple patterns of the first hidden layer into more complex patterns.\n",
      "In general, the more layers, the more complex the patterns can be.\n",
      "•Activation function: try replacing the Tanh activation function with the ReLU\n",
      "activation function, and train the network again. Notice that it finds a solution\n",
      "even faster, but this time the boundaries are linear. This is due to the shape of\n",
      "the ReLU function.\n",
      "•Local minima: modify the network architecture to have just one hidden layer\n",
      "with three neurons. Train it multiple times (to reset the network weights, click\n",
      "the reset button next to the play button). Notice that the training time varies a\n",
      "lot, and sometimes it even gets stuck in a local minimum.\n",
      "•Too small: now remove one neuron to keep just 2. Notice that the neural net‐\n",
      "work is now incapable of finding a good solution, even if you try multiple\n",
      "times. The model has too few parameters and it systematically underfits the\n",
      "training set.\n",
      "•Large enough: next, set the number of neurons to 8 and train the network sev‐\n",
      "eral times. Notice that it is now consistently fast and never gets stuck. This\n",
      "highlights an important finding in neural network theory: large neural net‐\n",
      "works almost never get stuck in local minima, and even when they do these\n",
      "local optima are almost as good as the global optimum. However, they can still\n",
      "get stuck on long plateaus for a long time.\n",
      "•Deep net and vanishing gradients: now change the dataset to be the spiral (bot‐\n",
      "tom right dataset under “DATA ”). Change the network architecture to have 4\n",
      "hidden layers with 8 neurons each. Notice that training takes much longer, and\n",
      "often gets stuck on plateaus for long periods of time. Also notice that the neu‐\n",
      "rons in the highest layers (i.e. on the right) tend to evolve faster than the neu‐\n",
      "rons in the lowest layers (i.e. on the left). This problem, called the “vanishing\n",
      "gradients” problem, can be alleviated using better weight initialization and\n",
      "322 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "other techniques, better optimizers (such as AdaGrad or Adam), or using\n",
      "Batch Normalization.\n",
      "•More: go ahead and play with the other parameters to get a feel of what they\n",
      "do. In fact, you should definitely play with this UI for at least one hour, it will\n",
      "grow your intuitions about neural networks significantly.\n",
      "2.Draw an ANN using the original artificial neurons (like the ones in Figure 10-3 )\n",
      "that computes A ⊕ B (where ⊕ represents the XOR operation). Hint: A ⊕ B = (A\n",
      "∧ ¬ B) ∨ (¬ A ∧ B).\n",
      "3.Why is it generally preferable to use a Logistic Regression classifier rather than a\n",
      "classical Perceptron (i.e., a single layer of threshold logic units trained using the\n",
      "Perceptron training algorithm)? How can you tweak a Perceptron to make it\n",
      "equivalent to a Logistic Regression classifier?\n",
      "4.Why was the logistic activation function a key ingredient in training the first\n",
      "MLPs?\n",
      "5.Name three popular activation functions. Can you draw them?\n",
      "6.Suppose you have an MLP composed of one input layer with 10 passthrough\n",
      "neurons, followed by one hidden layer with 50 artificial neurons, and finally one\n",
      "output layer with 3 artificial neurons. All artificial neurons use the ReLU activa‐\n",
      "tion function.\n",
      "•What is the shape of the input matrix X?\n",
      "•What about the shape of the hidden layer’s weight vector Wh, and the shape of\n",
      "its bias vector bh?\n",
      "•What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
      "•What is the shape of the network’s output matrix Y?\n",
      "•Write the equation that computes the network’s output matrix Y as a function\n",
      "of X, Wh, bh, Wo and bo.\n",
      "7.How many neurons do you need in the output layer if you want to classify email\n",
      "into spam or ham? What activation function should you use in the output layer?\n",
      "If instead you want to tackle MNIST, how many neurons do you need in the out‐\n",
      "put layer, using what activation function? Answer the same questions for getting\n",
      "your network to predict housing prices as in Chapter 2 .\n",
      "8.What is backpropagation and how does it work? What is the difference between\n",
      "backpropagation and reverse-mode autodiff?\n",
      "9.Can you list all the hyperparameters you can tweak in an MLP? If the MLP over‐\n",
      "fits the training data, how could you tweak these hyperparameters to try to solve\n",
      "the problem?\n",
      "Exercises | 323\n",
      "10.Train a deep MLP on the MNIST dataset and see if you can get over 98% preci‐\n",
      "sion. Try adding all the bells and whistles (i.e., save checkpoints, use early stop‐\n",
      "ping, plot learning curves using TensorBoard, and so on).\n",
      "Solutions to these exercises are available in ???.\n",
      "324 | Chapter 10: Introduction to Artificial  Neural Networks with Keras\n",
      "CHAPTER 11\n",
      "Training Deep Neural Networks\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 11 in the final\n",
      "release of the book.\n",
      "In Chapter 10  we introduced artificial neural networks and trained our first deep\n",
      "neural networks. But they were very shallow nets, with just a few hidden layers. What\n",
      "if you need to tackle a very complex problem, such as detecting hundreds of types of\n",
      "objects in high-resolution images? Y ou may need to train a much deeper DNN, per‐\n",
      "haps with 10 layers or much more, each containing hundreds of neurons, connected\n",
      "by hundreds of thousands of connections. This would not be a walk in the park:\n",
      "•First, you would be faced with the tricky vanishing gradients  problem (or the\n",
      "related exploding gradients  problem) that affects deep neural networks and makes\n",
      "lower layers very hard to train.\n",
      "•Second, you might not have enough training data for such a large network, or it\n",
      "might be too costly to label.\n",
      "•Third, training may be extremely slow.\n",
      "•Fourth, a model with millions of parameters would severely risk overfitting the\n",
      "training set, especially if there are not enough training instances, or they are too\n",
      "noisy.\n",
      "In this chapter, we will go through each of these problems in turn and present techni‐\n",
      "ques to solve them. We will start by explaining the vanishing gradients problem and\n",
      "exploring some of the most popular solutions to this problem. Next, we will look at\n",
      "transfer learning and unsupervised pretraining, which can help you tackle complex\n",
      "325\n",
      "1“Understanding the Difficulty of Training Deep Feedforward Neural Networks, ” X. Glorot, Y Bengio (2010).tasks even when you have little labeled data. Then we will discuss various optimizers\n",
      "that can speed up training large models tremendously compared to plain Gradient\n",
      "Descent. Finally, we will go through a few popular regularization techniques for large\n",
      "neural networks.\n",
      "With these tools, you will be able to train very deep nets: welcome to Deep Learning!\n",
      "Vanishing/Exploding Gradients Problems\n",
      "As we discussed in Chapter 10 , the backpropagation algorithm works by going from\n",
      "the output layer to the input layer, propagating the error gradient on the way. Once\n",
      "the algorithm has computed the gradient of the cost function with regards to each\n",
      "parameter in the network, it uses these gradients to update each parameter with a\n",
      "Gradient Descent step.\n",
      "Unfortunately, gradients often get smaller and smaller as the algorithm progresses\n",
      "down to the lower layers. As a result, the Gradient Descent update leaves the lower\n",
      "layer connection weights virtually unchanged, and training never converges to a good\n",
      "solution. This is called the vanishing gradients  problem. In some cases, the opposite\n",
      "can happen: the gradients can grow bigger and bigger, so many layers get insanely\n",
      "large weight updates and the algorithm diverges. This is the exploding gradients  prob‐\n",
      "lem, which is mostly encountered in recurrent neural networks (see ???). More gener‐\n",
      "ally, deep neural networks suffer from unstable gradients; different layers may learn at\n",
      "widely different speeds.\n",
      "Although this unfortunate behavior has been empirically observed for quite a while\n",
      "(it was one of the reasons why deep neural networks were mostly abandoned for a\n",
      "long time), it is only around 2010 that significant progress was made in understand‐\n",
      "ing it. A paper titled “Understanding the Difficulty of Training Deep Feedforward\n",
      "Neural Networks”  by Xavier Glorot and Y oshua Bengio1 found a few suspects, includ‐\n",
      "ing the combination of the popular logistic sigmoid activation function and the\n",
      "weight initialization technique that was most popular at the time, namely random ini‐\n",
      "tialization using a normal distribution with a mean of 0 and a standard deviation of 1.\n",
      "In short, they showed that with this activation function and this initialization scheme,\n",
      "the variance of the outputs of each layer is much greater than the variance of its\n",
      "inputs. Going forward in the network, the variance keeps increasing after each layer\n",
      "until the activation function saturates at the top layers. This is actually made worse by\n",
      "the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent\n",
      "function has a mean of 0 and behaves slightly better than the logistic function in deep\n",
      "networks).\n",
      "326 | Chapter 11: Training Deep Neural Networks\n",
      "2Here’s an analogy: if you set a microphone amplifier’s knob too close to zero, people won’t hear your voice, but\n",
      "if you set it too close to the max, your voice will be saturated and people won’t understand what you are say‐\n",
      "ing. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come\n",
      "out loud and clear at the end of the chain. Y our voice has to come out of each amplifier at the same amplitude\n",
      "as it came in.Looking at the logistic activation function (see Figure 11-1 ), you can see that when\n",
      "inputs become large (negative or positive), the function saturates at 0 or 1, with a\n",
      "derivative extremely close to 0. Thus when backpropagation kicks in, it has virtually\n",
      "no gradient to propagate back through the network, and what little gradient exists\n",
      "keeps getting diluted as backpropagation progresses down through the top layers, so\n",
      "there is really nothing left for the lower layers.\n",
      "Figure 11-1. Logistic activation function saturation\n",
      "Glorot and He Initialization\n",
      "In their paper, Glorot and Bengio propose a way to significantly alleviate this prob‐\n",
      "lem. We need the signal to flow properly in both directions: in the forward direction\n",
      "when making predictions, and in the reverse direction when backpropagating gradi‐\n",
      "ents. We don’t want the signal to die out, nor do we want it to explode and saturate.\n",
      "For the signal to flow properly, the authors argue that we need the variance of the\n",
      "outputs of each layer to be equal to the variance of its inputs,2 and we also need the\n",
      "gradients to have equal variance before and after flowing through a layer in the\n",
      "reverse direction (please check out the paper if you are interested in the mathematical\n",
      "details). It is actually not possible to guarantee both unless the layer has an equal\n",
      "number of inputs and neurons (these numbers are called the fan-in  and fan-out  of the\n",
      "layer), but they proposed a good compromise that has proven to work very well in\n",
      "practice: the connection weights of each layer must be initialized randomly as\n",
      "Vanishing/Exploding Gradients Problems | 327\n",
      "3Such as “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, ” K.\n",
      "He et al. (2015).described in Equation 11-1 , where f anavg=f anin+f anout/2. This initialization\n",
      "strategy is called Xavier initialization  (after the author’s first name) or Glorot initiali‐\n",
      "zation  (after his last name).\n",
      "Equation 11-1. Glorot initialization (when using the logistic activation function)\n",
      "Normal distribution with mean 0 and variance  σ2=1\n",
      "fanavg\n",
      "Or a uniform distribution between − r and  + r, with  r=3\n",
      "fanavg\n",
      "If you just replace fanavg with fanin in Equation 11-1 , you get an initialization strategy\n",
      "that was actually already proposed by Y ann LeCun in the 1990s, called LeCun initiali‐\n",
      "zation , which was even recommended in the 1998 book Neural Networks: Tricks of the\n",
      "Trade  by Genevieve Orr and Klaus-Robert Müller (Springer). It is equivalent to\n",
      "Glorot initialization when fanin = fanout. It took over a decade for researchers to realize\n",
      "just how important this trick really is. Using Glorot initialization can speed up train‐\n",
      "ing considerably, and it is one of the tricks that led to the current success of Deep\n",
      "Learning.\n",
      "Some papers3 have provided similar strategies for different activation functions.\n",
      "These strategies differ only by the scale of the variance and whether they use fanavg or\n",
      "fanin, as shown in Table 11-1  (for the uniform distribution, just compute r=3σ2).\n",
      "The initialization strategy for the ReLU activation function (and its variants, includ‐\n",
      "ing the ELU activation described shortly) is sometimes called He initialization  (after\n",
      "the last name of its author). The SELU activation function will be explained later in\n",
      "this chapter. It should be used with LeCun initialization (preferably with a normal\n",
      "distribution, as we will see).\n",
      "Table 11-1. Initialization parameters for each type of activation function\n",
      "Initialization Activation functions σ² (Normal)\n",
      "Glorot None, Tanh, Logistic, Softmax 1 / fan avg\n",
      "He ReLU & variants 2 / fan in\n",
      "LeCun SELU 1 / fan in\n",
      "By default, Keras uses Glorot initialization with a uniform distribution. Y ou can\n",
      "change this to He initialization by setting kernel_initializer=\"he_uniform\"  or ker\n",
      "nel_initializer=\"he_normal\"  when creating a layer, like this:\n",
      "328 | Chapter 11: Training Deep Neural Networks\n",
      "4Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: gradient descent\n",
      "may indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron’s\n",
      "inputs is positive again.\n",
      "5“Empirical Evaluation of Rectified Activations in Convolution Network, ” B. Xu et al. (2015).keras.layers.Dense(10, activation =\"relu\", kernel_initializer =\"he_normal\" )\n",
      "If you want He initialization with a uniform distribution, but based on fanavg rather\n",
      "than fanin, you can use the VarianceScaling  initializer like this:\n",
      "he_avg_init  = keras.initializers .VarianceScaling (scale=2., mode='fan_avg' ,\n",
      "                                                 distribution ='uniform' )\n",
      "keras.layers.Dense(10, activation =\"sigmoid\" , kernel_initializer =he_avg_init )\n",
      "Nonsaturating Activation Functions\n",
      "One of the insights in the 2010 paper by Glorot and Bengio was that the vanishing/\n",
      "exploding gradients problems were in part due to a poor choice of activation func‐\n",
      "tion. Until then most people had assumed that if Mother Nature had chosen to use\n",
      "roughly sigmoid activation functions in biological neurons, they must be an excellent\n",
      "choice. But it turns out that other activation functions behave much better in deep\n",
      "neural networks, in particular the ReLU activation function, mostly because it does\n",
      "not saturate for positive values (and also because it is quite fast to compute).\n",
      "Unfortunately, the ReLU activation function is not perfect. It suffers from a problem\n",
      "known as the dying ReLUs : during training, some neurons effectively die, meaning\n",
      "they stop outputting anything other than 0. In some cases, you may find that half of\n",
      "your network’s neurons are dead, especially if you used a large learning rate. A neu‐\n",
      "ron dies when its weights get tweaked in such a way that the weighted sum of its\n",
      "inputs are negative for all instances in the training set. When this happens, it just\n",
      "keeps outputting 0s, and gradient descent does not affect it anymore since the gradi‐\n",
      "ent of the ReLU function is 0 when its input is negative.4\n",
      "To solve this problem, you may want to use a variant of the ReLU function, such as\n",
      "the leaky ReLU . This function is defined as LeakyReLUα(z) = max( αz, z) (see\n",
      "Figure 11-2 ). The hyperparameter α defines how much the function “leaks”: it is the\n",
      "slope of the function for z < 0, and is typically set to 0.01. This small slope ensures\n",
      "that leaky ReLUs never die; they can go into a long coma, but they have a chance to\n",
      "eventually wake up. A 2015 paper5 compared several variants of the ReLU activation\n",
      "function and one of its conclusions was that the leaky variants always outperformed\n",
      "the strict ReLU activation function. In fact, setting α = 0.2 (huge leak) seemed to\n",
      "result in better performance than α = 0.01 (small leak). They also evaluated the\n",
      "randomized leaky ReLU  (RReLU), where α is picked randomly in a given range during\n",
      "training, and it is fixed to an average value during testing. It also performed fairly well\n",
      "and seemed to act as a regularizer (reducing the risk of overfitting the training set).\n",
      "Vanishing/Exploding Gradients Problems | 329\n",
      "6“Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs), ” D. Clevert, T. Unterthiner,\n",
      "S. Hochreiter (2015).Finally, they also evaluated the parametric leaky ReLU  (PReLU), where α is authorized\n",
      "to be learned during training (instead of being a hyperparameter, it becomes a\n",
      "parameter that can be modified by backpropagation like any other parameter). This\n",
      "was reported to strongly outperform ReLU on large image datasets, but on smaller\n",
      "datasets it runs the risk of overfitting the training set.\n",
      "Figure 11-2. Leaky ReLU\n",
      "Last but not least, a 2015 paper  by Djork-Arné Clevert et al.6 proposed a new activa‐\n",
      "tion function called the exponential linear unit  (ELU) that outperformed all the ReLU\n",
      "variants in their experiments: training time was reduced and the neural network per‐\n",
      "formed better on the test set. It is represented in Figure 11-3 , and Equation 11-2\n",
      "shows its definition.\n",
      "Equation 11-2. ELU activation function\n",
      "ELUαz=αexp z− 1 ifz< 0\n",
      "z ifz≥ 0\n",
      "330 | Chapter 11: Training Deep Neural Networks\n",
      "7“Self-Normalizing Neural Networks, \" G. Klambauer, T. Unterthiner and A. Mayr (2017).\n",
      "Figure 11-3. ELU activation function\n",
      "It looks a lot like the ReLU function, with a few major differences:\n",
      "•First it takes on negative values when z < 0, which allows the unit to have an\n",
      "average output closer to 0. This helps alleviate the vanishing gradients problem,\n",
      "as discussed earlier. The hyperparameter α defines the value that the ELU func‐\n",
      "tion approaches when z is a large negative number. It is usually set to 1, but you\n",
      "can tweak it like any other hyperparameter if you want.\n",
      "•Second, it has a nonzero gradient for z < 0, which avoids the dead neurons prob‐\n",
      "lem.\n",
      "•Third, if α is equal to 1 then the function is smooth everywhere, including\n",
      "around z = 0, which helps speed up Gradient Descent, since it does not bounce as\n",
      "much left and right of z = 0.\n",
      "The main drawback of the ELU activation function is that it is slower to compute\n",
      "than the ReLU and its variants (due to the use of the exponential function), but dur‐\n",
      "ing training this is compensated by the faster convergence rate. However, at test time\n",
      "an ELU network will be slower than a ReLU network.\n",
      "Moreover, in a 2017 paper7 by Günter Klambauer et al., called “Self-Normalizing\n",
      "Neural Networks” , the authors showed that if you build a neural network composed\n",
      "exclusively of a stack of dense layers, and if all hidden layers use the SELU activation\n",
      "function (which is just a scaled version of the ELU activation function, as its name\n",
      "suggests), then the network will self-normalize : the output of each layer will tend to\n",
      "preserve mean 0 and standard deviation 1 during training, which solves the vanish‐\n",
      "ing/exploding gradients problem. As a result, this activation function often outper‐\n",
      "Vanishing/Exploding Gradients Problems | 331\n",
      "forms other activation functions very significantly for such neural nets (especially\n",
      "deep ones). However, there are a few conditions for self-normalization to happen:\n",
      "•The input features must be standardized (mean 0 and standard deviation 1).\n",
      "•Every hidden layer’s weights must also be initialized using LeCun normal initiali‐\n",
      "zation. In Keras, this means setting kernel_initializer=\"lecun_normal\" .\n",
      "•The network’s architecture must be sequential. Unfortunately, if you try to use\n",
      "SELU in non-sequential architectures, such as recurrent networks (see ???) or\n",
      "networks with skip connections  (i.e., connections that skip layers, such as in wide\n",
      "& deep nets), self-normalization will not be guaranteed, so SELU will not neces‐\n",
      "sarily outperform other activation functions.\n",
      "•The paper only guarantees self-normalization if all layers are dense. However, in\n",
      "practice the SELU activation function seems to work great with convolutional\n",
      "neural nets as well (see Chapter 14 ).\n",
      "So which activation function should you use for the hidden layers\n",
      "of your deep neural networks? Although your mileage will vary, in\n",
      "general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh\n",
      "> logistic. If the network’s architecture prevents it from self-\n",
      "normalizing, then ELU may perform better than SELU (since SELU\n",
      "is not smooth at z = 0). If you care a lot about runtime latency, then\n",
      "you may prefer leaky ReLU. If you don’t want to tweak yet another\n",
      "hyperparameter, you may just use the default α values used by\n",
      "Keras (e.g., 0.3 for the leaky ReLU). If you have spare time and\n",
      "computing power, you can use cross-validation to evaluate other\n",
      "activation functions, in particular RReLU if your network is over‐\n",
      "fitting, or PReLU if you have a huge training set.\n",
      "To use the leaky ReLU activation function, you must create a LeakyReLU  instance like\n",
      "this:\n",
      "leaky_relu  = keras.layers.LeakyReLU (alpha=0.2)\n",
      "layer = keras.layers.Dense(10, activation =leaky_relu ,\n",
      "                           kernel_initializer =\"he_normal\" )\n",
      "For PReLU, just replace LeakyRelu(alpha=0.2)  with PReLU() . There is currently no\n",
      "official implementation of RReLU in Keras, but you can fairly easily implement your\n",
      "own (see the exercises at the end of Chapter 12 ).\n",
      "For SELU activation, just set activation=\"selu\"  and kernel_initial\n",
      "izer=\"lecun_normal\"  when creating a layer:\n",
      "layer = keras.layers.Dense(10, activation =\"selu\",\n",
      "                           kernel_initializer =\"lecun_normal\" )\n",
      "332 | Chapter 11: Training Deep Neural Networks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26“Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, ” Y . Gal and Z.\n",
      "Ghahramani (2016).\n",
      "27Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian\n",
      "inference in a specific type of probabilistic model called a deep Gaussian Process .\n",
      "If you want to regularize a self-normalizing network based on the\n",
      "SELU activation function (as discussed earlier), you should use\n",
      "AlphaDropout : this is a variant of dropout that preserves the mean\n",
      "and standard deviation of its inputs (it was introduced in the same\n",
      "paper as SELU, as regular dropout would break self-normalization).\n",
      "Monte-Carlo (MC) Dropout\n",
      "In 2016, a paper26 by Y arin Gal and Zoubin Ghahramani added more good reasons to\n",
      "use dropout:\n",
      "•First, the paper establishes a profound connection between dropout networks\n",
      "(i.e., neural networks containing a dropout layer before every weight layer) and\n",
      "approximate Bayesian inference27, giving dropout a solid mathematical justifica‐\n",
      "tion.\n",
      "•Second, they introduce a powerful technique called MC Dropout , which can\n",
      "boost the performance of any trained dropout model, without having to retrain it\n",
      "or even modify it at all!\n",
      "•Moreover, MC Dropout also provides a much better measure of the model’s\n",
      "uncertainty.\n",
      "•Finally, it is also amazingly simple to implement. If this all sounds like a “one\n",
      "weird trick” advertisement, then take a look at the following code. It is the full\n",
      "implementation of MC Dropout , boosting the dropout model we trained earlier,\n",
      "without retraining it:\n",
      "with keras.backend.learning_phase_scope (1): # force training mode = dropout on\n",
      "    y_probas  = np.stack([model.predict(X_test_scaled )\n",
      "                         for sample in range(100)])\n",
      "y_proba = y_probas .mean(axis=0)\n",
      "We first force training mode on, using a learning_phase_scope(1)  context. This\n",
      "turns dropout on within the with  block. Then we make 100 predictions over the test\n",
      "set, and we stack them. Since dropout is on, all predictions will be different. Recall\n",
      "that predict()  returns a matrix with one row per instance, and one column per class.\n",
      "Since there are 10,000 instances in the test set, and 10 classes, this is a matrix of shape\n",
      "[10000, 10]. We stack 100 such matrices, so y_probas  is an array of shape [100, 10000,\n",
      "10]. Once we average over the first dimension ( axis=0 ), we get y_proba , an array of\n",
      "shape [10000, 10], like we would get with a single prediction. That’s all! Averaging\n",
      "360 | Chapter 11: Training Deep Neural Networks\n",
      "over multiple predictions with dropout on gives us a Monte Carlo estimate that is\n",
      "generally more reliable than the result of a single prediction with dropout off. For\n",
      "example, let’s look at the model’s prediction for the first instance in the test set, with\n",
      "dropout off:\n",
      ">>> np.round(model.predict(X_test_scaled [:1]), 2)\n",
      "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
      "      dtype=float32)\n",
      "The model seems almost certain that this image belongs to class 9 (ankle boot).\n",
      "Should you trust it? Is there really so little room for doubt? Compare this with the\n",
      "predictions made when dropout is activated:\n",
      ">>> np.round(y_probas [:, :1], 2)\n",
      "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.17, 0.  , 0.68]],\n",
      "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.2 , 0.  , 0.64]],\n",
      "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.01, 0.  , 0.97]],\n",
      "       [...]\n",
      "This tells a very different story: apparently, when we activate dropout, the model is\n",
      "not sure anymore. It still seems to prefer class 9, but sometimes it hesitates with\n",
      "classes 5 (sandal) and 7 (sneaker), which makes sense given they’re all footwear. Once\n",
      "we average over the first dimension, we get the following MC dropout predictions:\n",
      ">>> np.round(y_proba[:1], 2)\n",
      "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.16, 0.  , 0.62]],\n",
      "      dtype=float32)\n",
      "The model still thinks this image belongs to class 9, but only with a 62% confidence,\n",
      "which seems much more reasonable than 99%. Plus it’s useful to know exactly which\n",
      "other classes it thinks are likely. And you can also take a look at the standard devia‐\n",
      "tion of the probability estimates :\n",
      ">>> y_std = y_probas .std(axis=0)\n",
      ">>> np.round(y_std[:1], 2)\n",
      "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.21, 0.02, 0.32]],\n",
      "      dtype=float32)\n",
      "Apparently there’s quite a lot of variance in the probability estimates: if you were\n",
      "building a risk-sensitive system (e.g., a medical or financial system), you should prob‐\n",
      "ably treat such an uncertain prediction with extreme caution. Y ou definitely would\n",
      "not treat it like a 99% confident prediction. Moreover, the model’s accuracy got a\n",
      "small boost from 86.8 to 86.9:\n",
      ">>> accuracy  = np.sum(y_pred == y_test) / len(y_test)\n",
      ">>> accuracy\n",
      "0.8694\n",
      "Avoiding Overfitting  Through Regularization | 361\n",
      "The number of Monte Carlo samples you use (100 in this example)\n",
      "is a hyperparameter you can tweak. The higher it is, the more accu‐\n",
      "rate the predictions and their uncertainty estimates will be. How‐\n",
      "ever, it you double it, inference time will also be doubled.\n",
      "Moreover, above a certain number of samples, you will notice little\n",
      "improvement. So your job is to find the right tradeoff between\n",
      "latency and accuracy, depending on your application.\n",
      "If your model contains other layers that behave in a special way during training (such\n",
      "as Batch Normalization layers), then you should not force training mode like we just\n",
      "did. Instead, you should replace the Dropout  layers with the following MCDropout\n",
      "class:\n",
      "class MCDropout (keras.layers.Dropout):\n",
      "    def call(self, inputs):\n",
      "        return super().call(inputs, training =True)\n",
      "We just sublass the Dropout  layer and override the call()  method to force its train\n",
      "ing argument to True  (see Chapter 12 ). Similarly, you could define an MCAlphaDrop\n",
      "out class by subclassing AlphaDropout  instead. If you are creating a model from\n",
      "scratch, it’s just a matter of using MCDropout  rather than Dropout . But if you have a\n",
      "model that was already trained using Dropout , you need to create a new model, iden‐\n",
      "tical to the existing model except replacing the Dropout  layers with MCDropout , then\n",
      "copy the existing model’s weights to your new model.\n",
      "In short, MC Dropout is a fantastic technique that boosts dropout models and pro‐\n",
      "vides better uncertainty estimates. And of course, since it is just regular dropout dur‐\n",
      "ing training, it also acts like a regularizer.\n",
      "Max-Norm Regularization\n",
      "Another regularization technique that is quite popular for neural networks is called\n",
      "max-norm regularization : for each neuron, it constrains the weights w of the incom‐\n",
      "ing connections such that ∥ *w* ∥2 ≤ _r_, where r is the max-norm hyperparameter\n",
      "and ∥ · ∥2 is the ℓ2 norm.\n",
      "Max-norm regularization does not add a regularization loss term to the overall loss\n",
      "function. Instead, it is typically implemented by computing ∥w∥2 after each training\n",
      "step and clipping w if needed ( w wr\n",
      "∥w∥2).\n",
      "Reducing r increases the amount of regularization and helps reduce overfitting. Max-\n",
      "norm regularization can also help alleviate the vanishing/exploding gradients prob‐\n",
      "lems (if you are not using Batch Normalization).\n",
      "362 | Chapter 11: Training Deep Neural Networks\n",
      "To implement max-norm regularization in Keras, just set every hidden layer’s ker\n",
      "nel_constraint  argument to a max_norm()  constraint, with the appropriate max\n",
      "value, for example:\n",
      "keras.layers.Dense(100, activation =\"elu\", kernel_initializer =\"he_normal\" ,\n",
      "                   kernel_constraint =keras.constraints .max_norm (1.))\n",
      "After each training iteration, the model’s fit()  method will call the object returned\n",
      "by max_norm() , passing it the layer’s weights and getting clipped weights in return,\n",
      "which then replace the layer’s weights. As we will see in Chapter 12 , you can define\n",
      "your own custom constraint function if you ever need to, and use it as the ker\n",
      "nel_constraint . Y ou can also constrain the bias terms by setting the bias_con\n",
      "straint  argument.\n",
      "The max_norm()  function has an axis  argument that defaults to 0. A Dense  layer usu‐\n",
      "ally has weights of shape [number of inputs, number of neurons], so using axis=0\n",
      "means that the max norm constraint will apply independently to each neuron’s weight\n",
      "vector. If you want to use max-norm with convolutional layers (see Chapter 14 ),\n",
      "make sure to set the max_norm()  constraint’s axis  argument appropriately (usually\n",
      "axis=[0, 1, 2] ).\n",
      "Summary and Practical Guidelines\n",
      "In this chapter, we have covered a wide range of techniques and you may be wonder‐\n",
      "ing which ones you should use. The configuration in Table 11-2  will work fine in\n",
      "most cases, without requiring much hyperparameter tuning.\n",
      "Table 11-2. Default DNN configuration\n",
      "Hyperparameter Default value\n",
      "Kernel initializer: LeCun initialization\n",
      "Activation function: SELU\n",
      "Normalization: None (self-normalization)\n",
      "Regularization: Early stopping\n",
      "Optimizer: Nadam\n",
      "Learning rate schedule: Performance scheduling\n",
      "Don’t forget to standardize the input features! Of course, you should also try to reuse\n",
      "parts of a pretrained neural network if you can find one that solves a similar problem,\n",
      "or use unsupervised pretraining if you have a lot of unlabeled data, or pretraining on\n",
      "an auxiliary task if you have a lot of labeled data for a similar task.\n",
      "The default configuration in Table 11-2  may need to be tweaked:\n",
      "Summary and Practical Guidelines | 363\n",
      "•If your model self-normalizes:\n",
      "—If it overfits the training set, then you should add alpha dropout (and always\n",
      "use early stopping as well). Do not use other regularization methods, or else\n",
      "they would break self-normalization.\n",
      "•If your model cannot self-normalize (e.g., it is a recurrent net or it contains skip\n",
      "connections):\n",
      "—Y ou can try using ELU (or another activation function) instead of SELU, it\n",
      "may perform better. Make sure to change the initialization method accord‐\n",
      "ingly (e.g., He init for ELU or ReLU).\n",
      "—If it is a deep network, you should use Batch Normalization after every hidden\n",
      "layer. If it overfits the training set, you can also try using max-norm or ℓ2 reg‐\n",
      "ularization.\n",
      "•If you need a sparse model, you can use ℓ1 regularization (and optionally zero out\n",
      "the tiny weights after training). If you need an even sparser model, you can try\n",
      "using FTRL instead of Nadam optimization, along with ℓ1 regularization. In any\n",
      "case, this will break self-normalization, so you will need to switch to BN if your\n",
      "model is deep.\n",
      "•If you need a low-latency model (one that performs lightning-fast predictions),\n",
      "you may need to use less layers, avoid Batch Normalization, and possibly replace\n",
      "the SELU activation function with the leaky ReLU. Having a sparse model will\n",
      "also help. Y ou may also want to reduce the float precision from 32-bits to 16-bit\n",
      "(or even 8-bits) (see ???).\n",
      "•If you are building a risk-sensitive application, or inference latency is not very\n",
      "important in your application, you can use MC Dropout to boost performance\n",
      "and get more reliable probability estimates, along with uncertainty estimates.\n",
      "With these guidelines, you are now ready to train very deep nets! I hope you are now\n",
      "convinced that you can go a very long way using just Keras. However, there may\n",
      "come a time when you need to have even more control, for example to write a custom\n",
      "loss function or to tweak the training algorithm. For such cases, you will need to use\n",
      "TensorFlow’s lower-level API, as we will see in the next chapter.\n",
      "Exercises\n",
      "1.Is it okay to initialize all the weights to the same value as long as that value is\n",
      "selected randomly using He initialization?\n",
      "2.Is it okay to initialize the bias terms to 0?\n",
      "3.Name three advantages of the SELU activation function over ReLU.\n",
      "364 | Chapter 11: Training Deep Neural Networks\n",
      "4.In which cases would you want to use each of the following activation functions:\n",
      "SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
      "5.What may happen if you set the momentum  hyperparameter too close to 1 (e.g.,\n",
      "0.99999) when using an SGD optimizer?\n",
      "6.Name three ways you can produce a sparse model.\n",
      "7.Does dropout slow down training? Does it slow down inference (i.e., making\n",
      "predictions on new instances)? What are about MC dropout?\n",
      "8.Deep Learning.\n",
      "a.Build a DNN with five hidden layers of 100 neurons each, He initialization,\n",
      "and the ELU activation function.\n",
      "b.Using Adam optimization and early stopping, try training it on MNIST but\n",
      "only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the\n",
      "next exercise. Y ou will need a softmax output layer with five neurons, and as\n",
      "always make sure to save checkpoints at regular intervals and save the final\n",
      "model so you can reuse it later.\n",
      "c.Tune the hyperparameters using cross-validation and see what precision you\n",
      "can achieve.\n",
      "d.Now try adding Batch Normalization and compare the learning curves: is it\n",
      "converging faster than before? Does it produce a better model?\n",
      "e.Is the model overfitting the training set? Try adding dropout to every layer\n",
      "and try again. Does it help?\n",
      "9.Transfer learning.\n",
      "a.Create a new DNN that reuses all the pretrained hidden layers of the previous\n",
      "model, freezes them, and replaces the softmax output layer with a new one.\n",
      "b.Train this new DNN on digits 5 to 9, using only 100 images per digit, and time\n",
      "how long it takes. Despite this small number of examples, can you achieve\n",
      "high precision?\n",
      "c.Try caching the frozen layers, and train the model again: how much faster is it\n",
      "now?\n",
      "d.Try again reusing just four hidden layers instead of five. Can you achieve a\n",
      "higher precision?\n",
      "e.Now unfreeze the top two hidden layers and continue training: can you get\n",
      "the model to perform even better?\n",
      "10.Pretraining on an auxiliary task.\n",
      "a.In this exercise you will build a DNN that compares two MNIST digit images\n",
      "and predicts whether they represent the same digit or not. Then you will reuse\n",
      "the lower layers of this network to train an MNIST classifier using very little\n",
      "Exercises | 365\n",
      "training data. Start by building two DNNs (let’s call them DNN A and B), both\n",
      "similar to the one you built earlier but without the output layer: each DNN\n",
      "should have five hidden layers of 100 neurons each, He initialization, and ELU\n",
      "activation. Next, add one more hidden layer with 10 units on top of both\n",
      "DNNs. To do this, you should use a keras.layers.Concatenate  layer to con‐\n",
      "catenate the outputs of both DNNs for each instance, then feed the result to\n",
      "the hidden layer. Finally, add an output layer with a single neuron using the\n",
      "logistic activation function.\n",
      "b.Split the MNIST training set in two sets: split #1 should containing 55,000\n",
      "images, and split #2 should contain contain 5,000 images. Create a function\n",
      "that generates a training batch where each instance is a pair of MNIST images\n",
      "picked from split #1. Half of the training instances should be pairs of images\n",
      "that belong to the same class, while the other half should be images from dif‐\n",
      "ferent classes. For each pair, the training label should be 0 if the images are\n",
      "from the same class, or 1 if they are from different classes.\n",
      "c.Train the DNN on this training set. For each image pair, you can simultane‐\n",
      "ously feed the first image to DNN A and the second image to DNN B. The\n",
      "whole network will gradually learn to tell whether two images belong to the\n",
      "same class or not.\n",
      "d.Now create a new DNN by reusing and freezing the hidden layers of DNN A\n",
      "and adding a softmax output layer on top with 10 neurons. Train this network\n",
      "on split #2 and see if you can achieve high performance despite having only\n",
      "500 images per class.\n",
      "Solutions to these exercises are available in ???.\n",
      "366 | Chapter 11: Training Deep Neural Networks\n",
      "CHAPTER 12\n",
      "Custom Models and Training with\n",
      "TensorFlow\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 12 in the final\n",
      "release of the book.\n",
      "So far we have used only TensorFlow’s high level API, tf.keras, but it already got us\n",
      "pretty far: we built various neural network architectures, including regression and\n",
      "classification nets, wide & deep nets and self-normalizing nets, using all sorts of tech‐\n",
      "niques, such as Batch Normalization, dropout, learning rate schedules, and more. In\n",
      "fact, 95% of the use cases you will encounter will not require anything else than\n",
      "tf.keras (and tf.data, see Chapter 13 ). But now it’s time to dive deeper into TensorFlow\n",
      "and take a look at its lower-level Python API . This will be useful when you need extra\n",
      "control, to write custom loss functions, custom metrics, layers, models, initializers,\n",
      "regularizers, weight constraints and more. Y ou may even need to fully control the\n",
      "training loop itself, for example to apply special transformations or constraints to the\n",
      "gradients (beyond just clipping them), or to use multiple optimizers for different\n",
      "parts of the network. We will cover all these cases in this chapter, then we will also\n",
      "look at how you can boost your custom models and training algorithms using Ten‐\n",
      "sorFlow’s automatic graph generation feature. But first, let’s take a quick tour of Ten‐\n",
      "sorFlow.\n",
      "367\n",
      "1TensorFlow also includes another Deep Learning API called the Estimators API , but it is now recommended\n",
      "to use tf.keras instead.\n",
      "TensorFlow 2.0 was released in March 2019, making TensorFlow\n",
      "much easier to use. The first edition of this book used TF 1, while\n",
      "this edition uses TF 2.\n",
      "A Quick Tour of TensorFlow\n",
      "As you know, TensorFlow  is a powerful library for numerical computation, particu‐\n",
      "larly well suited and fine-tuned for large-scale Machine Learning (but you could use\n",
      "it for anything else that requires heavy computations). It was developed by the Google\n",
      "Brain team and it powers many of Google’s large-scale services, such as Google Cloud\n",
      "Speech, Google Photos, and Google Search. It was open sourced in November 2015,\n",
      "and it is now the most popular deep learning library (in terms of citations in papers,\n",
      "adoption in companies, stars on github, etc.): countless projects use TensorFlow for\n",
      "all sorts of Machine Learning tasks, such as image classification, natural language\n",
      "processing (NLP), recommender systems, time series forecasting, and much more.\n",
      "So what does TensorFlow actually offer? Here’s a summary:\n",
      "•Its core is very similar to NumPy, but with GPU support.\n",
      "•It also supports distributed computing (across multiple devices and servers).\n",
      "•It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu‐\n",
      "tations for speed and memory usage: it works by extracting the computation\n",
      "graph  from a Python function, then optimizing it (e.g., by pruning unused nodes)\n",
      "and finally running it efficiently (e.g., by automatically running independent\n",
      "operations in parallel).\n",
      "•Computation graphs can be exported to a portable format, so you can train a\n",
      "TensorFlow model in one environment (e.g., using Python on Linux), and run it\n",
      "in another (e.g., using Java on an Android device).\n",
      "•It implements autodiff (see Chapter 10  and ???), and provides some excellent\n",
      "optimizers, such as RMSProp, Nadam and FTRL (see Chapter 11 ), so you can\n",
      "easily minimize all sorts of loss functions.\n",
      "•TensorFlow offers many more features, built on top of these core features: the\n",
      "most important is of course tf.keras1, but it also has data loading & preprocessing\n",
      "ops (tf.data, tf.io, etc.), image processing ops (tf.image), signal processing ops\n",
      "(tf.signal), and more (see Figure 12-1  for an overview of TensorFlow’s Python\n",
      "API).\n",
      "368 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "2If you ever need to (but you probably won’t), you can write your own operations using the C++ API.\n",
      "3If you are a researcher, you may be eligible to use these TPUs for free, see https://tensorflow.org/tfrc/  for more\n",
      "details.\n",
      "Figure 12-1. TensorFlow’s Python API\n",
      "We will cover many of the packages and functions of the Tensor‐\n",
      "Flow API, but it’s impossible to cover them all so you should really\n",
      "take some time to browse through the API: you will find that it is\n",
      "quite rich and well documented.\n",
      "At the lowest level, each TensorFlow operation is implemented using highly efficient\n",
      "C++ code2. Many operations (or ops for short) have multiple implementations, called\n",
      "kernels : each kernel is dedicated to a specific device type, such as CPUs, GPUs, or\n",
      "even TPUs ( Tensor Processing Units ). As you may know, GPUs can dramatically speed\n",
      "up computations by splitting computations into many smaller chunks and running\n",
      "them in parallel across many GPU threads. TPUs are even faster. Y ou can purchase\n",
      "your own GPU devices (for now, TensorFlow only supports Nvidia cards with CUDA\n",
      "Compute Capability 3.5+), but TPUs are only available on Google Cloud Machine\n",
      "Learning Engine  (see ???).3\n",
      "TensorFlow’s architecture is shown in Figure 12-2 : most of the time your code will\n",
      "use the high level APIs (especially tf.keras and tf.data), but when you need more flexi‐\n",
      "bility you will use the lower level Python API, handling tensors directly. Note that\n",
      "APIs for other languages are also available. In any case, TensorFlow’s execution\n",
      "A Quick Tour of TensorFlow | 369\n",
      "engine will take care of running the operations efficiently, even across multiple devi‐\n",
      "ces and machines if you tell it to.\n",
      "Figure 12-2. TensorFlow’s architecture\n",
      "TensorFlow runs not only on Windows, Linux, and MacOS, but also on mobile devi‐\n",
      "ces (using TensorFlow Lite ), including both iOS and Android (see ???). If you do not\n",
      "want to use the Python API, there are also C++, Java, Go and Swift APIs. There is\n",
      "even a Javascript implementation called TensorFlow.js  that makes it possible to run\n",
      "your models directly in your browser.\n",
      "There’s more to TensorFlow than just the library. TensorFlow is at the center of an\n",
      "extensive ecosystem of libraries. First, there’s TensorBoard for visualization (see\n",
      "Chapter 10 ). Next, there’s TensorFlow Extended (TFX) , which is a set of libraries built\n",
      "by Google to productionize TensorFlow projects: it includes tools for data validation,\n",
      "preprocessing, model analysis and serving (with TF Serving, see ???). Google also\n",
      "launched TensorFlow Hub , a way to easily download and reuse pretrained neural net‐\n",
      "works. Y ou can also get many neural network architectures, some of them pretrained,\n",
      "in TensorFlow’s model garden . Check out the TensorFlow Resources , or https://\n",
      "github.com/jtoy/awesome-tensorflow  for more TensorFlow-based projects. Y ou will\n",
      "find hundreds of TensorFlow projects on GitHub, so it is often easy to find existing\n",
      "code for whatever you are trying to do.\n",
      "More and more ML papers are released along with their implemen‐\n",
      "tation, and sometimes even with pretrained models. Check out\n",
      "https://paperswithcode.com/  to easily find them.\n",
      "370 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "Last but not least, TensorFlow has a dedicated team of passionate and helpful devel‐\n",
      "opers, and a large community contributing to improving it. To ask technical ques‐\n",
      "tions, you should use http://stackoverflow.com/  and tag your question with tensorflow\n",
      "and python . Y ou can file bugs and feature requests through GitHub. For general dis‐\n",
      "cussions, join the Google group .\n",
      "Okay, it’s time to start coding!\n",
      "Using TensorFlow like NumPy\n",
      "TensorFlow’s API revolves around tensors , hence the name Tensor-Flow. A tensor is\n",
      "usually a multidimensional array (exactly like a NumPy ndarray ), but it can also hold\n",
      "a scalar (a simple value, such as 42). These tensors will be important when we create\n",
      "custom cost functions, custom metrics, custom layers and more, so let’s see how to\n",
      "create and manipulate them.\n",
      "Tensors and Operations\n",
      "Y ou can easily create a tensor, using tf.constant() . For example, here is a tensor\n",
      "representing a matrix with two rows and three columns of floats:\n",
      ">>> tf.constant ([[1., 2., 3.], [4., 5., 6.]]) # matrix\n",
      "<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\n",
      "array([[1., 2., 3.],\n",
      "       [4., 5., 6.]], dtype=float32)>\n",
      ">>> tf.constant (42) # scalar\n",
      "<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\n",
      "Just like an ndarray , a tf.Tensor  has a shape and a data type ( dtype ):\n",
      ">>> t = tf.constant ([[1., 2., 3.], [4., 5., 6.]])\n",
      ">>> t.shape\n",
      "TensorShape([2, 3])\n",
      ">>> t.dtype\n",
      "tf.float32\n",
      "Indexing works much like in NumPy:\n",
      ">>> t[:, 1:]\n",
      "<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\n",
      "array([[2., 3.],\n",
      "       [5., 6.]], dtype=float32)>\n",
      ">>> t[..., 1, tf.newaxis]\n",
      "<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\n",
      "array([[2.],\n",
      "       [5.]], dtype=float32)>\n",
      "Most importantly, all sorts of tensor operations are available:\n",
      ">>> t + 10\n",
      "<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\n",
      "Using TensorFlow like NumPy | 371\n",
      "array([[11., 12., 13.],\n",
      "       [14., 15., 16.]], dtype=float32)>\n",
      ">>> tf.square(t)\n",
      "<tf.Tensor: id=20, shape=(2, 3), dtype=float32, numpy=\n",
      "array([[ 1.,  4.,  9.],\n",
      "       [16., 25., 36.]], dtype=float32)>\n",
      ">>> t @ tf.transpose (t)\n",
      "<tf.Tensor: id=24, shape=(2, 2), dtype=float32, numpy=\n",
      "array([[14., 32.],\n",
      "       [32., 77.]], dtype=float32)>\n",
      "Note that writing t + 10  is equivalent to calling tf.add(t, 10)  (indeed, Python calls\n",
      "the magic method t.__add__(10) , which just calls tf.add(t, 10) ). Other operators\n",
      "(like -, *, etc.) are also supported. The @ operator was added in Python 3.5, for matrix\n",
      "multiplication: it is equivalent to calling the tf.matmul()  function.\n",
      "Y ou will find all the basic math operations you need (e.g., tf.add() , tf.multiply() ,\n",
      "tf.square() , tf.exp() , tf.sqrt() …), and more generally most operations that you\n",
      "can find in NumPy (e.g., tf.reshape() , tf.squeeze() , tf.tile() ), but sometimes\n",
      "with a different name (e.g., tf.reduce_mean() , tf.reduce_sum() , tf.reduce_max() ,\n",
      "tf.math.log()  are the equivalent of np.mean() , np.sum() , np.max()  and np.log() ).\n",
      "When the name differs, there is often a good reason for it: for example, in Tensor‐\n",
      "Flow you must write tf.transpose(t) , you cannot just write t.T like in NumPy. The\n",
      "reason is that it does not do exactly the same thing: in TensorFlow, a new tensor is\n",
      "created with its own copy of the transposed data, while in NumPy, t.T is just a trans‐\n",
      "posed view on the same data. Similarly, the tf.reduce_sum()  operation is named this\n",
      "way because its GPU kernel (i.e., GPU implementation) uses a reduce algorithm that\n",
      "does not guarantee the order in which the elements are added: because 32-bit floats\n",
      "have limited precision, this means that the result may change ever so slightly every\n",
      "time you call this operation. The same is true of tf.reduce_mean()  (but of course\n",
      "tf.reduce_max()  is deterministic).\n",
      "372 | Chapter 12: Custom Models and Training with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4A notable exception is tf.math.log()  which is commonly used but there is no tf.log()  alias (as it might be\n",
      "confused with logging).\n",
      "Many functions and classes have aliases. For example, tf.add()\n",
      "and tf.math.add()  are the same function. This allows TensorFlow\n",
      "to have concise names for the most common operations4, while\n",
      "preserving well organized packages.\n",
      "Keras’ Low-Level API\n",
      "The Keras API actually has its own low-level API, located in keras.backend . It\n",
      "includes functions like square() , exp() , sqrt()  and so on. In tf.keras, these func‐\n",
      "tions generally just call the corresponding TensorFlow operations. If you want to\n",
      "write code that will be portable to other Keras implementations, you should use these\n",
      "Keras functions. However, they only cover a subset of all functions available in Ten‐\n",
      "sorFlow, so in this book we will use the TensorFlow operations directly. Here is as\n",
      "simple example using keras.backend , which is commonly named K for short:\n",
      ">>> from tensorflow  import keras\n",
      ">>> K = keras.backend\n",
      ">>> K.square(K.transpose (t)) + 10\n",
      "<tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=\n",
      "array([[11., 26.],\n",
      "       [14., 35.],\n",
      "       [19., 46.]], dtype=float32)>\n",
      "Tensors and NumPy\n",
      "Tensors play nice with NumPy: you can create a tensor from a NumPy array, and vice\n",
      "versa, and you can even apply TensorFlow operations to NumPy arrays and NumPy\n",
      "operations to tensors:\n",
      ">>> a = np.array([2., 4., 5.])\n",
      ">>> tf.constant (a)\n",
      "<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\n",
      ">>> t.numpy() # or np.array(t)\n",
      "array([[1., 2., 3.],\n",
      "       [4., 5., 6.]], dtype=float32)\n",
      ">>> tf.square(a)\n",
      "<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\n",
      ">>> np.square(t)\n",
      "array([[ 1.,  4.,  9.],\n",
      "       [16., 25., 36.]], dtype=float32)\n",
      "Using TensorFlow like NumPy | 373\n",
      "Notice that NumPy uses 64-bit precision by default, while Tensor‐\n",
      "Flow uses 32-bit. This is because 32-bit precision is generally more\n",
      "than enough for neural networks, plus it runs faster and uses less\n",
      "RAM. So when you create a tensor from a NumPy array, make sure\n",
      "to set dtype=tf.float32 .\n",
      "Type Conversions\n",
      "Type conversions can significantly hurt performance, and they can easily go unno‐\n",
      "ticed when they are done automatically. To avoid this, TensorFlow does not perform\n",
      "any type conversions automatically: it just raises an exception if you try to execute an\n",
      "operation on tensors with incompatible types. For example, you cannot add a float\n",
      "tensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\n",
      ">>> tf.constant (2.) + tf.constant (40)\n",
      "Traceback[...]InvalidArgumentError[...]expected to be a float[...]\n",
      ">>> tf.constant (2.) + tf.constant (40., dtype=tf.float64)\n",
      "Traceback[...]InvalidArgumentError[...]expected to be a double[...]\n",
      "This may be a bit annoying at first, but remember that it’s for a good cause! And of\n",
      "course you can use tf.cast()  when you really need to convert types:\n",
      ">>> t2 = tf.constant (40., dtype=tf.float64)\n",
      ">>> tf.constant (2.0) + tf.cast(t2, tf.float32)\n",
      "<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\n",
      "Variables\n",
      "So far, we have used constant tensors: as their name suggests, you cannot modify\n",
      "them. However, the weights in a neural network need to be tweaked by backpropaga‐\n",
      "tion, and other parameters may also need to change over time (e.g., a momentum\n",
      "optimizer keeps track of past gradients). What we need is a tf.Variable :\n",
      ">>> v = tf.Variable ([[1., 2., 3.], [4., 5., 6.]])\n",
      ">>> v\n",
      "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[1., 2., 3.],\n",
      "       [4., 5., 6.]], dtype=float32)>\n",
      "A tf.Variable  acts much like a constant tensor: you can perform the same opera‐\n",
      "tions with it, it plays nicely with NumPy as well, and it is just as picky with types. But\n",
      "it can also be modified in place using the assign()  method (or assign_add()  or\n",
      "assign_sub()  which increment or decrement the variable by the given value). Y ou\n",
      "can also modify individual cells (or slices), using the cell’s (or slice’s) assign()\n",
      "method (direct item assignment will not work), or using the scatter_update()  or\n",
      "scatter_nd_update()  methods:\n",
      "v.assign(2 * v)           # => [[2., 4., 6.], [8., 10., 12.]]\n",
      "v[0, 1].assign(42)        # => [[2., 42., 6.], [8., 10., 12.]]\n",
      "374 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "v[:, 2].assign([0., 1.])  # => [[2., 42., 0.], [8., 10., 1.]]\n",
      "v.scatter_nd_update (indices=[[0, 0], [1, 2]], updates=[100., 200.])\n",
      "                          # => [[100., 42., 0.], [8., 10., 200.]]\n",
      "In practice you will rarely have to create variables manually, since\n",
      "Keras provides an add_weight()  method that will take care of it for\n",
      "you, as we will see. Moreover, model parameters will generally be\n",
      "updated directly by the optimizers, so you will rarely need to\n",
      "update variables manually.\n",
      "Other Data Structures\n",
      "TensorFlow supports several other data structures, including the following (please see\n",
      "the notebook or ??? for more details):\n",
      "•Sparse tensors  (tf.SparseTensor ) efficiently represent tensors containing mostly\n",
      "0s. The tf.sparse  package contains operations for sparse tensors.\n",
      "•Tensor arrays  (tf.TensorArray ) are lists of tensors. They have a fixed size by\n",
      "default, but can optionally be made dynamic. All tensors they contain must have\n",
      "the same shape and data type.\n",
      "•Ragged tensors  (tf.RaggedTensor ) represent static lists of lists of tensors, where\n",
      "every tensor has the same shape and data type. The tf.ragged  package contains\n",
      "operations for ragged tensors.\n",
      "•String tensors  are regular tensors of type tf.string . These actually represent byte\n",
      "strings, not Unicode strings, so if you create a string tensor using a Unicode\n",
      "string (e.g., a regular Python 3 string like \"café\"` ), then it will get encoded to\n",
      "UTF-8 automatically (e.g., b\"caf\\xc3\\xa9\" ). Alternatively, you can represent\n",
      "Unicode strings using tensors of type tf.int32 , where each item represents a\n",
      "Unicode codepoint (e.g., [99, 97, 102, 233] ). The tf.strings  package (with\n",
      "an s) contains ops for byte strings and Unicode strings (and to convert one into\n",
      "the other).\n",
      "•Sets are just represented as regular tensors (or sparse tensors) containing one or\n",
      "more sets, and you can manipulate them using operations from the tf.sets\n",
      "package.\n",
      "•Queues , including First In, First Out (FIFO) queues ( FIFOQueue ), queues that can\n",
      "prioritize some items ( PriorityQueue ), queues that shuffle their items ( Random\n",
      "ShuffleQueue ), and queues that can batch items of different shapes by padding\n",
      "(PaddingFIFOQueue ). These classes are all in the tf.queue  package.\n",
      "With tensors, operations, variables and various data structures at your disposal, you\n",
      "are now ready to customize your models and training algorithms!\n",
      "Using TensorFlow like NumPy | 375\n",
      "Customizing Models and Training Algorithms\n",
      "Let’s start by creating a custom loss function, which is a simple and common use case.\n",
      "Custom Loss Functions\n",
      "Suppose you want to train a regression model, but your training set is a bit noisy. Of\n",
      "course, you start by trying to clean up your dataset by removing or fixing the outliers,\n",
      "but it turns out to be insufficient, the dataset is still noisy. Which loss function should\n",
      "you use? The mean squared error might penalize large errors too much, so your\n",
      "model will end up being imprecise. The mean absolute error would not penalize out‐\n",
      "liers as much, but training might take a while to converge and the trained model\n",
      "might not be very precise. This is probably a good time to use the Huber loss (intro‐\n",
      "duced in Chapter 10 ) instead of the good old MSE. The Huber loss is not currently\n",
      "part of the official Keras API, but it is available in tf.keras (just use an instance of the\n",
      "keras.losses.Huber  class). But let’s pretend it’s not there: implementing it is easy as\n",
      "pie! Just create a function that takes the labels and predictions as arguments, and use\n",
      "TensorFlow operations to compute every instance’s loss:\n",
      "def huber_fn (y_true, y_pred):\n",
      "    error = y_true - y_pred\n",
      "    is_small_error  = tf.abs(error) < 1\n",
      "    squared_loss  = tf.square(error) / 2\n",
      "    linear_loss   = tf.abs(error) - 0.5\n",
      "    return tf.where(is_small_error , squared_loss , linear_loss )\n",
      "For better performance, you should use a vectorized implementa‐\n",
      "tion, as in this example. Moreover, if you want to benefit from Ten‐\n",
      "sorFlow’s graph features, you should use only TensorFlow\n",
      "operations.\n",
      "It is also preferable to return a tensor containing one loss per instance, rather than\n",
      "returning the mean loss. This way, Keras can apply class weights or sample weights\n",
      "when requested (see Chapter 10 ).\n",
      "Next, you can just use this loss when you compile the Keras model, then train your\n",
      "model:\n",
      "model.compile(loss=huber_fn , optimizer =\"nadam\")\n",
      "model.fit(X_train, y_train, [...])\n",
      "And that’s it! For each batch during training, Keras will call the huber_fn()  function\n",
      "to compute the loss, and use it to perform a Gradient Descent step. Moreover, it will\n",
      "keep track of the total loss since the beginning of the epoch, and it will display the\n",
      "mean loss.\n",
      "376 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "But what happens to this custom loss when we save the model?\n",
      "Saving and Loading Models That Contain Custom Components\n",
      "Saving a model containing a custom loss function actually works fine, as Keras just\n",
      "saves the name of the function. However, whenever you load it, you need to provide a\n",
      "dictionary that maps the function name to the actual function. More generally, when\n",
      "you load a model containing custom objects, you need to map the names to the\n",
      "objects:\n",
      "model = keras.models.load_model (\"my_model_with_a_custom_loss.h5\" ,\n",
      "                                custom_objects ={\"huber_fn\" : huber_fn })\n",
      "With the current implementation, any error between -1 and 1 is considered “small” .\n",
      "But what if we want a different threshold? One solution is to create a function that\n",
      "creates a configured loss function:\n",
      "def create_huber (threshold =1.0):\n",
      "    def huber_fn (y_true, y_pred):\n",
      "        error = y_true - y_pred\n",
      "        is_small_error  = tf.abs(error) < threshold\n",
      "        squared_loss  = tf.square(error) / 2\n",
      "        linear_loss   = threshold  * tf.abs(error) - threshold **2 / 2\n",
      "        return tf.where(is_small_error , squared_loss , linear_loss )\n",
      "    return huber_fn\n",
      "model.compile(loss=create_huber (2.0), optimizer =\"nadam\")\n",
      "Unfortunately, when you save the model, the threshold  will not be saved. This means\n",
      "that you will have to specify the threshold  value when loading the model (note that\n",
      "the name to use is \"huber_fn\" , which is the name of the function we gave Keras, not\n",
      "the name of the function that created it):\n",
      "model = keras.models.load_model (\"my_model_with_a_custom_loss_threshold_2.h5\" ,\n",
      "                                custom_objects ={\"huber_fn\" : create_huber (2.0)})\n",
      "Y ou can solve this by creating a subclass of the keras.losses.Loss  class, and imple‐\n",
      "ment its get_config()  method:\n",
      "class HuberLoss (keras.losses.Loss):\n",
      "    def __init__ (self, threshold =1.0, **kwargs):\n",
      "        self.threshold  = threshold\n",
      "        super().__init__ (**kwargs)\n",
      "    def call(self, y_true, y_pred):\n",
      "        error = y_true - y_pred\n",
      "        is_small_error  = tf.abs(error) < self.threshold\n",
      "        squared_loss  = tf.square(error) / 2\n",
      "        linear_loss   = self.threshold  * tf.abs(error) - self.threshold **2 / 2\n",
      "        return tf.where(is_small_error , squared_loss , linear_loss )\n",
      "    def get_config (self):\n",
      "        base_config  = super().get_config ()\n",
      "        return {**base_config , \"threshold\" : self.threshold }\n",
      "Customizing Models and Training Algorithms | 377\n",
      "5It would not be a good idea to use a weighted mean: if we did, then two instances with the same weight but in\n",
      "different batches would have a different impact on training, depending on the total weight of each batch.\n",
      "The Keras API only specifies how to use subclassing to define lay‐\n",
      "ers, models, callbacks, and regularizers. If you build other compo‐\n",
      "nents (such as losses, metrics, initializers or constraints) using\n",
      "subclassing, they may not be portable to other Keras implementa‐\n",
      "tions.\n",
      "Let’s walk through this code:\n",
      "•The constructor accepts **kwargs  and passes them to the parent constructor,\n",
      "which handles standard hyperparameters: the name  of the loss and the reduction\n",
      "algorithm to use to aggregate the individual instance losses. By default, it is\n",
      "\"sum_over_batch_size\" , which means that the loss will be the sum of the\n",
      "instance losses, possibly weighted by the sample weights, if any, and then divide\n",
      "the result by the batch size (not by the sum of weights, so this is not the weighted\n",
      "mean).5. Other possible values are \"sum\"  and None .\n",
      "•The call()  method takes the labels and predictions, computes all the instance\n",
      "losses, and returns them.\n",
      "•The get_config()  method returns a dictionary mapping each hyperparameter\n",
      "name to its value. It first calls the parent class’s get_config()  method, then adds\n",
      "the new hyperparameters to this dictionary (note that the convenient {**x}  syn‐\n",
      "tax was added in Python 3.5).\n",
      "Y ou can then use any instance of this class when you compile the model:\n",
      "model.compile(loss=HuberLoss (2.), optimizer =\"nadam\")\n",
      "When you save the model, the threshold will be saved along with it, and when you\n",
      "load the model you just need to map the class name to the class itself:\n",
      "model = keras.models.load_model (\"my_model_with_a_custom_loss_class.h5\" ,\n",
      "                                custom_objects ={\"HuberLoss\" : HuberLoss })\n",
      "When you save a model, Keras calls the loss instance’s get_config()  method and\n",
      "saves the config as JSON in the HDF5 file. When you load the model, it calls the\n",
      "from_config()  class method on the HuberLoss  class: this method is implemented by\n",
      "the base class ( Loss ) and just creates an instance of the class, passing **config  to the\n",
      "constructor.\n",
      "That’s it for losses! It was not too hard, was it? Well it’s just as simple for custom acti‐\n",
      "vation functions, initializers, regularizers, and constraints. Let’s look at these now.\n",
      "378 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "Custom Activation Functions, Initializers, Regularizers, and\n",
      "Constraints\n",
      "Most Keras functionalities, such as losses, regularizers, constraints, initializers, met‐\n",
      "rics, activation functions, layers and even full models can be customized in very much\n",
      "the same way. Most of the time, you will just need to write a simple function, with the\n",
      "appropriate inputs and outputs. For example, here are examples of a custom activa‐\n",
      "tion function (equivalent to keras.activations.softplus  or tf.nn.softplus ), a\n",
      "custom Glorot initializer (equivalent to keras.initializers.glorot_normal ), a cus‐\n",
      "tom ℓ1 regularizer (equivalent to keras.regularizers.l1(0.01) ) and a custom con‐\n",
      "straint that ensures weights are all positive (equivalent to\n",
      "keras.constraints.nonneg()  or tf.nn.relu ):\n",
      "def my_softplus (z): # return value is just tf.nn.softplus(z)\n",
      "    return tf.math.log(tf.exp(z) + 1.0)\n",
      "def my_glorot_initializer (shape, dtype=tf.float32):\n",
      "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
      "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
      "def my_l1_regularizer (weights):\n",
      "    return tf.reduce_sum (tf.abs(0.01 * weights))\n",
      "def my_positive_weights (weights): # return value is just tf.nn.relu(weights)\n",
      "    return tf.where(weights < 0., tf.zeros_like (weights), weights)\n",
      "As you can see, the arguments depend on the type of custom function. These custom\n",
      "functions can then be used normally, for example:\n",
      "layer = keras.layers.Dense(30, activation =my_softplus ,\n",
      "                           kernel_initializer =my_glorot_initializer ,\n",
      "                           kernel_regularizer =my_l1_regularizer ,\n",
      "                           kernel_constraint =my_positive_weights )\n",
      "The activation function will be applied to the output of this Dense  layer, and its result\n",
      "will be passed on to the next layer. The layer’s weights will be initialized using the\n",
      "value returned by the initializer. At each training step the weights will be passed to the\n",
      "regularization function to compute the regularization loss, which will be added to the\n",
      "main loss to get the final loss used for training. Finally, the constraint function will be\n",
      "called after each training step, and the layer’s weights will be replaced by the con‐\n",
      "strained weights.\n",
      "If a function has some hyperparameters that need to be saved along with the model,\n",
      "then you will want to subclass the appropriate class, such as keras.regulariz\n",
      "ers.Regularizer , keras.constraints.Constraint , keras.initializers.Initial\n",
      "izer  or keras.layers.Layer  (for any layer, including activation functions). For\n",
      "example, much like we did for the custom loss, here is a simple class for ℓ1 regulariza‐\n",
      "Customizing Models and Training Algorithms | 379\n",
      "6However, the Huber loss is seldom used as a metric (the MAE or MSE are preferred).tion, that saves its factor  hyperparameter (this time we do not need to call the parent\n",
      "constructor or the get_config()  method, as they are not defined by the parent class):\n",
      "class MyL1Regularizer (keras.regularizers .Regularizer ):\n",
      "    def __init__ (self, factor):\n",
      "        self.factor = factor\n",
      "    def __call__ (self, weights):\n",
      "        return tf.reduce_sum (tf.abs(self.factor * weights))\n",
      "    def get_config (self):\n",
      "        return {\"factor\" : self.factor}\n",
      "Note that you must implement the call()  method for losses, layers (including activa‐\n",
      "tion functions) and models, or the __call__()  method for regularizers, initializers\n",
      "and constraints. For metrics, things are a bit different, as we will see now.\n",
      "Custom Metrics\n",
      "Losses and metrics are conceptually not the same thing: losses are used by Gradient\n",
      "Descent to train  a model, so they must be differentiable (at least where they are evalu‐\n",
      "ated) and their gradients should not be 0 everywhere. Plus, it’s okay if they are not\n",
      "easily interpretable by humans (e.g. cross-entropy). In contrast, metrics are used to\n",
      "evaluate  a model, they must be more easily interpretable, and they can be non-\n",
      "differentiable or have 0 gradients everywhere (e.g., accuracy).\n",
      "That said, in most cases, defining a custom metric function is exactly the same as\n",
      "defining a custom loss function. In fact, we could even use the Huber loss function we\n",
      "created earlier as a metric6, it would work just fine (and persistence would also work\n",
      "the same way, in this case only saving the name of the function, \"huber_fn\" ):\n",
      "model.compile(loss=\"mse\", optimizer =\"nadam\", metrics=[create_huber (2.0)])\n",
      "For each batch during training, Keras will compute this metric and keep track of its\n",
      "mean since the beginning of the epoch. Most of the time, this is exactly what you\n",
      "want. But not always! Consider a binary classifier’s precision, for example. As we saw\n",
      "in Chapter 3 , precision is the number of true positives divided by the number of posi‐\n",
      "tive predictions (including both true positives and false positives). Suppose the model\n",
      "made 5 positive predictions in the first batch, 4 of which were correct: that’s 80% pre‐\n",
      "cision. Then suppose the model made 3 positive predictions in the second batch, but\n",
      "they were all incorrect: that’s 0% precision for the second batch. If you just compute\n",
      "the mean of these two precisions, you get 40%. But wait a second, this is not the mod‐\n",
      "el’s precision over these two batches! Indeed, there were a total of 4 true positives (4 +\n",
      "0) out of 8 positive predictions (5 + 3), so the overall precision is 50%, not 40%. What\n",
      "we need is an object that can keep track of the number of true positives and the num‐\n",
      "380 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "ber of false positives, and compute their ratio when requested. This is precisely what\n",
      "the keras.metrics.Precision  class does:\n",
      ">>> precision  = keras.metrics.Precision ()\n",
      ">>> precision ([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])\n",
      "<tf.Tensor: id=581729, shape=(), dtype=float32, numpy=0.8>\n",
      ">>> precision ([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\n",
      "<tf.Tensor: id=581780, shape=(), dtype=float32, numpy=0.5>\n",
      "In this example, we created a Precision  object, then we used it like a function, pass‐\n",
      "ing it the labels and predictions for the first batch, then for the second batch (note\n",
      "that we could also have passed sample weights). We used the same number of true\n",
      "and false positives as in the example we just discussed. After the first batch, it returns\n",
      "the precision of 80%, then after the second batch it returns 50% (which is the overall\n",
      "precision so far, not the second batch’s precision). This is called a streaming metric  (or\n",
      "stateful metric ), as it is gradually updated, batch after batch.\n",
      "At any point, we can call the result()  method to get the current value of the metric.\n",
      "We can also look at its variables (tracking the number of true and false positives)\n",
      "using the variables  attribute, and reset these variables using the reset_states()\n",
      "method:\n",
      ">>> p.result()\n",
      "<tf.Tensor: id=581794, shape=(), dtype=float32, numpy=0.5>\n",
      ">>> p.variables\n",
      "[<tf.Variable 'true_positives:0' [...] numpy=array([4.], dtype=float32)>,\n",
      " <tf.Variable 'false_positives:0' [...] numpy=array([4.], dtype=float32)>]\n",
      ">>> p.reset_states () # both variables get reset to 0.0\n",
      "If you need to create such a streaming metric, you can just create a subclass of the\n",
      "keras.metrics.Metric  class. Here is a simple example that keeps track of the total\n",
      "Huber loss and the number of instances seen so far. When asked for the result, it\n",
      "returns the ratio, which is simply the mean Huber loss:\n",
      "class HuberMetric (keras.metrics.Metric):\n",
      "    def __init__ (self, threshold =1.0, **kwargs):\n",
      "        super().__init__ (**kwargs) # handles base args (e.g., dtype)\n",
      "        self.threshold  = threshold\n",
      "        self.huber_fn  = create_huber (threshold )\n",
      "        self.total = self.add_weight (\"total\", initializer =\"zeros\")\n",
      "        self.count = self.add_weight (\"count\", initializer =\"zeros\")\n",
      "    def update_state (self, y_true, y_pred, sample_weight =None):\n",
      "        metric = self.huber_fn (y_true, y_pred)\n",
      "        self.total.assign_add (tf.reduce_sum (metric))\n",
      "        self.count.assign_add (tf.cast(tf.size(y_true), tf.float32))\n",
      "    def result(self):\n",
      "        return self.total / self.count\n",
      "    def get_config (self):\n",
      "        base_config  = super().get_config ()\n",
      "        return {**base_config , \"threshold\" : self.threshold }\n",
      "Customizing Models and Training Algorithms | 381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7This class is for illustration purposes only. A simpler and better implementation would just subclass the\n",
      "keras.metrics.Mean  class, see the notebook for an example.\n",
      "Let’s walk through this code:7:\n",
      "•The constructor uses the add_weight()  method to create the variables needed to\n",
      "keep track of the metric’s state over multiple batches, in this case the sum of all\n",
      "Huber losses ( total ) and the number of instances seen so far ( count ). Y ou could\n",
      "just create variables manually if you preferred. Keras tracks any tf.Variable  that\n",
      "is set as an attribute (and more generally, any “trackable” object, such as layers or\n",
      "models).\n",
      "•The update_state()  method is called when you use an instance of this class as a\n",
      "function (as we did with the Precision  object). It updates the variables given the\n",
      "labels and predictions for one batch (and sample weights, but in this case we just\n",
      "ignore them).\n",
      "•The result()  method computes and returns the final result, in this case just the\n",
      "mean Huber metric over all instances. When you use the metric as a function, the\n",
      "update_state()  method gets called first, then the result()  method is called,\n",
      "and its output is returned.\n",
      "•We also implement the get_config()  method to ensure the threshold  gets\n",
      "saved along with the model.\n",
      "•The default implementation of the reset_states()  method just resets all vari‐\n",
      "ables to 0.0 (but you can override it if needed).\n",
      "Keras will take care of variable persistence seamlessly, no action is\n",
      "required.\n",
      "When you define a metric using a simple function, Keras automatically calls it for\n",
      "each batch, and it keeps track of the mean during each epoch, just like we did man‐\n",
      "ually. So the only benefit of our HuberMetric  class is that the threshold  will be saved.\n",
      "But of course, some metrics, like precision, cannot simply be averaged over batches:\n",
      "in thoses cases, there’s no other option than to implement a streaming metric.\n",
      "Now that we have built a streaming metric, building a custom layer will seem like a\n",
      "walk in the park!\n",
      "382 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "Custom Layers\n",
      "Y ou may occasionally want to build an architecture that contains an exotic layer for\n",
      "which TensorFlow does not provide a default implementation. In this case, you will\n",
      "need to create a custom layer. Or sometimes you may simply want to build a very\n",
      "repetitive architecture, containing identical blocks of layers repeated many times, and\n",
      "it would be convenient to treat each block of layers as a single layer. For example, if\n",
      "the model is a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to\n",
      "define a custom layer D containing layers A, B, C, and your model would then simply\n",
      "be D, D, D. Let’s see how to build custom layers.\n",
      "First, some layers have no weights, such as keras.layers.Flatten  or keras.lay\n",
      "ers.ReLU . If you want to create a custom layer without any weights, the simplest\n",
      "option is to write a function and wrap it in a keras.layers.Lambda  layer. For exam‐\n",
      "ple, the following layer will apply the exponential function to its inputs:\n",
      "exponential_layer  = keras.layers.Lambda(lambda x: tf.exp(x))\n",
      "This custom layer can then be used like any other layer, using the sequential API, the\n",
      "functional API, or the subclassing API. Y ou can also use it as an activation function\n",
      "(or you could just use activation=tf.exp , or activation=keras.activations.expo\n",
      "nential , or simply activation=\"exponential\" ). The exponential layer is sometimes\n",
      "used in the output layer of a regression model when the values to predict have very\n",
      "different scales (e.g., 0.001, 10., 1000.).\n",
      "As you probably guessed by now, to build a custom stateful layer (i.e., a layer with\n",
      "weights), you need to create a subclass of the keras.layers.Layer  class. For exam‐\n",
      "ple, the following class implements a simplified version of the Dense  layer:\n",
      "class MyDense(keras.layers.Layer):\n",
      "    def __init__ (self, units, activation =None, **kwargs):\n",
      "        super().__init__ (**kwargs)\n",
      "        self.units = units\n",
      "        self.activation  = keras.activations .get(activation )\n",
      "    def build(self, batch_input_shape ):\n",
      "        self.kernel = self.add_weight (\n",
      "            name=\"kernel\" , shape=[batch_input_shape [-1], self.units],\n",
      "            initializer =\"glorot_normal\" )\n",
      "        self.bias = self.add_weight (\n",
      "            name=\"bias\", shape=[self.units], initializer =\"zeros\")\n",
      "        super().build(batch_input_shape ) # must be at the end\n",
      "    def call(self, X):\n",
      "        return self.activation (X @ self.kernel + self.bias)\n",
      "    def compute_output_shape (self, batch_input_shape ):\n",
      "        return tf.TensorShape (batch_input_shape .as_list()[:-1] + [self.units])\n",
      "Customizing Models and Training Algorithms | 383\n",
      "8This function is specific to tf.keras. Y ou could use keras.activations.Activation  instead.\n",
      "9The Keras API calls this argument input_shape , but since it also includes the batch dimension, I prefer to call\n",
      "it batch_input_shape . Same for compute_output_shape() .    def get_config (self):\n",
      "        base_config  = super().get_config ()\n",
      "        return {**base_config , \"units\": self.units,\n",
      "                \"activation\" : keras.activations .serialize (self.activation )}\n",
      "Let’s walk through this code:\n",
      "•The constructor takes all the hyperparameters as arguments (in this example just\n",
      "units  and activation ), and importantly it also takes a **kwargs  argument. It\n",
      "calls the parent constructor, passing it the kwargs : this takes care of standard\n",
      "arguments such as input_shape , trainable , name , and so on. Then it saves the\n",
      "hyperparameters as attributes, converting the activation  argument to the\n",
      "appropriate activation function using the keras.activations.get()  function (it\n",
      "accepts functions, standard strings like \"relu\"  or \"selu\" , or simply None )8.\n",
      "•The build()  method’s role is to create the layer’s variables, by calling the\n",
      "add_weight()  method for each weight. The build()  method is called the first\n",
      "time the layer is used. At that point, Keras will know the shape of this layer’s\n",
      "inputs, and it will pass it to the build()  method9, which is often necessary to cre‐\n",
      "ate some of the weights. For example, we need to know the number of neurons in\n",
      "the previous layer in order to create the connection weights matrix (i.e., the \"ker\n",
      "nel\" ): this corresponds to the size of the last dimension of the inputs. At the end\n",
      "of the build()  method (and only at the end), you must call the parent’s build()\n",
      "method: this tells Keras that the layer is built (it just sets self.built = True ).\n",
      "•The call()  method actually performs the desired operations. In this case, we\n",
      "compute the matrix multiplication of the inputs X and the layer’s kernel, we add\n",
      "the bias vector, we apply the activation function to the result, and this gives us the\n",
      "output of the layer.\n",
      "•The compute_output_shape()  method simply returns the shape of this layer’s\n",
      "outputs. In this case, it is the same shape as the inputs, except the last dimension\n",
      "is replaced with the number of neurons in the layer. Note that in tf.keras, shapes\n",
      "are instances of the tf.TensorShape  class, which you can convert to Python lists\n",
      "using as_list() .\n",
      "•The get_config()  method is just like earlier. Note that we save the activation\n",
      "function’s full configuration by calling keras.activations.serialize() .\n",
      "Y ou can now use a MyDense  layer just like any other layer!\n",
      "384 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "Y ou can generally omit the compute_output_shape()  method, as\n",
      "tf.keras automatically infers the output shape, except when the\n",
      "layer is dynamic (as we will see shortly). In other Keras implemen‐\n",
      "tations, this method is either required or by default it assumes the\n",
      "output shape is the same as the input shape.\n",
      "To create a layer with multiple inputs (e.g., Concatenate ), the argument to the call()\n",
      "method should be a tuple containing all the inputs, and similarly the argument to the\n",
      "compute_output_shape()  method should be a tuple containing each input’s batch\n",
      "shape. To create a layer with multiple outputs, the call()  method should return the\n",
      "list of outputs, and the compute_output_shape()  should return the list of batch out‐\n",
      "put shapes (one per output). For example, the following toy layer takes two inputs\n",
      "and returns three outputs:\n",
      "class MyMultiLayer (keras.layers.Layer):\n",
      "    def call(self, X):\n",
      "        X1, X2 = X\n",
      "        return [X1 + X2, X1 * X2, X1 / X2]\n",
      "    def compute_output_shape (self, batch_input_shape ):\n",
      "        b1, b2 = batch_input_shape\n",
      "        return [b1, b1, b1] # should probably handle broadcasting rules\n",
      "This layer may now be used like any other layer, but of course only using the func‐\n",
      "tional and subclassing APIs, not the sequential API (which only accepts layers with\n",
      "one input and one output).\n",
      "If your layer needs to have a different behavior during training and during testing\n",
      "(e.g., if it uses Dropout  or BatchNormalization  layers), then you must add a train\n",
      "ing argument to the call()  method and use this argument to decide what to do. For\n",
      "example, let’s create a layer that adds Gaussian noise during training (for regulariza‐\n",
      "tion), but does nothing during testing (Keras actually has a layer that does the same\n",
      "thing: keras.layers.GaussianNoise ):\n",
      "class MyGaussianNoise (keras.layers.Layer):\n",
      "    def __init__ (self, stddev, **kwargs):\n",
      "        super().__init__ (**kwargs)\n",
      "        self.stddev = stddev\n",
      "    def call(self, X, training =None):\n",
      "        if training :\n",
      "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
      "            return X + noise\n",
      "        else:\n",
      "            return X\n",
      "    def compute_output_shape (self, batch_input_shape ):\n",
      "        return batch_input_shape\n",
      "Customizing Models and Training Algorithms | 385\n",
      "10The name “subclassing API” usually refers only to the creation of custom models by subclassing, although\n",
      "many other things can be created by subclassing, as we saw in this chapter.With that, you can now build any custom layer you need! Now let’s create custom\n",
      "models.\n",
      "Custom Models\n",
      "We already looked at custom model classes in Chapter 10  when we discussed the sub‐\n",
      "classing API.10 It is actually quite straightforward, just subclass the keras.mod\n",
      "els.Model  class, create layers and variables in the constructor, and implement the\n",
      "call()  method to do whatever you want the model to do. For example, suppose you\n",
      "want to build the model represented in Figure 12-3 :\n",
      "Figure 12-3. Custom Model Example\n",
      "The inputs go through a first dense layer, then through a residual block  composed of\n",
      "two dense layers and an addition operation (as we will see in Chapter 14 , a residual\n",
      "block adds its inputs to its outputs), then through this same residual block 3 more\n",
      "times, then through a second residual block, and the final result goes through a dense\n",
      "output layer. Note that this model does not make much sense, it’s just an example to\n",
      "illustrate the fact that you can easily build any kind of model you want, even contain‐\n",
      "386 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "ing loops and skip connections. To implement this model, it is best to first create a\n",
      "ResidualBlock  layer, since we are going to create a couple identical blocks (and we\n",
      "might want to reuse it in another model):\n",
      "class ResidualBlock (keras.layers.Layer):\n",
      "    def __init__ (self, n_layers , n_neurons , **kwargs):\n",
      "        super().__init__ (**kwargs)\n",
      "        self.hidden = [keras.layers.Dense(n_neurons , activation =\"elu\",\n",
      "                                          kernel_initializer =\"he_normal\" )\n",
      "                       for _ in range(n_layers )]\n",
      "    def call(self, inputs):\n",
      "        Z = inputs\n",
      "        for layer in self.hidden:\n",
      "            Z = layer(Z)\n",
      "        return inputs + Z\n",
      "This layer is a bit special since it contains other layers. This is handled transparently\n",
      "by Keras: it automatically detects that the hidden  attribute contains trackable objects\n",
      "(layers in this case), so their variables are automatically added to this layer’s list of\n",
      "variables. The rest of this class is self-explanatory. Next, let’s use the subclassing API\n",
      "to define the model itself:\n",
      "class ResidualRegressor (keras.models.Model):\n",
      "    def __init__ (self, output_dim , **kwargs):\n",
      "        super().__init__ (**kwargs)\n",
      "        self.hidden1 = keras.layers.Dense(30, activation =\"elu\",\n",
      "                                          kernel_initializer =\"he_normal\" )\n",
      "        self.block1 = ResidualBlock (2, 30)\n",
      "        self.block2 = ResidualBlock (2, 30)\n",
      "        self.out = keras.layers.Dense(output_dim )\n",
      "    def call(self, inputs):\n",
      "        Z = self.hidden1(inputs)\n",
      "        for _ in range(1 + 3):\n",
      "            Z = self.block1(Z)\n",
      "        Z = self.block2(Z)\n",
      "        return self.out(Z)\n",
      "We create the layers in the constructor, and use them in the call()  method. This\n",
      "model can then be used like any other model (compile it, fit it, evaluate it and use it to\n",
      "make predictions). If you also want to be able to save the model using the save()\n",
      "method, and load it using the keras.models.load_model()  function, you must\n",
      "implement the get_config()  method (as we did earlier) in both the ResidualBlock\n",
      "class and the ResidualRegressor  class. Alternatively, you can just save and load the\n",
      "weights using the save_weights()  and load_weights()  methods.\n",
      "The Model  class is actually a subclass of the Layer  class, so models can be defined and\n",
      "used exactly like layers. But a model also has some extra functionalities, including of\n",
      "course its compile() , fit() , evaluate()  and predict()  methods (and a few var‐\n",
      "Customizing Models and Training Algorithms | 387\n",
      "iants, such as train_on_batch()  or fit_generator() ), plus the get_layers()\n",
      "method (which can return any of the model’s layers by name or by index), and the\n",
      "save()  method (and support for keras.models.load_model()  and keras.mod\n",
      "els.clone_model() ). So if models provide more functionalities than layers, why not\n",
      "just define every layer as a model? Well, technically you could, but it is probably\n",
      "cleaner to distinguish the internal components of your model (layers or reusable\n",
      "blocks of layers) from the model itself. The former should subclass the Layer  class,\n",
      "while the latter should subclass the Model  class.\n",
      "With that, you can quite naturally and concisely build almost any model that you find\n",
      "in a paper, either using the sequential API, the functional API, the subclassing API, or\n",
      "even a mix of these. “ Almost” any model? Y es, there are still a couple things that we\n",
      "need to look at: first, how to define losses or metrics based on model internals, and\n",
      "second how to build a custom training loop.\n",
      "Losses and Metrics Based on Model Internals\n",
      "The custom losses and metrics we defined earlier were all based on the labels and the\n",
      "predictions (and optionally sample weights). However, you will occasionally want to\n",
      "define losses based on other parts of your model, such as the weights or activations of\n",
      "its hidden layers. This may be useful for regularization purposes, or to monitor some\n",
      "internal aspect of your model.\n",
      "To define a custom loss based on model internals, just compute it based on any part\n",
      "of the model you want, then pass the result to the add_loss()  method. For example,\n",
      "the following custom model represents a standard MLP regressor with 5 hidden lay‐\n",
      "ers, except it also implements a reconstruction loss  (see ???): we add an extra Dense\n",
      "layer on top of the last hidden layer, and its role is to try to reconstruct the inputs of\n",
      "the model. Since the reconstruction must have the same shape as the model’s inputs,\n",
      "we need to create this Dense  layer in the build()  method to have access to the shape\n",
      "of the inputs. In the call()  method, we compute both the regular output of the MLP ,\n",
      "plus the output of the reconstruction layer. We then compute the mean squared dif‐\n",
      "ference between the reconstructions and the inputs, and we add this value (times\n",
      "0.05) to the model’s list of losses by calling add_loss() . During training, Keras will\n",
      "add this loss to the main loss (which is why we scaled down the reconstruction loss,\n",
      "to ensure the main loss dominates). As a result, the model will be forced to preserve\n",
      "as much information as possible through the hidden layers, even information that is\n",
      "not directly useful for the regression task itself. In practice, this loss sometimes\n",
      "improves generalization; it is a regularization loss:\n",
      "class ReconstructingRegressor (keras.models.Model):\n",
      "    def __init__ (self, output_dim , **kwargs):\n",
      "        super().__init__ (**kwargs)\n",
      "        self.hidden = [keras.layers.Dense(30, activation =\"selu\",\n",
      "                                          kernel_initializer =\"lecun_normal\" )\n",
      "388 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "                       for _ in range(5)]\n",
      "        self.out = keras.layers.Dense(output_dim )\n",
      "    def build(self, batch_input_shape ):\n",
      "        n_inputs  = batch_input_shape [-1]\n",
      "        self.reconstruct  = keras.layers.Dense(n_inputs )\n",
      "        super().build(batch_input_shape )\n",
      "    def call(self, inputs):\n",
      "        Z = inputs\n",
      "        for layer in self.hidden:\n",
      "            Z = layer(Z)\n",
      "        reconstruction  = self.reconstruct (Z)\n",
      "        recon_loss  = tf.reduce_mean (tf.square(reconstruction  - inputs))\n",
      "        self.add_loss (0.05 * recon_loss )\n",
      "        return self.out(Z)\n",
      "Similarly, you can add a custom metric based on model internals by computing it in\n",
      "any way you want, as long at the result is the output of a metric object. For example,\n",
      "you can create a keras.metrics.Mean()  object in the constructor, then call it in the\n",
      "call()  method, passing it the recon_loss , and finally add it to the model by calling\n",
      "the model’s add_metric()  method. This way, when you train the model, Keras will\n",
      "display both the mean loss over each epoch (the loss is the sum of the main loss plus\n",
      "0.05 times the reconstruction loss) and the mean reconstruction error over each\n",
      "epoch. Both will go down during training:\n",
      "Epoch 1/5\n",
      "11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\n",
      "Epoch 2/5\n",
      "11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\n",
      "[...]\n",
      "In over 99% of the cases, everything we have discussed so far will be sufficient to\n",
      "implement whatever model you want to build, even with complex architectures, los‐\n",
      "ses, metrics, and so on. However, in some rare cases you may need to customize the\n",
      "training loop itself. However, before we get there, we need to look at how to compute\n",
      "gradients automatically in TensorFlow.\n",
      "Computing Gradients Using Autodiff\n",
      "To understand how to use autodiff (see Chapter 10  and ???) to compute gradients\n",
      "automatically, let’s consider a simple toy function:\n",
      "def f(w1, w2):\n",
      "    return 3 * w1 ** 2 + 2 * w1 * w2\n",
      "If you know calculus, you can analytically find that the partial derivative of this func‐\n",
      "tion with regards to w1 is 6 * w1  + 2 * w2 . Y ou can also find that its partial derivative\n",
      "with regards to w2 is 2 * w1 . For example, at the point (w1, w2)  = (5, 3) , these par‐\n",
      "Customizing Models and Training Algorithms | 389\n",
      "tial derivatives are equal to 36 and 10, respectively, so the gradient vector at this point\n",
      "is (36, 10). But if this were a neural network, the function would be much more com‐\n",
      "plex, typically with tens of thousands of parameters, and finding the partial deriva‐\n",
      "tives analytically by hand would be an almost impossible task. One solution could be\n",
      "to compute an approximation of each partial derivative by measuring how much the\n",
      "function’s output changes when you tweak the corresponding parameter:\n",
      ">>> w1, w2 = 5, 3\n",
      ">>> eps = 1e-6\n",
      ">>> (f(w1 + eps, w2) - f(w1, w2)) / eps\n",
      "36.000003007075065\n",
      ">>> (f(w1, w2 + eps) - f(w1, w2)) / eps\n",
      "10.000000003174137\n",
      "Looks about right! This works rather well and it is trivial to implement, but it is just\n",
      "an approximation, and importantly you need to call f() at least once per parameter\n",
      "(not twice, since we could compute f(w1, w2)  just once). This makes this approach\n",
      "intractable for large neural networks. So instead we should use autodiff (see Chap‐\n",
      "ter 10  and ???). TensorFlow makes this pretty simple:\n",
      "w1, w2 = tf.Variable (5.), tf.Variable (3.)\n",
      "with tf.GradientTape () as tape:\n",
      "    z = f(w1, w2)\n",
      "gradients  = tape.gradient (z, [w1, w2])\n",
      "We first define two variables w1 and w2, then we create a tf.GradientTape  context\n",
      "that will automatically record every operation that involves a variable, and finally we\n",
      "ask this tape to compute the gradients of the result z with regards to both variables\n",
      "[w1, w2] . Let’s take a look at the gradients that TensorFlow computed:\n",
      ">>> gradients\n",
      "[<tf.Tensor: id=828234, shape=(), dtype=float32, numpy=36.0>,\n",
      " <tf.Tensor: id=828229, shape=(), dtype=float32, numpy=10.0>]\n",
      "Perfect! Not only is the result accurate (the precision is only limited by the floating\n",
      "point errors), but the gradient()  method only goes through the recorded computa‐\n",
      "tions once (in reverse order), no matter how many variables there are, so it is incredi‐\n",
      "bly efficient. It’s like magic!\n",
      "Only put the strict minimum inside the tf.GradientTape()  block,\n",
      "to save memory. Alternatively, you can pause recording by creating\n",
      "a with tape.stop_recording()  block inside the tf.Gradient\n",
      "Tape()  block.\n",
      "The tape is automatically erased immediately after you call its gradient()  method, so\n",
      "you will get an exception if you try to call gradient()  twice:\n",
      "390 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "with tf.GradientTape () as tape:\n",
      "    z = f(w1, w2)\n",
      "dz_dw1 = tape.gradient (z, w1) # => tensor 36.0\n",
      "dz_dw2 = tape.gradient (z, w2) # RuntimeError!\n",
      "If you need to call gradient()  more than once, you must make the tape persistent,\n",
      "and delete it when you are done with it to free resources:\n",
      "with tf.GradientTape (persistent =True) as tape:\n",
      "    z = f(w1, w2)\n",
      "dz_dw1 = tape.gradient (z, w1) # => tensor 36.0\n",
      "dz_dw2 = tape.gradient (z, w2) # => tensor 10.0, works fine now!\n",
      "del tape\n",
      "By default, the tape will only track operations involving variables, so if you try to\n",
      "compute the gradient of z with regards to anything else than a variable, the result will\n",
      "be None :\n",
      "c1, c2 = tf.constant (5.), tf.constant (3.)\n",
      "with tf.GradientTape () as tape:\n",
      "    z = f(c1, c2)\n",
      "gradients  = tape.gradient (z, [c1, c2]) # returns [None, None]\n",
      "However, you can force the tape to watch any tensors you like, to record every opera‐\n",
      "tion that involves them. Y ou can then compute gradients with regards to these ten‐\n",
      "sors, as if they were variables:\n",
      "with tf.GradientTape () as tape:\n",
      "    tape.watch(c1)\n",
      "    tape.watch(c2)\n",
      "    z = f(c1, c2)\n",
      "gradients  = tape.gradient (z, [c1, c2]) # returns [tensor 36., tensor 10.]\n",
      "This can be useful in some cases, for example if you want to implement a regulariza‐\n",
      "tion loss that penalizes activations that vary a lot when the inputs vary little: the loss\n",
      "will be based on the gradient of the activations with regards to the inputs. Since the\n",
      "inputs are not variables, you would need to tell the tape to watch them.\n",
      "If you compute the gradient of a list of tensors (e.g., [z1, z2, z3] ) with regards to\n",
      "some variables (e.g., [w1, w2] ), TensorFlow actually efficiently computes the sum of\n",
      "the gradients of these tensors (i.e., gradient(z1, [w1, w2]) , plus gradient(z2,\n",
      "[w1, w2]) , plus gradient(z3, [w1, w2]) ). Due to the way reverse-mode autodiff\n",
      "works, it is not possible to compute the individual gradients ( z1, z2 and z3) without\n",
      "actually calling gradient()  multiple times (once for z1, once for z2 and once for z3),\n",
      "which requires making the tape persistent (and deleting it afterwards).\n",
      "Customizing Models and Training Algorithms | 391\n",
      "Moreover, it is actually possible to compute second order partial derivatives (the Hes‐\n",
      "sians, i.e., the partial derivatives of the partial derivatives)! To do this, we need to\n",
      "record the operations that are performed when computing the first-order partial\n",
      "derivatives (the Jacobians): this requires a second tape. Here is how it works:\n",
      "with tf.GradientTape (persistent =True) as hessian_tape :\n",
      "    with tf.GradientTape () as jacobian_tape :\n",
      "        z = f(w1, w2)\n",
      "    jacobians  = jacobian_tape .gradient (z, [w1, w2])\n",
      "hessians  = [hessian_tape .gradient (jacobian , [w1, w2])\n",
      "            for jacobian  in jacobians ]\n",
      "del hessian_tape\n",
      "The inner tape is used to compute the Jacobians, as we did earlier. The outer tape is\n",
      "used to compute the partial derivatives of each Jacobian. Since we need to call gradi\n",
      "ent()  once for each Jacobian (or else we would get the sum of the partial derivatives\n",
      "over all the Jabobians, as explained earlier), we need the outer tape to be persistent, so\n",
      "we delete it at the end. The Jacobians are obviously the same as earlier (36 and 5), but\n",
      "now we also have the Hessians:\n",
      ">>> hessians  # dz_dw1_dw1, dz_dw1_dw2, dz_dw2_dw1, dz_dw2_dw2\n",
      "[[<tf.Tensor: id=830578, shape=(), dtype=float32, numpy=6.0>,\n",
      "  <tf.Tensor: id=830595, shape=(), dtype=float32, numpy=2.0>],\n",
      " [<tf.Tensor: id=830600, shape=(), dtype=float32, numpy=2.0>, None]]\n",
      "Let’s verify these Hessians. The first two are the partial derivatives of 6 * w1  + 2 * w2\n",
      "(which is, as we saw earlier, the partial derivative of f with regards to w1), with\n",
      "regards to w1 and w2. The result is correct: 6 for w1 and 2 for w2. The next two are the\n",
      "partial derivatives of 2 * w1  (the partial derivative of f with regards to w2), with\n",
      "regards to w1 and w2, which are 2 for w1 and 0 for w2. Note that TensorFlow returns\n",
      "None  instead of 0 since w2 does not appear at all in 2 * w1 . TensorFlow also returns\n",
      "None  when you use an operation whose gradients are not defined (e.g., tf.argmax() ).\n",
      "In some rare cases you may want to stop gradients from backpropagating through\n",
      "some part of your neural network. To do this, you must use the tf.stop_gradient()\n",
      "function: it just returns its inputs during the forward pass (like tf.identity() ), but\n",
      "it does not let gradients through during backpropagation (it acts like a constant). For\n",
      "example:\n",
      "def f(w1, w2):\n",
      "    return 3 * w1 ** 2 + tf.stop_gradient (2 * w1 * w2)\n",
      "with tf.GradientTape () as tape:\n",
      "    z = f(w1, w2) # same result as without stop_gradient()\n",
      "gradients  = tape.gradient (z, [w1, w2]) # => returns [tensor 30., None]\n",
      "392 | Chapter 12: Custom Models and Training with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finally, you may occasionally run into some numerical issues when computing gradi‐\n",
      "ents. For example, if you compute the gradients of the my_softplus()  function for\n",
      "large inputs, the result will be NaN:\n",
      ">>> x = tf.Variable ([100.])\n",
      ">>> with tf.GradientTape () as tape:\n",
      "...     z = my_softplus (x)\n",
      "...\n",
      ">>> tape.gradient (z, [x])\n",
      "<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\n",
      "This is because computing the gradients of this function using autodiff leads to some\n",
      "numerical difficulties: due to floating point precision errors, autodiff ends up com‐\n",
      "puting infinity divided by infinity (which returns NaN). Fortunately, we can analyti‐\n",
      "cally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which\n",
      "is numerically stable. Next, we can tell TensorFlow to use this stable function when\n",
      "computing the gradients of the my_softplus()  function, by decorating it with\n",
      "@tf.custom_gradient , and making it return both its normal output and the function\n",
      "that computes the derivatives (note that it will receive as input the gradients that were\n",
      "backpropagated so far, down to the softplus function, and according to the chain rule\n",
      "we should multiply them with this function’s gradients):\n",
      "@tf.custom_gradient\n",
      "def my_better_softplus (z):\n",
      "    exp = tf.exp(z)\n",
      "    def my_softplus_gradients (grad):\n",
      "        return grad / (1 + 1 / exp)\n",
      "    return tf.math.log(exp + 1), my_softplus_gradients\n",
      "Now when we compute the gradients of the my_better_softplus()  function, we get\n",
      "the proper result, even for large input values (however, the main output still explodes\n",
      "because of the exponential: one workaround is to use tf.where()  to just return the\n",
      "inputs when they are large).\n",
      "Congratulations! Y ou can now compute the gradients of any function (provided it is\n",
      "differentiable at the point where you compute it), you can even compute Hessians,\n",
      "block backpropagation when needed and even write your own gradient functions!\n",
      "This is probably more flexibility than you will ever need, even if you build your own\n",
      "custom training loops, as we will see now.\n",
      "Custom Training Loops\n",
      "In some rare cases, the fit()  method may not be flexible enough for what you need\n",
      "to do. For example, the Wide and Deep paper we discussed in Chapter 10  actually\n",
      "uses two different optimizers: one for the wide path and the other for the deep path.\n",
      "Since the fit()  method only uses one optimizer (the one that we specify when\n",
      "Customizing Models and Training Algorithms | 393\n",
      "compiling the model), implementing this paper requires writing your own custom\n",
      "loop.\n",
      "Y ou may also like to write your own custom training loops simply to feel more confi‐\n",
      "dent that it does precisely what you intent it to do (perhaps you are unsure about\n",
      "some details of the fit()  method). It can sometimes feel safer to make everything\n",
      "explicit. However, remember that writing a custom training loop will make your code\n",
      "longer, more error prone and harder to maintain.\n",
      "Unless you really need the extra flexibility, you should prefer using\n",
      "the fit()  method rather than implementing your own training\n",
      "loop, especially if you work in a team.\n",
      "First, let’s build a simple model. No need to compile it, since we will handle the train‐\n",
      "ing loop manually:\n",
      "l2_reg = keras.regularizers .l2(0.05)\n",
      "model = keras.models.Sequential ([\n",
      "    keras.layers.Dense(30, activation =\"elu\", kernel_initializer =\"he_normal\" ,\n",
      "                       kernel_regularizer =l2_reg),\n",
      "    keras.layers.Dense(1, kernel_regularizer =l2_reg)\n",
      "])\n",
      "Next, let’s create a tiny function that will randomly sample a batch of instances from\n",
      "the training set (in Chapter 13  we will discuss the Data API, which offers a much bet‐\n",
      "ter alternative):\n",
      "def random_batch (X, y, batch_size =32):\n",
      "    idx = np.random.randint(len(X), size=batch_size )\n",
      "    return X[idx], y[idx]\n",
      "Let’s also define a function that will display the training status, including the number\n",
      "of steps, the total number of steps, the mean loss since the start of the epoch (i.e., we\n",
      "will use the Mean  metric to compute it), and other metrics:\n",
      "def print_status_bar (iteration , total, loss, metrics=None):\n",
      "    metrics = \" - \".join([\"{}: {:.4f}\" .format(m.name, m.result())\n",
      "                         for m in [loss] + (metrics or [])])\n",
      "    end = \"\" if iteration  < total else \"\\n\"\n",
      "    print(\"\\r{}/{} - \" .format(iteration , total) + metrics,\n",
      "          end=end)\n",
      "This code is self-explanatory, unless you are unfamiliar with Python string format‐\n",
      "ting: {:.4f}  will format a float with 4 digits after the decimal point. Moreover, using\n",
      "\\r (carriage return) along with end=\"\"  ensures that the status bar always gets printed\n",
      "on the same line. In the notebook, the print_status_bar()  function also includes a\n",
      "progress bar, but you could use the handy tqdm library instead.\n",
      "394 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "With that, let’s get down to business! First, we need to define some hyperparameters,\n",
      "choose the optimizer, the loss function and the metrics (just the MAE in this exam‐\n",
      "ple):\n",
      "n_epochs  = 5\n",
      "batch_size  = 32\n",
      "n_steps = len(X_train) // batch_size\n",
      "optimizer  = keras.optimizers .Nadam(lr=0.01)\n",
      "loss_fn = keras.losses.mean_squared_error\n",
      "mean_loss  = keras.metrics.Mean()\n",
      "metrics = [keras.metrics.MeanAbsoluteError ()]\n",
      "And now we are ready to build the custom loop!\n",
      "for epoch in range(1, n_epochs  + 1):\n",
      "    print(\"Epoch {}/{}\" .format(epoch, n_epochs ))\n",
      "    for step in range(1, n_steps + 1):\n",
      "        X_batch, y_batch = random_batch (X_train_scaled , y_train)\n",
      "        with tf.GradientTape () as tape:\n",
      "            y_pred = model(X_batch, training =True)\n",
      "            main_loss  = tf.reduce_mean (loss_fn(y_batch, y_pred))\n",
      "            loss = tf.add_n([main_loss ] + model.losses)\n",
      "        gradients  = tape.gradient (loss, model.trainable_variables )\n",
      "        optimizer .apply_gradients (zip(gradients , model.trainable_variables ))\n",
      "        mean_loss (loss)\n",
      "        for metric in metrics:\n",
      "            metric(y_batch, y_pred)\n",
      "        print_status_bar (step * batch_size , len(y_train), mean_loss , metrics)\n",
      "    print_status_bar (len(y_train), len(y_train), mean_loss , metrics)\n",
      "    for metric in [mean_loss ] + metrics:\n",
      "        metric.reset_states ()\n",
      "There’s a lot going on in this code, so let’s walk through it:\n",
      "•We create two nested loops: one for the epochs, the other for the batches within\n",
      "an epoch.\n",
      "•Then we sample a random batch from the training set.\n",
      "•Inside the tf.GradientTape()  block, we make a prediction for one batch (using\n",
      "the model as a function), and we compute the loss: it is equal to the main loss\n",
      "plus the other losses (in this model, there is one regularization loss per layer).\n",
      "Since the mean_squared_error()  function returns one loss per instance, we\n",
      "compute the mean over the batch using tf.reduce_mean()  (if you wanted to\n",
      "apply different weights to each instance, this is where you would do it). The regu‐\n",
      "larization losses are already reduced to a single scalar each, so we just need to\n",
      "sum them (using tf.add_n() , which sums multiple tensors of the same shape\n",
      "and data type).\n",
      "Customizing Models and Training Algorithms | 395\n",
      "11The truth is we did not process every single instance in the training set because we sampled instances ran‐\n",
      "domly, so some were processed more than once while others were not processed at all. In practice that’s fine.\n",
      "Moreover, if the training set size is not a multiple of the batch size, we will miss a few instances.\n",
      "12Alternatively, check out K.learning_phase() , K.set_learning_phase()  and K.learning_phase_scope() .\n",
      "13With the exception of optimizers, as very few people ever customize these: see the notebook for an example.•Next, we ask the tape  to compute the gradient of the loss with regards to each\n",
      "trainable variable ( not all variables!), and we apply them to the optimizer to per‐\n",
      "form a Gradient Descent step.\n",
      "•Next we update the mean loss and the metrics (over the current epoch), and we\n",
      "display the status bar.\n",
      "•At the end of each epoch, we display the status bar again to make it look com‐\n",
      "plete11 and to print a line feed, and we reset the states of the mean loss and the\n",
      "metrics.\n",
      "If you set the optimizer’s clipnorm  or clipvalue  hyperparameters, it will take care of\n",
      "this for you. If you want to apply any other transformation to the gradients, simply do\n",
      "so before calling the apply_gradients()  method.\n",
      "If you add weight constraints to your model (e.g., by setting kernel_constraint  or\n",
      "bias_constraint  when creating a layer), you should update the training loop to\n",
      "apply these constraints just after apply_gradients() :\n",
      "for variable  in model.variables :\n",
      "    if variable .constraint  is not None:\n",
      "        variable .assign(variable .constraint (variable ))\n",
      "Most importantly, this training loop does not handle layers that behave differently\n",
      "during training and testing (e.g., BatchNormalization  or Dropout ). To handle these,\n",
      "you need to call the model with training=True  and make sure it propagates this to\n",
      "every layer that needs it.12\n",
      "As you can see, there are quite a lot of things you need to get right, it is easy to make a\n",
      "mistake. But on the bright side, you get full control, so it’s your call.\n",
      "Now that you know how to customize any part of your models13 and training algo‐\n",
      "rithms, let’s see how you can use TensorFlow’s automatic graph generation feature: it\n",
      "can speed up your custom code considerably, and it will also make it portable to any\n",
      "platform supported by TensorFlow (see ???).\n",
      "TensorFlow Functions and Graphs\n",
      "In TensorFlow 1, graphs were unavoidable (as were the complexities that came with\n",
      "them): they were a central part of TensorFlow’s API. In TensorFlow 2, they are still\n",
      "396 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "there, but not as central, and much (much!) simpler to use. To demonstrate this, let’s\n",
      "start with a trivial function that just computes the cube of its input:\n",
      "def cube(x):\n",
      "    return x ** 3\n",
      "We can obviously call this function with a Python value, such as an int or a float, or\n",
      "we can call it with a tensor:\n",
      ">>> cube(2)\n",
      "8\n",
      ">>> cube(tf.constant (2.0))\n",
      "<tf.Tensor: id=18634148, shape=(), dtype=float32, numpy=8.0>\n",
      "Now, let’s use tf.function()  to convert this Python function to a TensorFlow Func‐\n",
      "tion:\n",
      ">>> tf_cube = tf.function (cube)\n",
      ">>> tf_cube\n",
      "<tensorflow.python.eager.def_function.Function at 0x1546fc080>\n",
      "This TF Function can then be used exactly like the original Python function, and it\n",
      "will return the same result (but as tensors):\n",
      ">>> tf_cube(2)\n",
      "<tf.Tensor: id=18634201, shape=(), dtype=int32, numpy=8>\n",
      ">>> tf_cube(tf.constant (2.0))\n",
      "<tf.Tensor: id=18634211, shape=(), dtype=float32, numpy=8.0>\n",
      "Under the hood, tf.function()  analyzed the computations performed by the cube()\n",
      "function and generated an equivalent computation graph! As you can see, it was\n",
      "rather painless (we will see how this works shortly). Alternatively, we could have used\n",
      "tf.function  as a decorator; this is actually more common:\n",
      "@tf.function\n",
      "def tf_cube(x):\n",
      "    return x ** 3\n",
      "The original Python function is still available via the TF Function’s python_function\n",
      "attribute, in case you ever need it:\n",
      ">>> tf_cube.python_function (2)\n",
      "8\n",
      "TensorFlow optimizes the computation graph, pruning unused nodes, simplifying\n",
      "expressions (e.g., 1 + 2 would get replaced with 3), and more. Once the optimized\n",
      "graph is ready, the TF Function efficiently executes the operations in the graph, in the\n",
      "appropriate order (and in parallel when it can). As a result, a TF Function will usually\n",
      "run much faster than the original Python function, especially if it performs complex\n",
      "TensorFlow Functions and Graphs | 397\n",
      "14However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\n",
      "tf_cube()  actually runs much slower than cube() .\n",
      "computations.14 Most of the time you will not really need to know more than that:\n",
      "when you want to boost a Python function, just transform it into a TF Function.\n",
      "That’s all!\n",
      "Moreover, when you write a custom loss function, a custom metric, a custom layer or\n",
      "any other custom function, and you use it in a Keras model (as we did throughout\n",
      "this chapter), Keras automatically converts your function into a TF Function, no need\n",
      "to use tf.function() . So most of the time, all this magic is 100% transparent.\n",
      "Y ou can tell Keras not to convert your Python functions to TF\n",
      "Functions by setting dynamic=True  when creating a custom layer\n",
      "or a custom model. Alternatively, you can set run_eagerly=True\n",
      "when calling the model’s compile()  method.\n",
      "TF Function generates a new graph for every unique set of input shapes and data\n",
      "types, and it caches it for subsequent calls. For example, if you call tf_cube(tf.con\n",
      "stant(10)) , a graph will be generated for int32 tensors of shape []. Then if you call\n",
      "tf_cube(tf.constant(20)) , the same graph will be reused. But if you then call\n",
      "tf_cube(tf.constant([10, 20])) , a new graph will be generated for int32 tensors\n",
      "of shape [2]. This is how TF Functions handle polymorphism (i.e., varying argument\n",
      "types and shapes). However, this is only true for tensor arguments: if you pass numer‐\n",
      "ical Python values to a TF Function, a new graph will be generated for every distinct\n",
      "value: for example, calling tf_cube(10)  and tf_cube(20)  will generate two graphs.\n",
      "If you call a TF Function many times with different numerical\n",
      "Python values, then many graphs will be generated, slowing down\n",
      "your program and using up a lot of RAM. Python values should be\n",
      "reserved for arguments that will have few unique values, such as\n",
      "hyperparameters like the number of neurons per layer. This allows\n",
      "TensorFlow to better optimize each variant of your model.\n",
      "Autograph and Tracing\n",
      "So how does TensorFlow generate graphs? Well, first it starts by analyzing the Python\n",
      "function’s source code to capture all the control flow statements, such as for loops\n",
      "and while  loops, if statements, as well as break , continue  and return  statements.\n",
      "This first step is called autograph . The reason TensorFlow has to analyze the source\n",
      "code is that Python does not provide any other way to capture control flow state‐\n",
      "ments: it offers magic methods like __add__()  or __mul__()  to capture operators like\n",
      "398 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "+ and *, but there are no __while__()  or __if__()  magic methods. After analyzing\n",
      "the function’s code, autograph outputs an upgraded version of that function in which\n",
      "all the control flow statements are replaced by the appropriate TensorFlow opera‐\n",
      "tions, such as tf.while_loop()  for loops and tf.cond()  for if statements. For\n",
      "example, in Figure 12-4 , autograph analyzes the source code of the sum_squares()\n",
      "Python function, and it generates the tf__sum_squares()  function. In this function,\n",
      "the for loop is replaced by the definition of the loop_body()  function (containing\n",
      "the body of the original for loop), followed by a call to the for_stmt()  function. This\n",
      "call will build the appropriate tf.while_loop()  operation in the computation graph.\n",
      "Figure 12-4. How TensorFlow generates graphs using autograph and tracing\n",
      "Next, TensorFlow calls this “upgraded” function, but instead of passing the actual\n",
      "argument, it passes a symbolic tensor , meaning a tensor without any actual value, only\n",
      "a name, a data type, and a shape. For example, if you call sum_squares(tf.con\n",
      "stant(10)) , then the tf__sum_squares()  function will actually be called with a sym‐\n",
      "bolic tensor of type int32 and shape []. The function will run in graph mode , meaning\n",
      "that each TensorFlow operation will just add a node in the graph to represent itself\n",
      "and its output tensor(s) (as opposed to the regular mode, called eager execution , or\n",
      "eager mode ). In graph mode, TF operations do not perform any actual computations.\n",
      "This should feel familiar if you know TensorFlow 1, as graph mode was the default\n",
      "mode. In Figure 12-4 , you can see the tf__sum_squares()  function being called with\n",
      "a symbolic tensor as argument (in this case, an int32 tensor of shape []), and the final\n",
      "graph generated during tracing. The ellipses represent operations, and the arrows\n",
      "represent tensors (both the generated function and the graph are simplified).\n",
      "TensorFlow Functions and Graphs | 399\n",
      "To view the generated function’s source code, you can call tf.auto\n",
      "graph.to_code(sum_squares.python_function) . The code is not\n",
      "meant to be pretty, but it can sometimes help for debugging.\n",
      "TF Function Rules\n",
      "Most of the time, converting a Python function that performs TensorFlow operations\n",
      "into a TF Function is trivial: just decorate it with @tf.function  or let Keras take care\n",
      "of it for you. However, there are a few rules to respect:\n",
      "•If you call any external library, including NumPy or even the standard library,\n",
      "this call will run only during tracing, it will not be part of the graph. Indeed, a\n",
      "TensorFlow graph can only include TensorFlow constructs (tensors, operations,\n",
      "variables, datasets, and so on). So make sure you use tf.reduce_sum()  instead of\n",
      "np.sum() , and tf.sort()  instead of the built-in sorted()  function, and so on\n",
      "(unless you really want the code to run only during tracing).\n",
      "—For example, if you define a TF function f(x)  that just returns np.ran\n",
      "dom.rand() , a random number will only be generated when the function is\n",
      "traced, so f(tf.constant(2.))  and f(tf.constant(3.))  will return the\n",
      "same random number, but f(tf.constant([2., 3.]))  will return a different\n",
      "one. If you replace np.random.rand()  with tf.random.uniform([]) , then a\n",
      "new random number will be generated upon every call, since the operation\n",
      "will be part of the graph.\n",
      "—If your non-TensorFlow code has side-effects (such as logging something or\n",
      "updating a Python counter), then you should not expect that side-effect to\n",
      "occur every time you call the TF Function, as it will only occur when the func‐\n",
      "tion is traced.\n",
      "—Y ou can wrap arbitrary Python code in a tf.py_function()  operation, but\n",
      "this will hinder performance, as TensorFlow will not be able to do any graph\n",
      "optimization on this code, and it will also reduce portability, as the graph will\n",
      "only run on platforms where Python is available (and the right libraries\n",
      "installed).\n",
      "•Y ou can call other Python functions or TF Functions, but they should follow the\n",
      "same rules, as TensorFlow will also capture their operations in the computation\n",
      "graph. Note that these other functions do not need to be decorated with\n",
      "@tf.function .\n",
      "•If the function creates a TensorFlow variable (or any other stateful TensorFlow\n",
      "object, such as a dataset or a queue), it must do so upon the very first call, and\n",
      "only then, or else you will get an exception. It is usually preferable to create vari‐\n",
      "ables outside of the TF Function (e.g., in the build()  method of a custom layer).\n",
      "400 | Chapter 12: Custom Models and Training with TensorFlow\n",
      "•The source code of your Python function should be available to TensorFlow. If\n",
      "the source code is unavailable (for example, if you define your function in the\n",
      "Python shell, which does not give access to the source code, or if you deploy only\n",
      "the compiled Python files *.pyc  to production), then the graph generation pro‐\n",
      "cess will fail or have limited functionality.\n",
      "•TensorFlow will only capture for loops that iterate over a tensor or a Dataset . So\n",
      "make sure you use for i in tf.range(10)  rather than for i in range(10) , or\n",
      "else the loop will not be captured in the graph. Instead, it will run during tracing.\n",
      "This may be what you want, if the for loop is meant to build the graph, for exam‐\n",
      "ple to create each layer in a neural network.\n",
      "•And as always, for performance reasons, you should prefer a vectorized imple‐\n",
      "mentation whenever you can, rather than using loops.\n",
      "It’s time to sum up! In this chapter we started with a brief overview of TensorFlow,\n",
      "then we looked at TensorFlow’s low-level API, including tensors, operations, variables\n",
      "and special data structures. We then used these tools to customize almost every com‐\n",
      "ponent in tf.keras. Finally, we looked at how TF Functions can boost performance,\n",
      "how graphs are generated using autograph and tracing, and what rules to follow when\n",
      "you write TF Functions (if you would like to open the black box a bit further, for\n",
      "example to explore the generated graphs, you will find further technical details\n",
      "in ???).\n",
      "In the next chapter, we will look at how to efficiently load and preprocess data with\n",
      "TensorFlow.\n",
      "TensorFlow Functions and Graphs | 401\n",
      "\n",
      "CHAPTER 13\n",
      "Loading and Preprocessing Data with\n",
      "TensorFlow\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 13 in the final\n",
      "release of the book.\n",
      "So far we have used only datasets that fit in memory, but Deep Learning systems are\n",
      "often trained on very large datasets that will not fit in RAM. Ingesting a large dataset\n",
      "and preprocessing it efficiently can be tricky to implement with other Deep Learning\n",
      "libraries, but TensorFlow makes it easy thanks to the Data API : you just create a data‐\n",
      "set object, tell it where to get the data, then transform it in any way you want, and\n",
      "TensorFlow takes care of all the implementation details, such as multithreading,\n",
      "queuing, batching, prefetching, and so on.\n",
      "Off the shelf, the Data API can read from text files (such as CSV files), binary files\n",
      "with fixed-size records, and binary files that use TensorFlow’s TFRecord format,\n",
      "which supports records of varying sizes. TFRecord is a flexible and efficient binary\n",
      "format based on Protocol Buffers (an open source binary format). The Data API also\n",
      "has support for reading from SQL databases. Moreover, many Open Source exten‐\n",
      "sions are available to read from all sorts of data sources, such as Google’s BigQuery\n",
      "service.\n",
      "However, reading huge datasets efficiently is not the only difficulty: the data also\n",
      "needs to be preprocessed. Indeed, it is not always composed strictly of convenient\n",
      "numerical fields: sometimes there will be text features, categorical features, and so on.\n",
      "To handle this, TensorFlow provides the Features API : it lets you easily convert these\n",
      "features to numerical features that can be consumed by your neural network. For\n",
      "403\n",
      "example, categorical features with a large number of categories (such as cities, or\n",
      "words) can be encoded using embeddings  (as we will see, an embedding is a trainable\n",
      "dense vector that represents a category).\n",
      "Both the Data API and the Features API work seamlessly with\n",
      "tf.keras.\n",
      "In this chapter, we will cover the Data API, the TFRecord format and the Features\n",
      "API in detail. We will also take a quick look at a few related projects from Tensor‐\n",
      "Flow’s ecosystem:\n",
      "•TF Transform ( tf.Transform ) makes it possible to write a single preprocessing\n",
      "function that can be run both in batch mode on your full training set, before\n",
      "training (to speed it up), and then exported to a TF Function and incorporated\n",
      "into your trained model, so that once it is deployed in production, it can take\n",
      "care of preprocessing new instances on the fly.\n",
      "•TF Datasets (TFDS) provides a convenient function to download many common\n",
      "datasets of all kinds, including large ones like ImageNet, and it provides conve‐\n",
      "nient dataset objects to manipulate them using the Data API.\n",
      "So let’s get started!\n",
      "The Data API\n",
      "The whole Data API revolves around the concept of a dataset : as you might suspect,\n",
      "this represents a sequence of data items. Usually you will use datasets that gradually\n",
      "read data from disk, but for simplicity let’s just create a dataset entirely in RAM using\n",
      "tf.data.Dataset.from_tensor_slices() :\n",
      ">>> X = tf.range(10)  # any data tensor\n",
      ">>> dataset = tf.data.Dataset.from_tensor_slices (X)\n",
      ">>> dataset\n",
      "<TensorSliceDataset shapes: (), types: tf.int32>\n",
      "The from_tensor_slices()  function takes a tensor and creates a tf.data.Dataset\n",
      "whose elements are all the slices of X (along the first dimension), so this dataset con‐\n",
      "tains 10 items: tensors 0, 1, 2, …, 9. In this case we would have obtained the same\n",
      "dataset if we had used tf.data.Dataset.range(10) .\n",
      "Y ou can simply iterate over a dataset’s items like this:\n",
      ">>> for item in dataset:\n",
      "...     print(item)\n",
      "404 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "...\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "[...]\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "Chaining Transformations\n",
      "Once you have a dataset, you can apply all sorts of transformations to it by calling its\n",
      "transformation methods. Each method returns a new dataset, so you can chain trans‐\n",
      "formations like this (this chain is illustrated in Figure 13-1 ):\n",
      ">>> dataset = dataset.repeat(3).batch(7)\n",
      ">>> for item in dataset:\n",
      "...     print(item)\n",
      "...\n",
      "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\n",
      "tf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\n",
      "tf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\n",
      "tf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\n",
      "tf.Tensor([8 9], shape=(2,), dtype=int32)\n",
      "Figure 13-1. Chaining Dataset Transformations\n",
      "In this example, we first call the repeat()  method on the original dataset, and it\n",
      "returns a new dataset that will repeat the items of the original dataset 3 times. Of\n",
      "course, this will not copy the whole data in memory 3 times! In fact, if you call this\n",
      "method with no arguments, the new dataset will repeat the source dataset forever.\n",
      "Then we call the batch()  method on this new dataset, and again this creates a new\n",
      "dataset. This one will group the items of the previous dataset in batches of 7 items.\n",
      "Finally, we iterate over the items of this final dataset. As you can see, the batch()\n",
      "method had to output a final batch of size 2 instead of 7, but you can call it with\n",
      "drop_remainder=True  if you want it to drop this final batch so that all batches have\n",
      "the exact same size.\n",
      "The Data API | 405\n",
      "The dataset methods do not modify datasets, they create new ones,\n",
      "so make sure to keep a reference to these new datasets (e.g., data\n",
      "set = ... ), or else nothing will happen.\n",
      "Y ou can also apply any transformation you want to the items by calling the map()\n",
      "method. For example, this creates a new dataset with all items doubled:\n",
      ">>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\n",
      "This function is the one you will call to apply any preprocessing you want to your\n",
      "data. Sometimes, this will include computations that can be quite intensive, such as\n",
      "reshaping or rotating an image, so you will usually want to spawn multiple threads to\n",
      "speed things up: it’s as simple as setting the num_parallel_calls  argument.\n",
      "While the map()  applies a transformation to each item, the apply()  method applies a\n",
      "transformation to the dataset as a whole. For example, the following code “unbatches”\n",
      "the dataset, by applying the unbatch()  function to the dataset (this function is cur‐\n",
      "rently experimental, but it will most likely move to the core API in a future release).\n",
      "Each item in the new dataset will be a single integer tensor instead of a batch of 7\n",
      "integers:\n",
      ">>> dataset = dataset.apply(tf.data.experimental .unbatch()) # Items: 0,2,4,...\n",
      "It is also possible to simply filter the dataset using the filter()  method:\n",
      ">>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\n",
      "Y ou will often want to look at just a few items from a dataset. Y ou can use the take()\n",
      "method for that:\n",
      ">>> for item in dataset.take(3):\n",
      "...     print(item)\n",
      "...\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(4, shape=(), dtype=int64)\n",
      "Shuffling  the Data\n",
      "As you know, Gradient Descent works best when the instances in the training set are\n",
      "independent and identically distributed (see Chapter 4 ). A simple way to ensure this\n",
      "is to shuffle the instances. For this, you can just use the shuffle()  method. It will\n",
      "create a new dataset that will start by filling up a buffer with the first items of the\n",
      "source dataset, then whenever it is asked for an item, it will pull one out randomly\n",
      "from the buffer, and replace it with a fresh one from the source dataset, until it has\n",
      "iterated entirely through the source dataset. At this point it continues to pull out\n",
      "items randomly from the buffer until it is empty. Y ou must specify the buffer size, and\n",
      "406 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "1Imagine a sorted deck of cards on your left: suppose you just take the top 3 cards and shuffle them, then pick\n",
      "one randomly and put it to your right, keeping the other 2 in your hands. Take another card on your left,\n",
      "shuffle the 3 cards in your hands and pick one of them randomly, and put it on your right. When you are\n",
      "done going through all the cards like this, you will have a deck of cards on your right: do you think it will be\n",
      "perfectly shuffled?\n",
      "it is important to make it large enough or else shuffling will not be very efficient.1\n",
      "However, obviously do not exceed the amount of RAM you have, and even if you\n",
      "have plenty of it, there’s no need to go well beyond the dataset’s size. Y ou can provide\n",
      "a random seed if you want the same random order every time you run your program.\n",
      ">>> dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n",
      ">>> dataset = dataset.shuffle(buffer_size =5, seed=42).batch(7)\n",
      ">>> for item in dataset:\n",
      "...     print(item)\n",
      "...\n",
      "tf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\n",
      "tf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\n",
      "tf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\n",
      "tf.Tensor([3 6], shape=(2,), dtype=int64)\n",
      "If you call repeat()  on a shuffled dataset, by default it will generate\n",
      "a new order at every iteration. This is generally a good idea, but if\n",
      "you prefer to reuse the same order at each iteration (e.g., for tests\n",
      "or debugging), you can set reshuffle_each_iteration=False .\n",
      "For a large dataset that does not fit in memory, this simple shuffling-buffer approach\n",
      "may not be sufficient, since the buffer will be small compared to the dataset. One sol‐\n",
      "ution is to shuffle the source data itself (for example, on Linux you can shuffle text\n",
      "files using the shuf  command). This will definitely improve shuffling a lot! However,\n",
      "even if the source data is shuffled, you will usually want to shuffle it some more, or\n",
      "else the same order will be repeated at each epoch, and the model may end up being\n",
      "biased (e.g., due to some spurious patterns present by chance in the source data’s\n",
      "order). To shuffle the instances some more, a common approach is to split the source\n",
      "data into multiple files, then read them in a random order during training. However,\n",
      "instances located in the same file will still end up close to each other. To avoid this\n",
      "you can pick multiple files randomly, and read them simultaneously, interleaving\n",
      "their lines. Then on top of that you can add a shuffling buffer using the shuffle()\n",
      "method. If all this sounds like a lot of work, don’t worry: the Data API actually makes\n",
      "all this possible in just a few lines of code. Let’s see how to do this.\n",
      "The Data API | 407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interleaving Lines From Multiple Files\n",
      "First, let’s suppose that you loaded the California housing dataset, you shuffled it\n",
      "(unless it was already shuffled), you split it into a training set, a validation set and a\n",
      "test set, then you split each set into many CSV files that each look like this (each row\n",
      "contains 8 input features plus the target median house value):\n",
      "MedInc,HouseAge,AveRooms,AveBedrms,Popul,AveOccup,Lat,Long,MedianHouseValue\n",
      "3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442\n",
      "5.3275,5.0,6.4900,0.9910,3464.0,3.4433,33.69,-117.39,1.687\n",
      "3.1,29.0,7.5423,1.5915,1328.0,2.2508,38.44,-122.98,1.621\n",
      "[...]\n",
      "Let’s also suppose train_filepaths  contains the list of file paths (and you also have\n",
      "valid_filepaths  and test_filepaths ):\n",
      ">>> train_filepaths\n",
      "['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv',...]\n",
      "Now let’s create a dataset containing only these file paths:\n",
      "filepath_dataset  = tf.data.Dataset.list_files (train_filepaths , seed=42)\n",
      "By default, the list_files()  function returns a dataset that shuffles the file paths. In\n",
      "general this is a good thing, but you can set shuffle=False  if you do not want that,\n",
      "for some reason.\n",
      "Next, we can call the interleave()  method to read from 5 files at a time and inter‐\n",
      "leave their lines (skipping the first line of each file, which is the header row, using the\n",
      "skip()  method):\n",
      "n_readers  = 5\n",
      "dataset = filepath_dataset .interleave (\n",
      "    lambda filepath : tf.data.TextLineDataset (filepath ).skip(1),\n",
      "    cycle_length =n_readers )\n",
      "The interleave()  method will create a dataset that will pull 5 file paths from the\n",
      "filepath_dataset , and for each one it will call the function we gave it (a lambda in\n",
      "this example) to create a new dataset, in this case a TextLineDataset . It will then\n",
      "cycle through these 5 datasets, reading one line at a time from each until all datasets\n",
      "are out of items. Then it will get the next 5 file paths from the filepath_dataset , and\n",
      "interleave them the same way, and so on until it runs out of file paths.\n",
      "For interleaving to work best, it is preferable to have files of identi‐\n",
      "cal length, or else the end of the longest files will not be interleaved.\n",
      "408 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "By default, interleave()  does not use parallelism, it just reads one line at a time\n",
      "from each file, sequentially. However, if you want it to actually read files in parallel,\n",
      "you can set the num_parallel_calls  argument to the number of threads you want.\n",
      "Y ou can even set it to tf.data.experimental.AUTOTUNE  to make TensorFlow choose\n",
      "the right number of threads dynamically based on the available CPU (however, this is\n",
      "an experimental feature for now). Let’s look at what the dataset contains now:\n",
      ">>> for line in dataset.take(5):\n",
      "...     print(line.numpy())\n",
      "...\n",
      "b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\n",
      "b'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\n",
      "b'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\n",
      "b'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\n",
      "b'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'\n",
      "These are the first rows (ignoring the header row) of 5 CSV files, chosen randomly.\n",
      "Looks good! But as you can see, these are just byte strings, we need to parse them,\n",
      "and also scale the data.\n",
      "Preprocessing the Data\n",
      "Let’s implement a small function that will perform this preprocessing:\n",
      "X_mean, X_std = [...] # mean and scale of each feature in the training set\n",
      "n_inputs  = 8\n",
      "def preprocess (line):\n",
      "  defs = [0.] * n_inputs  + [tf.constant ([], dtype=tf.float32)]\n",
      "  fields = tf.io.decode_csv (line, record_defaults =defs)\n",
      "  x = tf.stack(fields[:-1])\n",
      "  y = tf.stack(fields[-1:])\n",
      "  return (x - X_mean) / X_std, y\n",
      "Let’s walk through this code:\n",
      "•First, we assume that you have precomputed the mean and standard deviation of\n",
      "each feature in the training set. X_mean  and X_std  are just 1D tensors (or NumPy\n",
      "arrays) containing 8 floats, one per input feature.\n",
      "•The preprocess()  function takes one CSV line, and starts by parsing it. For this,\n",
      "it uses the tf.io.decode_csv()  function, which takes two arguments: the first is\n",
      "the line to parse, and the second is an array containing the default value for each\n",
      "column in the CSV file. This tells TensorFlow not only the default value for each\n",
      "column, but also the number of columns and the type of each column. In this\n",
      "example, we tell it that all feature columns are floats and missing values should\n",
      "default to 0, but we provide an empty array of type tf.float32  as the default\n",
      "value for the last column (the target): this tells TensorFlow that this column con‐\n",
      "The Data API | 409\n",
      "tains floats, but that there is no default value, so it will raise an exception if it\n",
      "encounters a missing value.\n",
      "•The decode_csv()  function returns a list of scalar tensors (one per column) but\n",
      "we need to return 1D tensor arrays. So we call tf.stack()  on all tensors except\n",
      "for the last one (the target): this will stack these tensors into a 1D array. We then\n",
      "do the same for the target value (this makes it a 1D tensor array with a single\n",
      "value, rather than a scalar tensor).\n",
      "•Finally, we scale the input features by subtracting the feature means and then\n",
      "dividing by the feature standard deviations, and we return a tuple containing the\n",
      "scaled features and the target.\n",
      "Let’s test this preprocessing function:\n",
      ">>> preprocess (b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782' )\n",
      "(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy=\n",
      " array([ 0.16579159,  1.216324  , -0.05204564, -0.39215982, -0.5277444 ,\n",
      "        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n",
      " <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\n",
      "We can now apply this preprocessing function to the dataset.\n",
      "Putting Everything Together\n",
      "To make the code reusable, let’s put together everything we have discussed so far into\n",
      "a small helper function: it will create and return a dataset that will efficiently load Cal‐\n",
      "ifornia housing data from multiple CSV files, then shuffle it, preprocess it and batch it\n",
      "(see Figure 13-2 ):\n",
      "def csv_reader_dataset (filepaths , repeat=None, n_readers =5,\n",
      "                       n_read_threads =None, shuffle_buffer_size =10000,\n",
      "                       n_parse_threads =5, batch_size =32):\n",
      "    dataset = tf.data.Dataset.list_files (filepaths ).repeat(repeat)\n",
      "    dataset = dataset.interleave (\n",
      "        lambda filepath : tf.data.TextLineDataset (filepath ).skip(1),\n",
      "        cycle_length =n_readers , num_parallel_calls =n_read_threads )\n",
      "    dataset = dataset.shuffle(shuffle_buffer_size )\n",
      "    dataset = dataset.map(preprocess , num_parallel_calls =n_parse_threads )\n",
      "    dataset = dataset.batch(batch_size )\n",
      "    return dataset.prefetch (1)\n",
      "410 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "2In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna‐\n",
      "tively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE  (this is an\n",
      "experimental feature for now).\n",
      "Figure 13-2. Loading and Preprocessing Data From Multiple CSV Files\n",
      "Everything should make sense in this code, except the very last line ( prefetch(1) ),\n",
      "which is actually quite important for performance.\n",
      "Prefetching\n",
      "By calling prefetch(1)  at the end, we are creating a dataset that will do its best to\n",
      "always be one batch ahead2. In other words, while our training algorithm is working\n",
      "on one batch, the dataset will already be working in parallel on getting the next batch\n",
      "ready. This can improve performance dramatically, as is illustrated on Figure 13-3 . If\n",
      "we also ensure that loading and preprocessing are multithreaded (by setting num_par\n",
      "allel_calls  when calling interleave()  and map() ), we can exploit multiple cores\n",
      "on the CPU and hopefully make preparing one batch of data shorter than running a\n",
      "training step on the GPU: this way the GPU will be almost 100% utilized (except for\n",
      "the data transfer time from the CPU to the GPU), and training will run much faster.\n",
      "The Data API | 411\n",
      "Figure 13-3. Speedup Training Thanks  to Prefetching and Multithreading\n",
      "If you plan to purchase a GPU card, its processing power and its\n",
      "memory size are of course very important (in particular, a large\n",
      "RAM is crucial for computer vision), but its memory bandwidth  is\n",
      "just as important as the processing power to get good performance:\n",
      "this is the number of gigabytes of data it can get in or out of its\n",
      "RAM per second.\n",
      "With that, you can now build efficient input pipelines to load and preprocess data\n",
      "from multiple text files. We have discussed the most common dataset methods, but\n",
      "there are a few more you may want to look at: concatenate() , zip() , window() ,\n",
      "reduce() , cache() , shard() , flat_map()  and padded_batch() . There are also a cou‐\n",
      "ple more class methods: from_generator()  and from_tensors() , which create a new\n",
      "dataset from a Python generator or a list of tensors respectively. Please check the API\n",
      "documentation for more details. Also note that there are experimental features avail‐\n",
      "able in tf.data.experimental , many of which will most likely make it to the core\n",
      "API in future releases (e.g., check out the CsvDataset  class and the SqlDataset\n",
      "classes).\n",
      "412 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "3Support for datasets is specific to tf.keras, it will not work on other implementations of the Keras API.\n",
      "4The number of steps per epoch is optional if the dataset just goes through the data once, but if you do not\n",
      "specify it, the progress bar will not be displayed during the first epoch.\n",
      "5Note that for now the dataset must be created within the TF Function. This may be fixed by the time you read\n",
      "these lines (see TensorFlow issue #25414).Using the Dataset With tf.keras\n",
      "Now we can use the csv_reader_dataset()  function to create a dataset for the train‐\n",
      "ing set (ensuring it repeats the data forever), the validation set and the test set:\n",
      "train_set  = csv_reader_dataset (train_filepaths , repeat=None)\n",
      "valid_set  = csv_reader_dataset (valid_filepaths )\n",
      "test_set  = csv_reader_dataset (test_filepaths )\n",
      "And now we can simply build and train a Keras model using these datasets.3 All we\n",
      "need to do is to call the fit()  method with the datasets instead of X_train  and\n",
      "y_train , and specify the number of steps per epoch for each set:4\n",
      "model = keras.models.Sequential ([...])\n",
      "model.compile([...])\n",
      "model.fit(train_set , steps_per_epoch =len(X_train) // batch_size , epochs=10,\n",
      "          validation_data =valid_set ,\n",
      "          validation_steps =len(X_valid) // batch_size )\n",
      "Similarly, we can pass a dataset to the evaluate()  and predict()  methods (and again\n",
      "specify the number of steps per epoch):\n",
      "model.evaluate (test_set , steps=len(X_test) // batch_size )\n",
      "model.predict(new_set, steps=len(X_new) // batch_size )\n",
      "Unlike the other sets, the new_set  will usually not contain labels (if it does, Keras will\n",
      "just ignore them). Note that in all these cases, you can still use NumPy arrays instead\n",
      "of datasets if you want (but of course they need to have been loaded and preprocessed\n",
      "first).\n",
      "If you want to build your own custom training loop (as in Chapter 12 ), you can just\n",
      "iterate over the training set, very naturally:\n",
      "for X_batch, y_batch in train_set :\n",
      "    [...] # perform one gradient descent step\n",
      "In fact, it is even possible to create a tf.function (see Chapter 12 ) that performs the\n",
      "whole training loop!5\n",
      "@tf.function\n",
      "def train(model, optimizer , loss_fn, n_epochs , [...]):\n",
      "    train_set  = csv_reader_dataset (train_filepaths , repeat=n_epochs , [...])\n",
      "    for X_batch, y_batch in train_set :\n",
      "        with tf.GradientTape () as tape:\n",
      "The Data API | 413\n",
      "            y_pred = model(X_batch)\n",
      "            main_loss  = tf.reduce_mean (loss_fn(y_batch, y_pred))\n",
      "            loss = tf.add_n([main_loss ] + model.losses)\n",
      "        grads = tape.gradient (loss, model.trainable_variables )\n",
      "        optimizer .apply_gradients (zip(grads, model.trainable_variables ))\n",
      "Congratulations, you now know how to build powerful input pipelines using the Data\n",
      "API! However, so far we have used CSV files, which are common, simple and conve‐\n",
      "nient, but they are not really efficient, and they do not support large or complex data\n",
      "structures very well, such as images or audio. So let’s use TFRecords instead.\n",
      "If you are happy with CSV files (or whatever other format you are\n",
      "using), you do not have  to use TFRecords. As the saying goes, if it\n",
      "ain’t broke, don’t fix it! TFRecords are useful when the bottleneck\n",
      "during training is loading and parsing the data.\n",
      "The TFRecord Format\n",
      "The TFRecord format is TensorFlow’s preferred format for storing large amounts of\n",
      "data and reading it efficiently. It is a very simple binary format that just contains a\n",
      "sequence of binary records of varying sizes (each record just has a length, a CRC\n",
      "checksum to check that the length was not corrupted, then the actual data, and finally\n",
      "a CRC checksum for the data). Y ou can easily create a TFRecord file using the\n",
      "tf.io.TFRecordWriter  class:\n",
      "with tf.io.TFRecordWriter (\"my_data.tfrecord\" ) as f:\n",
      "    f.write(b\"This is the first record\" )\n",
      "    f.write(b\"And this is the second record\" )\n",
      "And you can then use a tf.data.TFRecordDataset  to read one or more TFRecord\n",
      "files:\n",
      "filepaths  = [\"my_data.tfrecord\" ]\n",
      "dataset = tf.data.TFRecordDataset (filepaths )\n",
      "for item in dataset:\n",
      "    print(item)\n",
      "This will output:\n",
      "tf.Tensor(b'This is the first record', shape=(), dtype=string)\n",
      "tf.Tensor(b'And this is the second record', shape=(), dtype=string)\n",
      "By default, a TFRecordDataset  will read files one by one, but you\n",
      "can make it read multiple files in parallel and interleave their\n",
      "records by setting num_parallel_reads . Alternatively, you could\n",
      "obtain the same result by using list_files()  and interleave()\n",
      "as we did earlier to read multiple CSV files.\n",
      "414 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "6Since protobuf objects are meant to be serialized and transmitted, they are called messages .Compressed TFRecord Files\n",
      "It can sometimes be useful to compress your TFRecord files, especially if they need to\n",
      "be loaded via a network connection. Y ou can create a compressed TFRecord file by\n",
      "setting the options  argument:\n",
      "options = tf.io.TFRecordOptions (compression_type =\"GZIP\")\n",
      "with tf.io.TFRecordWriter (\"my_compressed.tfrecord\" , options) as f:\n",
      "  [...]\n",
      "When reading a compressed TFRecord file, you need to specify the compression type:\n",
      "dataset = tf.data.TFRecordDataset ([\"my_compressed.tfrecord\" ],\n",
      "                                  compression_type =\"GZIP\")\n",
      "A Brief Introduction to Protocol Buffers\n",
      "Even though each record can use any binary format you want, TFRecord files usually\n",
      "contain serialized Protocol Buffers (also called protobufs ). This is a portable, extensi‐\n",
      "ble and efficient binary format developed at Google back in 2001 and Open Sourced\n",
      "in 2008, and they are now widely used, in particular in gRPC , Google’s remote proce‐\n",
      "dure call system. Protocol Buffers are defined using a simple language that looks like\n",
      "this:\n",
      "syntax = \"proto3\" ;\n",
      "message Person {\n",
      "  string name = 1;\n",
      "  int32 id = 2;\n",
      "  repeated  string email = 3;\n",
      "}\n",
      "This definition says we are using the protobuf format version 3, and it specifies that\n",
      "each Person  object6 may (optionally) have a name  of type string , an id of type int32 ,\n",
      "and zero or more email  fields, each of type string . The numbers 1, 2 and 3 are the\n",
      "field identifiers: they will be used in each record’s binary representation. Once you\n",
      "have a definition in a .proto  file, you can compile it. This requires protoc , the proto‐\n",
      "buf compiler, to generate access classes in Python (or some other language). Note that\n",
      "the protobuf definitions we will use have already been compiled for you, and their\n",
      "Python classes are part of TensorFlow, so you will not need to use protoc . All you\n",
      "need to know is how to use protobuf access classes in Python. To illustrate the basics,\n",
      "let’s look at a simple example that uses the access classes generated for the Person\n",
      "protobuf (the code is explained in the comments):\n",
      ">>> from person_pb2  import Person  # import the generated access class\n",
      ">>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\" ])  # create a Person\n",
      ">>> print(person)  # display the Person\n",
      "The TFRecord Format | 415\n",
      "7This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\n",
      "about protobufs, please visit https://homl.info/protobuf .name: \"Al\"\n",
      "id: 123\n",
      "email: \"a@b.com\"\n",
      ">>> person.name  # read a field\n",
      "\"Al\"\n",
      ">>> person.name = \"Alice\"  # modify a field\n",
      ">>> person.email[0]  # repeated fields can be accessed like arrays\n",
      "\"a@b.com\"\n",
      ">>> person.email.append(\"c@d.com\" )  # add an email address\n",
      ">>> s = person.SerializeToString ()  # serialize the object to a byte string\n",
      ">>> s\n",
      "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'\n",
      ">>> person2 = Person()  # create a new Person\n",
      ">>> person2.ParseFromString (s)  # parse the byte string (27 bytes long)\n",
      "27\n",
      ">>> person == person2  # now they are equal\n",
      "True\n",
      "In short, we import the Person  class generated by protoc , we create an instance and\n",
      "we play with it, visualizing it, reading and writing some fields, then we serialize it\n",
      "using the SerializeToString()  method. This is the binary data that is ready to be\n",
      "saved or transmitted over the network. When reading or receiving this binary data,\n",
      "we can parse it using the ParseFromString()  method, and we get a copy of the object\n",
      "that was serialized.7\n",
      "We could save the serialized Person  object to a TFRecord file, then we could load and\n",
      "parse it: everything would work fine. However, SerializeToString()  and ParseFrom\n",
      "String()  are not TensorFlow operations (and neither are the other operations in this\n",
      "code), so they cannot be included in a TensorFlow Function (except by wrapping\n",
      "them in a tf.py_function()  operation, which would make the code slower and less\n",
      "portable, as we saw in Chapter 12 ). Fortunately, TensorFlow does include special pro‐\n",
      "tobuf definitions for which it provides parsing operations.\n",
      "TensorFlow Protobufs\n",
      "The main protobuf typically used in a TFRecord file is the Example  protobuf, which\n",
      "represents one instance in a dataset. It contains a list of named features, where each\n",
      "feature can either be a list of byte strings, a list of floats or a list of integers. Here is the\n",
      "protobuf definition:\n",
      "syntax = \"proto3\" ;\n",
      "message BytesList  { repeated  bytes value = 1; }\n",
      "message FloatList  { repeated  float value = 1 [packed = true]; }\n",
      "message Int64List  { repeated  int64 value = 1 [packed = true]; }\n",
      "416 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "8Why was Example  even defined since it contains no more than a Features  object? Well, TensorFlow may one\n",
      "day decide to add more fields to it. As long as the new Example  definition still contains the features  field,\n",
      "with the same id, it will be backward compatible. This extensibility is one of the great features of protobufs.message Feature {\n",
      "    oneof kind {\n",
      "        BytesList  bytes_list  = 1;\n",
      "        FloatList  float_list  = 2;\n",
      "        Int64List  int64_list  = 3;\n",
      "    }\n",
      "};\n",
      "message Features  { map<string, Feature> feature = 1; };\n",
      "message Example { Features  features  = 1; };\n",
      "The definitions of BytesList , FloatList  and Int64List  are straightforward enough\n",
      "([packed = true]  is used for repeated numerical fields, for a more efficient encod‐\n",
      "ing). A Feature  either contains a BytesList , a FloatList  or an Int64List . A Fea\n",
      "tures  (with an s) contains a dictionary that maps a feature name to the\n",
      "corresponding feature value. And finally, an Example  just contains a Features  object.8\n",
      "Here is how you could create a tf.train.Example  representing the same person as\n",
      "earlier, and write it to TFRecord file:\n",
      "from tensorflow.train  import BytesList , FloatList , Int64List\n",
      "from tensorflow.train  import Feature, Features , Example\n",
      "person_example  = Example(\n",
      "    features =Features (\n",
      "        feature={\n",
      "            \"name\": Feature(bytes_list =BytesList (value=[b\"Alice\"])),\n",
      "            \"id\": Feature(int64_list =Int64List (value=[123])),\n",
      "            \"emails\" : Feature(bytes_list =BytesList (value=[b\"a@b.com\" ,\n",
      "                                                          b \"c@d.com\" ]))\n",
      "        }))\n",
      "The code is a bit verbose and repetitive, but it’s rather straightforward (and you could\n",
      "easily wrap it inside a small helper function). Now that we have an Example  protobuf,\n",
      "we can serialize it by calling its SerializeToString()  method, then write the result‐\n",
      "ing data to a TFRecord file:\n",
      "with tf.io.TFRecordWriter (\"my_contacts.tfrecord\" ) as f:\n",
      "    f.write(person_example .SerializeToString ())\n",
      "Normally you would write much more than just one example! Typically, you would\n",
      "create a conversion script that reads from your current format (say, CSV files), creates\n",
      "an Example  protobuf for each instance, serializes them and saves them to several\n",
      "TFRecord files, ideally shuffling them in the process. This requires a bit of work, so\n",
      "once again make sure it is really necessary (perhaps your pipeline works fine with\n",
      "CSV files).\n",
      "The TFRecord Format | 417\n",
      "Now that we have a nice TFRecord file containing a serialized Example , let’s try to\n",
      "load it.\n",
      "Loading and Parsing Examples\n",
      "To load the serialized Example  protobufs, we will use a tf.data.TFRecordDataset\n",
      "once again, and we will parse each Example  using tf.io.parse_single_example() .\n",
      "This is a TensorFlow operation so it can be included in a TF Function. It requires at\n",
      "least two arguments: a string scalar tensor containing the serialized data, and a\n",
      "description of each feature. The description is a dictionary that maps each feature\n",
      "name to either a tf.io.FixedLenFeature  descriptor indicating the feature’s shape,\n",
      "type and default value, or a tf.io.VarLenFeature  descriptor indicating only the type\n",
      "(if the length may vary, such as for the \"emails\"  feature). For example:\n",
      "feature_description  = {\n",
      "    \"name\": tf.io.FixedLenFeature ([], tf.string, default_value =\"\"),\n",
      "    \"id\": tf.io.FixedLenFeature ([], tf.int64, default_value =0),\n",
      "    \"emails\" : tf.io.VarLenFeature (tf.string),\n",
      "}\n",
      "for serialized_example  in tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]):\n",
      "    parsed_example  = tf.io.parse_single_example (serialized_example ,\n",
      "                                                feature_description )\n",
      "The fixed length features are parsed as regular tensors, but the variable length fea‐\n",
      "tures are parsed as sparse tensors. Y ou can convert a sparse tensor to a dense tensor\n",
      "using tf.sparse.to_dense() , but in this case it is simpler to just access its values:\n",
      ">>> tf.sparse.to_dense (parsed_example [\"emails\" ], default_value =b\"\")\n",
      "<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\n",
      ">>> parsed_example [\"emails\" ].values\n",
      "<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\n",
      "A BytesList  can contain any binary data you want, including any serialized object.\n",
      "For example, you can use tf.io.encode_jpeg()  to encode an image using the JPEG\n",
      "format, and put this binary data in a BytesList . Later, when your code reads the\n",
      "TFRecord, it will start by parsing the Example , then you will need to call\n",
      "tf.io.decode_jpeg()  to parse the data and get the original image (or you can use\n",
      "tf.io.decode_image() , which can decode any BMP , GIF, JPEG or PNG image). Y ou\n",
      "can also store any tensor you want in a BytesList  by serializing the tensor using\n",
      "tf.io.serialize_tensor() , then putting the resulting byte string in a BytesList\n",
      "feature. Later, when you parse the TFRecord, you can parse this data using\n",
      "tf.io.parse_tensor() .\n",
      "Instead of parsing examples one by one using tf.io.parse_single_example() , you\n",
      "may want to parse them batch by batch using tf.io.parse_example() :\n",
      "418 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset = tf.data.TFRecordDataset ([\"my_contacts.tfrecord\" ]).batch(10)\n",
      "for serialized_examples  in dataset:\n",
      "    parsed_examples  = tf.io.parse_example (serialized_examples ,\n",
      "                                          feature_description )\n",
      "As you can see, the Example  proto will probably be sufficient for most use cases.\n",
      "However, it may be a bit cumbersome to use when you are dealing with lists of lists.\n",
      "For example, suppose you want to classify text documents. Each document may be\n",
      "represented as a list of sentences, where each sentence is represented as a list of\n",
      "words. And perhaps each document also has a list of comments, where each com‐\n",
      "ment is also represented as a list of words. Moreover, there may be some contextual\n",
      "data as well, such as the document’s author, title and publication date. TensorFlow’s\n",
      "SequenceExample  protobuf is designed for such use cases.\n",
      "Handling Lists of Lists Using the SequenceExample  Protobuf\n",
      "Here is the definition of the SequenceExample  protobuf:\n",
      "message FeatureList  { repeated  Feature feature = 1; };\n",
      "message FeatureLists  { map<string, FeatureList > feature_list  = 1; };\n",
      "message SequenceExample  {\n",
      "    Features  context = 1;\n",
      "    FeatureLists  feature_lists  = 2;\n",
      "};\n",
      "A SequenceExample  contains a Features  object for the contextual data and a Fea\n",
      "tureLists  object which contains one or more named FeatureList  objects (e.g., a\n",
      "FeatureList  named \"content\"  and another named \"comments\" ). Each FeatureList\n",
      "just contains a list of Feature  objects, each of which may be a list of byte strings, a list\n",
      "of 64-bit integers or a list of floats (in this example, each Feature  would represent a\n",
      "sentence or a comment, perhaps in the form of a list of word identifiers). Building a\n",
      "SequenceExample , serializing it and parsing it is very similar to building, serializing\n",
      "and parsing an Example , but you must use tf.io.parse_single_sequence_exam\n",
      "ple()  to parse a single SequenceExample  or tf.io.parse_sequence_example()  to\n",
      "parse a batch, and both functions return a tuple containing the context features (as a\n",
      "dictionary) and the feature lists (also as a dictionary). If the feature lists contain\n",
      "sequences of varying sizes (as in the example above), you may want to convert them\n",
      "to ragged tensors using tf.RaggedTensor.from_sparse()  (see the notebook for the\n",
      "full code):\n",
      "parsed_context , parsed_feature_lists  = tf.io.parse_single_sequence_example (\n",
      "    serialized_sequence_example , context_feature_descriptions ,\n",
      "    sequence_feature_descriptions )\n",
      "parsed_content  = tf.RaggedTensor .from_sparse (parsed_feature_lists [\"content\" ])\n",
      "Now that you know how to efficiently store, load and parse data, the next step is to\n",
      "prepare it so that it can be fed to a neural network. This means converting all features\n",
      "The TFRecord Format | 419\n",
      "into numerical features (ideally not too sparse), scaling them, and more. In particular,\n",
      "if your data contains categorical features or text features, they need to be converted to\n",
      "numbers. For this, the Features API  can help.\n",
      "The Features API\n",
      "Preprocessing your data can be performed in many ways: it can be done ahead of\n",
      "time when preparing your data files, using any tool you like. Or you can preprocess\n",
      "your data on the fly when loading it with the Data API (e.g., using the dataset’s map()\n",
      "method, as we saw earlier). Or you can include a preprocessing layer directly in your\n",
      "model. Whichever solution you prefer, the Features API can help you: it is a set of\n",
      "functions available in the tf.feature_column  package, which let you define how\n",
      "each feature (or group of features) in your data should be preprocessed (therefore you\n",
      "can think of this API as the analog of Scikit-Learn’s ColumnTransformer  class). We\n",
      "will start by looking at the different types of columns available, and then we will look\n",
      "at how to use them.\n",
      "Let’s go back to the variant of the California housing dataset that we used in Chap‐\n",
      "ter 2 , since it includes a categorical feature and missing data. Here is a simple numeri‐\n",
      "cal column named \"housing_median_age\" :\n",
      "housing_median_age  = tf.feature_column .numeric_column (\"housing_median_age\" )\n",
      "Numeric columns let you specify a normalization function using the normalizer_fn\n",
      "argument. For example, let’s tweak the \"housing_median_age\"  column to define how\n",
      "it should be scaled. Note that this requires computing ahead of time the mean and\n",
      "standard deviation of this feature in the training set:\n",
      "age_mean , age_std = X_mean[1], X_std[1]  # The median age is column in 1\n",
      "housing_median_age  = tf.feature_column .numeric_column (\n",
      "    \"housing_median_age\" , normalizer_fn =lambda x: (x - age_mean ) / age_std)\n",
      "In some cases, it might improve performance to bucketize some numerical features,\n",
      "effectively transforming a numerical feature into a categorical feature. For example,\n",
      "let’s create a bucketized column based on the median_income  column, with 5 buckets:\n",
      "less than 1.5 ($15,000), then 1.5 to 3, 3 to 4.5, 4.5 to 6., and above 6. (notice that when\n",
      "you specify 4 boundaries, there are actually 5 buckets):\n",
      "median_income  = tf.feature_column .numeric_column (\"median_income\" )\n",
      "bucketized_income  = tf.feature_column .bucketized_column (\n",
      "    median_income , boundaries =[1.5, 3., 4.5, 6.])\n",
      "If the median_income  feature is equal to, say, 3.2, then the bucketized_income  feature\n",
      "will automatically be equal to 2 (i.e., the index of the corresponding income bucket).\n",
      "Choosing the right boundaries can be somewhat of an art, but one approach is to just\n",
      "use percentiles of the data (e.g., the 10th percentile, the 20th percentile, and so on). If\n",
      "a feature is multimodal , meaning it has separate peaks in its distribution, you may\n",
      "420 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "want to define a bucket for each mode, placing the boundaries in between the peaks.\n",
      "Whether you use the percentiles or the modes, you need to analyze the distribution of\n",
      "your data ahead of time, just like we had to measure the mean and standard deviation\n",
      "ahead of time to normalize the housing_median_age  column.\n",
      "Categorical Features\n",
      "For categorical features such as ocean_proximity , there are several options. If it is\n",
      "already represented as a category ID (i.e., an integer from 0 to the max ID), then you\n",
      "can use the categorical_column_with_identity()  function (specifying the max\n",
      "ID). If not, and you know the list of all possible categories, then you can use categori\n",
      "cal_column_with_vocabulary_list() :\n",
      "ocean_prox_vocab  = ['<1H OCEAN' , 'INLAND' , 'ISLAND' , 'NEAR BAY' , 'NEAR OCEAN' ]\n",
      "ocean_proximity  = tf.feature_column .categorical_column_with_vocabulary_list (\n",
      "    \"ocean_proximity\" , ocean_prox_vocab )\n",
      "If you prefer to have TensorFlow load the vocabulary from a file, you can call catego\n",
      "rical_column_with_vocabulary_file()  instead. As you might expect, these two\n",
      "functions will simply map each category to its index in the vocabulary (e.g., NEAR\n",
      "BAY  will be mapped to 3), and unknown categories will be mapped to -1.\n",
      "For categorical columns with a large vocabulary (e.g., for zipcodes, cities, words,\n",
      "products, users, etc.), it may not be convenient to get the full list of possible cate‐\n",
      "gories, or perhaps categories may be added or removed so frequently that using cate‐\n",
      "gory indices would be too unreliable. In this case, you may prefer to use a\n",
      "categorical_column_with_hash_bucket() . If we had a \"city\"  feature in the dataset,\n",
      "we could encode it like this:\n",
      "city_hash  = tf.feature_column .categorical_column_with_hash_bucket (\n",
      "    \"city\", hash_bucket_size =1000)\n",
      "This feature will compute a hash for each category (i.e., for each city), modulo the\n",
      "number of hash buckets ( hash_bucket_size ). Y ou must set the number of buckets\n",
      "high enough to avoid getting too many collisions (i.e., different categories ending up\n",
      "in the same bucket), but the higher you set it, the more RAM will be used (by the\n",
      "embedding table, as we will see shortly).\n",
      "Crossed Categorical Features\n",
      "If you suspect that two (or more) categorical features are more meaningful when used\n",
      "jointly, then you can create a crossed column . For example, suppose people are partic‐\n",
      "ularly fond of old houses inland and new houses near the ocean, then it might help to\n",
      "The Features API | 421\n",
      "9Since the housing_median_age  feature was normalized, the boundaries are for normalized ages.create a bucketized column for the housing_median_age  feature9, and cross it with\n",
      "the ocean_proximity  column. The crossed column will compute a hash of every age\n",
      "& ocean proximity combination it comes across, modulo the hash_bucket_size , and\n",
      "this will give it the cross category ID. Y ou may then choose to use only this crossed\n",
      "column in your model, or also include the individual columns.\n",
      "bucketized_age  = tf.feature_column .bucketized_column (\n",
      "    housing_median_age , boundaries =[-1., -0.5, 0., 0.5, 1.]) # age was scaled\n",
      "age_and_ocean_proximity  = tf.feature_column .crossed_column (\n",
      "    [bucketized_age , ocean_proximity ], hash_bucket_size =100)\n",
      "Another common use case for crossed columns is to cross latitude and longitude into\n",
      "a single categorical feature: you start by bucketizing the latitude and longitude, for\n",
      "example into 20 buckets each, then you cross these bucketized features into a loca\n",
      "tion  column. This will create a 20×20 grid over California, and each cell in the grid\n",
      "will correspond to one category:\n",
      "latitude  = tf.feature_column .numeric_column (\"latitude\" )\n",
      "longitude  = tf.feature_column .numeric_column (\"longitude\" )\n",
      "bucketized_latitude  = tf.feature_column .bucketized_column (\n",
      "    latitude , boundaries =list(np.linspace (32., 42., 20 - 1)))\n",
      "bucketized_longitude  = tf.feature_column .bucketized_column (\n",
      "    longitude , boundaries =list(np.linspace (-125., -114., 20 - 1)))\n",
      "location  = tf.feature_column .crossed_column (\n",
      "    [bucketized_latitude , bucketized_longitude ], hash_bucket_size =1000)\n",
      "Encoding Categorical Features Using One-Hot Vectors\n",
      "No matter which option you choose to build a categorical feature (categorical col‐\n",
      "umns, bucketized columns or crossed columns), it must be encoded before you can\n",
      "feed it to a neural network. There are two options to encode a categorical feature:\n",
      "one-hot vectors or embeddings . For the first option, simply use the indicator_col\n",
      "umn()  function:\n",
      "ocean_proximity_one_hot  = tf.feature_column .indicator_column (ocean_proximity )\n",
      "A one-hot vector encoding has the size of the vocabulary length, which is fine if there\n",
      "are just a few possible categories, but if the vocabulary is large, you will end up with\n",
      "too many inputs fed to your neural network: it will have too many weights to learn\n",
      "and it will probably not perform very well. In particular, this will typically be the case\n",
      "when you use hash buckets. In this case, you should probably encode them using\n",
      "embeddings  instead.\n",
      "422 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "As a rule of thumb (but your mileage may vary!), if the number of\n",
      "categories is lower than 10, then one-hot encoding is generally the\n",
      "way to go. If the number of categories is greater than 50 (which is\n",
      "often the case when you use hash buckets), then embeddings are\n",
      "usually preferable. In between 10 and 50 categories, you may want\n",
      "to experiment with both options and see which one works best for\n",
      "your use case. Also, embeddings typically require more training\n",
      "data, unless you can reuse pretrained embeddings.\n",
      "Encoding Categorical Features Using Embeddings\n",
      "An embedding is a trainable dense vector that represents a category. By default,\n",
      "embeddings are initialized randomly, so for example the \"NEAR BAY\"  category could\n",
      "be represented initially by a random vector such as [0.131, 0.890] , while the \"NEAR\n",
      "OCEAN\"  category may be represented by another random vector such as [0.631,\n",
      "0.791]  (in this example, we are using 2D embeddings, but the number of dimensions\n",
      "is a hyperparameter you can tweak). Since these embeddings are trainable, they will\n",
      "gradually improve during training, and as they represent fairly similar categories,\n",
      "Gradient Descent will certainly end up pushing them closer together, while it will\n",
      "tend to move them away from the \"INLAND\"  category’s embedding (see Figure 13-4 ).\n",
      "Indeed, the better the representation, the easier it will be for the neural network to\n",
      "make accurate predictions, so training tends to make embeddings useful representa‐\n",
      "tions of the categories. This is called representation learning  (we will see other types of\n",
      "representation learning in ???).\n",
      "The Features API | 423\n",
      "10“Distributed Representations of Words and Phrases and their Compositionality” , T. Mikolov et al. (2013).\n",
      "Figure 13-4. Embeddings Will Gradually Improve During Training\n",
      "Word Embeddings\n",
      "Not only will embeddings generally be useful representations for the task at hand, but\n",
      "quite often these same embeddings can be reused successfully for other tasks as well.\n",
      "The most common example of this is word embeddings  (i.e., embeddings of individual\n",
      "words): when you are working on a natural language processing task, you are often\n",
      "better off reusing pretrained word embeddings than training your own. The idea of\n",
      "using vectors to represent words dates back to the 1960s, and many sophisticated\n",
      "techniques have been used to generate useful vectors, including using neural net‐\n",
      "works, but things really took off in 2013, when Tomáš Mikolov and other Google\n",
      "researchers published a paper10 describing how to learn word embeddings using deep\n",
      "neural networks, much faster than previous attempts. This allowed them to learn\n",
      "embeddings on a very large corpus of text: they trained a deep neural network to pre‐\n",
      "dict the words near any given word. This allowed them to obtain astounding word\n",
      "embeddings. For example, synonyms had very close embeddings, and semantically\n",
      "related words such as France, Spain, Italy, and so on, ended up clustered together. But\n",
      "it’s not just about proximity: word embeddings were also organized along meaningful\n",
      "axes in the embedding space. Here is a famous example: if you compute King – Man\n",
      "+ Woman (adding and subtracting the embedding vectors of these words), then the\n",
      "result will be very close to the embedding of the word Queen (see Figure 13-5 ). In\n",
      "other words, the word embeddings encode the concept of gender! Similarly, you can\n",
      "compute Madrid – Spain + France, and of course the result is close to Paris, which\n",
      "seems to show that the notion of capital city was also encoded in the embeddings.\n",
      "424 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "Figure 13-5. Word Embeddings\n",
      "Let’s go back to the Features API. Here is how you could encode the ocean_proxim\n",
      "ity categories as 2D embeddings:\n",
      "ocean_proximity_embed  = tf.feature_column .embedding_column (ocean_proximity ,\n",
      "                                                           dimension =2)\n",
      "Each of the five ocean_proximity  categories will now be represented as a 2D vector.\n",
      "These vectors are stored in an embedding matrix  with one row per category, and one\n",
      "column per embedding dimension, so in this example it is a 5×2 matrix. When an\n",
      "embedding column is given a category index as input (say, 3, which corresponds to\n",
      "the category \"NEAR BAY\" ), it just performs a lookup in the embedding matrix and\n",
      "returns the corresponding row (say, [0.331, 0.190] ). Unfortunately, the embedding\n",
      "matrix can be quite large, especially when you have a large vocabulary: if this is the\n",
      "case, the model can only learn good representations for the categories for which it has\n",
      "sufficient training data. To reduce the size of the embedding matrix, you can of\n",
      "course try lowering the dimension  hyperparameter, but if you reduce this parameter\n",
      "too much, the representations may not be as good. Another option is to reduce the\n",
      "vocabulary size (e.g., if you are dealing with text, you can try dropping the rare words\n",
      "from the vocabulary, and replace them all with a token like \"<unknown>\"  or \"<UNK>\" ).\n",
      "If you are using hash buckets, you can also try reducing the hash_bucket_size  (but\n",
      "not too much, or else you will get collisions).\n",
      "The Features API | 425\n",
      "If there are no pretrained embeddings that you can reuse for the\n",
      "task you are trying to tackle, and if you do not have enough train‐\n",
      "ing data to learn them, then you can try to learn them on some\n",
      "auxiliary task for which it is easier to obtain plenty of training data.\n",
      "After that, you can reuse the trained embeddings for your main\n",
      "task.\n",
      "Using Feature Columns for Parsing\n",
      "Let’s suppose you have created feature columns for each of your input features, as well\n",
      "as for the target. What can you do with them? Well, for one you can pass them to the\n",
      "make_parse_example_spec()  function to generate feature descriptions (so you don’t\n",
      "have to do it manually, as we did earlier):\n",
      "columns = [bucketized_age , ....., median_house_value ] # all features + target\n",
      "feature_descriptions  = tf.feature_column .make_parse_example_spec (columns)\n",
      "Y ou don’t always have to create a separate feature column for each\n",
      "and every feature. For example, instead of having 2 numerical fea‐\n",
      "ture columns, you could choose to have a single 2D column: just\n",
      "set shape=[2]  when calling numerical_column() .\n",
      "Y ou can then create a function that parses serialized examples using these feature\n",
      "descriptions, and separates the target column from the input features:\n",
      "def parse_examples (serialized_examples ):\n",
      "    examples  = tf.io.parse_example (serialized_examples , feature_descriptions )\n",
      "    targets = examples .pop(\"median_house_value\" ) # separate the targets\n",
      "    return examples , targets\n",
      "Next, you can create a TFRecordDataset  that will read batches of serialized examples\n",
      "(assuming the TFRecord file contains serialized Example  protobufs with the appropri‐\n",
      "ate features):\n",
      "batch_size  = 32\n",
      "dataset = tf.data.TFRecordDataset ([\"my_data_with_features.tfrecords\" ])\n",
      "dataset = dataset.repeat().shuffle(10000).batch(batch_size ).map(parse_examples )\n",
      "Using Feature Columns in Your Models\n",
      "Feature columns can also be used directly in your model, to convert all your input\n",
      "features into a single dense vector which the neural network can then process. For\n",
      "this, all you need to do is add a keras.layers.DenseFeatures  layer as the first layer\n",
      "in your model, passing it the list of feature columns (excluding the target column):\n",
      "columns_without_target  = columns[:-1]\n",
      "model = keras.models.Sequential ([\n",
      "    keras.layers.DenseFeatures (feature_columns =columns_without_target ),\n",
      "426 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "    keras.layers.Dense(1)\n",
      "])\n",
      "model.compile(loss=\"mse\", optimizer =\"sgd\", metrics=[\"accuracy\" ])\n",
      "steps_per_epoch  = len(X_train) // batch_size\n",
      "history = model.fit(dataset, steps_per_epoch =steps_per_epoch , epochs=5)\n",
      "The DenseFeatures  layer will take care of converting every input feature to a dense\n",
      "representation, and it will also apply any extra transformation we specified, such as\n",
      "scaling the housing_median_age  using the normalizer_fn  function we provided. Y ou\n",
      "can take a closer look at what the DenseFeatures  layer does by calling it directly:\n",
      ">>> some_columns  = [ocean_proximity_embed , bucketized_income ]\n",
      ">>> dense_features  = keras.layers.DenseFeatures (some_columns )\n",
      ">>> dense_features ({\n",
      "...     \"ocean_proximity\" : [[\"NEAR OCEAN\" ], [\"INLAND\" ], [\"INLAND\" ]],\n",
      "...     \"median_income\" : [[3.], [7.2], [1.]]\n",
      "... })\n",
      "...\n",
      "<tf.Tensor: id=559790, shape=(3, 7), dtype=float32, numpy=\n",
      "array([[ 0. , 0. , 1. , 0. , 0. ,-0.36277947 , 0.30109018],\n",
      "       [ 0. , 0. , 0. , 0. , 1. , 0.22548223 , 0.33142096],\n",
      "       [ 1. , 0. , 0. , 0. , 0. , 0.22548223 , 0.33142096]], dtype=float32)>\n",
      "In this example, we create a DenseFeatures  layer with just two columns, and we call\n",
      "it with some data, in the form of a dictionary of features. In this case, since the bucke\n",
      "tized_income  column relies on the median_income  column, the dictionary must\n",
      "include the \"median_income\"  key, and similarly since the ocean_proximity_embed\n",
      "column is based on the ocean_proximity  column, the dictionary must include the\n",
      "\"ocean_proximity\"  key. Columns are handled in alphabetical order, so first we look\n",
      "at the bucketized income column (its name is the same as the median_income  column\n",
      "name, plus \"_bucketized\" ). The incomes 3, 7.2 and 1 get mapped respectively to cat‐\n",
      "egory 2 (for incomes between 1.5 and 3), category 0 (for incomes below 1.5), and cat‐\n",
      "egory 4 (for incomes greater than 6). Then these category IDs get one-hot encoded:\n",
      "category 2 gets encoded as [0., 0., 1., 0., 0.]  and so on (note that bucketized\n",
      "columns get one-hot encoded by default, no need to call indicator_column() ). Now\n",
      "on to the ocean_proximity_embed  column. The \"NEAR OCEAN\"  and \"INLAND\"  cate‐\n",
      "gories just get mapped to their respective embeddings (which were initialized ran‐\n",
      "domly). The resulting tensor is the concatenation of the one-hot vectors and the\n",
      "embeddings.\n",
      "Now you can feed all kinds of features to a neural network, including numerical fea‐\n",
      "tures, categorical features, and even text (by splitting the text into words, then using\n",
      "word embedding)! However, performing all the preprocessing on the fly can slow\n",
      "down training. Let’s see how this can be improved.\n",
      "The Features API | 427\n",
      "TF Transform\n",
      "If preprocessing is computationally expensive, then handling it before training rather\n",
      "than on the fly may give you a significant speedup: the data will be preprocessed just\n",
      "once per instance before  training, rather than once per instance and per epoch during\n",
      "training. Tools like Apache Beam let you run efficient data processing pipelines over\n",
      "large amounts of data, even distributed across multiple servers, so why not use it to\n",
      "preprocess all the training data? This works great and indeed can speed up training,\n",
      "but there is one problem: once your model is trained, suppose you want to deploy it\n",
      "to a mobile app: you will need to write some code in your app to take care of prepro‐\n",
      "cessing the data before it is fed to the model. And suppose you also want to deploy\n",
      "the model to TensorFlow.js so it runs in a web browser? Once again, you will need to\n",
      "write some preprocessing code. This can become a maintenance nightmare: when‐\n",
      "ever you want to change the preprocessing logic, you will need to update your Apache\n",
      "Beam code, your mobile app code and your Javascript code. It is not only time con‐\n",
      "suming, but also error prone: you may end up with subtle differences between the\n",
      "preprocessing operations performed before training and the ones performed in your\n",
      "app or in the browser. This training/serving skew  will lead to bugs or degraded perfor‐\n",
      "mance.\n",
      "One improvement would be to take the trained model (trained on data that was pre‐\n",
      "processed by your Apache Beam code), and before deploying it to your app or the\n",
      "browser, add an extra input layer to take care of preprocessing on the fly (either by\n",
      "writing a custom layer or by using a DenseFeatures  layer). That’s definitely better,\n",
      "since now you just have two versions of your preprocessing code: the Apache Beam\n",
      "code and the preprocessing layer’s code.\n",
      "But what if you could define your preprocessing operations just once? This is what\n",
      "TF Transform was designed for. It is part of TensorFlow Extended  (TFX), an end-to-\n",
      "end platform for productionizing TensorFlow models. First, to use a TFX component,\n",
      "such as TF Transform, you must install it, it does not come bundled with TensorFlow.\n",
      "Y ou define your preprocessing function just once (in Python), by using TF Transform\n",
      "functions for scaling, bucketizing, crossing features, and more. Y ou can also use any\n",
      "TensorFlow operation you need. Here is what this preprocessing function might look\n",
      "like if we just had two features:\n",
      "import tensorflow_transform  as tft\n",
      "def preprocess (inputs):  # inputs is a batch of input features\n",
      "    median_age  = inputs[\"housing_median_age\" ]\n",
      "    ocean_proximity  = inputs[\"ocean_proximity\" ]\n",
      "    standardized_age  = tft.scale_to_z_score (median_age  - tft.mean(median_age ))\n",
      "    ocean_proximity_id  = tft.compute_and_apply_vocabulary (ocean_proximity )\n",
      "    return {\n",
      "        \"standardized_median_age\" : standardized_age ,\n",
      "428 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "11At the time of writing, TFDS requires you to download a few files manually for ImageNet (for legal reasons),\n",
      "but this will hopefully get resolved soon.\n",
      "        \"ocean_proximity_id\" : ocean_proximity_id\n",
      "    }\n",
      "Next, TF Transform lets you apply this preprocess()  function to the whole training\n",
      "set using Apache Beam (it provides an AnalyzeAndTransformDataset  class that you\n",
      "can use for this purpose in your Apache Beam pipeline). In the process, it will also\n",
      "compute all the necessary statistics over the whole training set: in this example, the\n",
      "mean and standard deviation of the housing_median_age  feature, and the vocabulary\n",
      "for the ocean_proximity  feature. The components that compute these statistics are\n",
      "called analyzers .\n",
      "Importantly, TF Transform will also generate an equivalent TensorFlow Function that\n",
      "you can plug into the model you deploy. This TF Function contains all the necessary\n",
      "statistics computed by Apache Beam (the mean, standard deviation, and vocabulary),\n",
      "simply included as constants.\n",
      "At the time of this writing, TF Transform only supports Tensor‐\n",
      "Flow 1. Moreover, Apache Beam only has partial support for\n",
      "Python 3. That said, both these limitations will likely be fixed by\n",
      "the time your read this.\n",
      "With the Data API, TFRecords, the Features API and TF Transform, you can build\n",
      "highly scalable input pipelines for training, and also benefit from fast and portable\n",
      "data preprocessing in production.\n",
      "But what if you just wanted to use a standard dataset? Well in that case, things are\n",
      "much simpler: just use TFDS!\n",
      "The TensorFlow Datasets (TFDS) Project\n",
      "The TensorFlow Datasets  project makes it trivial to download common datasets, from\n",
      "small ones like MNIST or Fashion MNIST, to huge datasets like ImageNet11 (you will\n",
      "need quite a bit of disk space!). The list includes image datasets, text datasets (includ‐\n",
      "ing translation datasets), audio and video datasets, and more. Y ou can visit https://\n",
      "homl.info/tfds  to view the full list, along with a description of each dataset.\n",
      "TFDS is not bundled with TensorFlow, so you need to install the tensorflow-\n",
      "datasets  library (e.g., using pip). Then all you need to do is call the tfds.load()\n",
      "function, and it will download the data you want (unless it was already downloaded\n",
      "earlier), and return the data as a dictionary of Datasets  (typically one for training,\n",
      "The TensorFlow Datasets (TFDS) Project | 429\n",
      "and one for testing, but this depends on the dataset you choose). For example, let’s\n",
      "download MNIST:\n",
      "import tensorflow_datasets  as tfds\n",
      "dataset = tfds.load(name=\"mnist\")\n",
      "mnist_train , mnist_test  = dataset[\"train\"], dataset[\"test\"]\n",
      "Y ou can then apply any transformation you want (typically repeating, batching and\n",
      "prefetching), and you’re ready to train your model. Here is a simple example:\n",
      "mnist_train  = mnist_train .repeat(5).batch(32).prefetch (1)\n",
      "for item in mnist_train :\n",
      "    images = item[\"image\"]\n",
      "    labels = item[\"label\"]\n",
      "    [...]\n",
      "In general, load()  returns a shuffled training set, so there’s no need\n",
      "to shuffle it some more.\n",
      "Note that each item in the dataset is a dictionary containing both the features and the\n",
      "labels. But Keras expects each item to be a tuple containing 2 elements (again, the fea‐\n",
      "tures and the labels). Y ou could transform the dataset using the map()  method, like\n",
      "this:\n",
      "mnist_train  = mnist_train .repeat(5).batch(32)\n",
      "mnist_train  = mnist_train .map(lambda items: (items[\"image\"], items[\"label\"]))\n",
      "mnist_train  = mnist_train .prefetch (1)\n",
      "Or you can just ask the load()  function to do this for you by setting as_super\n",
      "vised=True  (obviously this works only for labeled datasets). Y ou can also specify the\n",
      "batch size if you want. Then the dataset can be passed directly to your tf.keras model:\n",
      "dataset = tfds.load(name=\"mnist\", batch_size =32, as_supervised =True)\n",
      "mnist_train  = dataset[\"train\"].repeat().prefetch (1)\n",
      "model = keras.models.Sequential ([...])\n",
      "model.compile(loss=\"sparse_categorical_crossentropy\" , optimizer =\"sgd\")\n",
      "model.fit(mnist_train , steps_per_epoch =60000 // 32, epochs=5)\n",
      "This was quite a technical chapter, and you may feel that it is a bit far from the\n",
      "abstract beauty of neural networks, but the fact is deep learning often involves large\n",
      "amounts of data, and knowing how to load, parse and preprocess it efficiently is a\n",
      "crucial skill to have. In the next chapter, we will look at Convolutional Neural Net‐\n",
      "works, which are among the most successful neural net architectures for image pro‐\n",
      "cessing, and many other applications.\n",
      "430 | Chapter 13: Loading and Preprocessing Data with TensorFlow\n",
      "CHAPTER 14\n",
      "Deep Computer Vision Using Convolutional\n",
      "Neural Networks\n",
      "With Early Release ebooks, you get books in their earliest form—\n",
      "the author’s raw and unedited content as he or she writes—so you\n",
      "can take advantage of these technologies long before the official\n",
      "release of these titles. The following will be Chapter 14 in the final\n",
      "release of the book.\n",
      "Although IBM’s Deep Blue supercomputer beat the chess world champion Garry Kas‐\n",
      "parov back in 1996, it wasn’t until fairly recently that computers were able to reliably\n",
      "perform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\n",
      "spoken words. Why are these tasks so effortless to us humans? The answer lies in the\n",
      "fact that perception largely takes place outside the realm of our consciousness, within\n",
      "specialized visual, auditory, and other sensory modules in our brains. By the time\n",
      "sensory information reaches our consciousness, it is already adorned with high-level\n",
      "features; for example, when you look at a picture of a cute puppy, you cannot choose\n",
      "not to see the puppy, or not to notice its cuteness. Nor can you explain how you rec‐\n",
      "ognize a cute puppy; it’s just obvious to you. Thus, we cannot trust our subjective\n",
      "experience: perception is not trivial at all, and to understand it we must look at how\n",
      "the sensory modules work.\n",
      "Convolutional neural networks (CNNs) emerged from the study of the brain’s visual\n",
      "cortex, and they have been used in image recognition since the 1980s. In the last few\n",
      "years, thanks to the increase in computational power, the amount of available training\n",
      "data, and the tricks presented in Chapter 11  for training deep nets, CNNs have man‐\n",
      "aged to achieve superhuman performance on some complex visual tasks. They power\n",
      "image search services, self-driving cars, automatic video classification systems, and\n",
      "more. Moreover, CNNs are not restricted to visual perception: they are also successful\n",
      "431\n",
      "1“Single Unit Activity in Striate Cortex of Unrestrained Cats, ” D. Hubel and T. Wiesel (1958).\n",
      "2“Receptive Fields of Single Neurones in the Cat’s Striate Cortex, ” D. Hubel and T. Wiesel (1959).\n",
      "3“Receptive Fields and Functional Architecture of Monkey Striate Cortex, ” D. Hubel and T. Wiesel (1968).at many other tasks, such as voice recognition  or natural language processing  (NLP);\n",
      "however, we will focus on visual applications for now.\n",
      "In this chapter we will present where CNNs came from, what their building blocks\n",
      "look like, and how to implement them using TensorFlow and Keras. Then we will dis‐\n",
      "cuss some of the best CNN architectures, and discuss other visual tasks, including\n",
      "object detection  (classifying multiple objects in an image and placing bounding boxes\n",
      "around them) and semantic segmentation  (classifying each pixel according to the class\n",
      "of the object it belongs to).\n",
      "The Architecture of the Visual Cortex\n",
      "David H. Hubel and Torsten Wiesel performed a series of experiments on cats in\n",
      "19581 and 19592 (and a few years later on monkeys3), giving crucial insights on the\n",
      "structure of the visual cortex (the authors received the Nobel Prize in Physiology or\n",
      "Medicine in 1981 for their work). In particular, they showed that many neurons in\n",
      "the visual cortex have a small local receptive field, meaning they react only to visual\n",
      "stimuli located in a limited region of the visual field (see Figure 14-1 , in which the\n",
      "local receptive fields of five neurons are represented by dashed circles). The receptive\n",
      "fields of different neurons may overlap, and together they tile the whole visual field.\n",
      "Moreover, the authors showed that some neurons react only to images of horizontal\n",
      "lines, while others react only to lines with different orientations (two neurons may\n",
      "have the same receptive field but react to different line orientations). They also\n",
      "noticed that some neurons have larger receptive fields, and they react to more com‐\n",
      "plex patterns that are combinations of the lower-level patterns. These observations\n",
      "led to the idea that the higher-level neurons are based on the outputs of neighboring\n",
      "lower-level neurons (in Figure 14-1 , notice that each neuron is connected only to a\n",
      "few neurons from the previous layer). This powerful architecture is able to detect all\n",
      "sorts of complex patterns in any area of the visual field.\n",
      "432 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "4“Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected\n",
      "by Shift in Position, ” K. Fukushima (1980).\n",
      "5“Gradient-Based Learning Applied to Document Recognition, ” Y . LeCun et al. (1998).\n",
      "Figure 14-1. Local receptive fields  in the visual cortex\n",
      "These studies of the visual cortex inspired the neocognitron, introduced in 1980 ,4\n",
      "which gradually evolved into what we now call convolutional neural networks . An\n",
      "important milestone was a 1998 paper5 by Y ann LeCun, Léon Bottou, Y oshua Bengio,\n",
      "and Patrick Haffner, which introduced the famous LeNet-5  architecture, widely used\n",
      "to recognize handwritten check numbers. This architecture has some building blocks\n",
      "that you already know, such as fully connected layers and sigmoid activation func‐\n",
      "tions, but it also introduces two new building blocks: convolutional layers  and pooling\n",
      "layers . Let’s look at them now.\n",
      "Why not simply use a regular deep neural network with fully con‐\n",
      "nected layers for image recognition tasks? Unfortunately, although\n",
      "this works fine for small images (e.g., MNIST), it breaks down for\n",
      "larger images because of the huge number of parameters it\n",
      "requires. For example, a 100 × 100 image has 10,000 pixels, and if\n",
      "the first layer has just 1,000 neurons (which already severely\n",
      "restricts the amount of information transmitted to the next layer),\n",
      "this means a total of 10 million connections. And that’s just the first\n",
      "layer. CNNs solve this problem using partially connected layers and\n",
      "weight sharing.\n",
      "The Architecture of the Visual Cortex | 433\n",
      "6A convolution is a mathematical operation that slides one function over another and measures the integral of\n",
      "their pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform,\n",
      "and is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\n",
      "similar to convolutions (see https://homl.info/76  for more details).\n",
      "Convolutional Layer\n",
      "The most important building block of a CNN is the convolutional layer :6 neurons in\n",
      "the first convolutional layer are not connected to every single pixel in the input image\n",
      "(like they were in previous chapters), but only to pixels in their receptive fields (see\n",
      "Figure 14-2 ). In turn, each neuron in the second convolutional layer is connected\n",
      "only to neurons located within a small rectangle in the first layer. This architecture\n",
      "allows the network to concentrate on small low-level features in the first hidden layer,\n",
      "then assemble them into larger higher-level features in the next hidden layer, and so\n",
      "on. This hierarchical structure is common in real-world images, which is one of the\n",
      "reasons why CNNs work so well for image recognition.\n",
      "Figure 14-2. CNN layers with rectangular local receptive fields\n",
      "Until now, all multilayer neural networks we looked at had layers\n",
      "composed of a long line of neurons, and we had to flatten input\n",
      "images to 1D before feeding them to the neural network. Now each\n",
      "layer is represented in 2D, which makes it easier to match neurons\n",
      "with their corresponding inputs.\n",
      "A neuron located in row i, column j of a given layer is connected to the outputs of the\n",
      "neurons in the previous layer located in rows i to i + fh – 1, columns j to j + fw – 1,\n",
      "where fh and fw are the height and width of the receptive field (see Figure 14-3 ). In\n",
      "order for a layer to have the same height and width as the previous layer, it is com‐\n",
      "434 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mon to add zeros around the inputs, as shown in the diagram. This is called zero pad‐\n",
      "ding.\n",
      "Figure 14-3. Connections between layers and zero padding\n",
      "It is also possible to connect a large input layer to a much smaller layer by spacing out\n",
      "the receptive fields, as shown in Figure 14-4 . The shift from one receptive field to the\n",
      "next is called the stride . In the diagram, a 5 × 7 input layer (plus zero padding) is con‐\n",
      "nected to a 3 × 4 layer, using 3 × 3 receptive fields and a stride of 2 (in this example\n",
      "the stride is the same in both directions, but it does not have to be so). A neuron loca‐\n",
      "ted in row i, column j in the upper layer is connected to the outputs of the neurons in\n",
      "the previous layer located in rows i × sh to i × sh + fh – 1, columns j × sw to j × sw + fw –\n",
      "1, where sh and sw are the vertical and horizontal strides.\n",
      "Convolutional Layer | 435\n",
      "Figure 14-4. Reducing dimensionality using a stride of 2\n",
      "Filters\n",
      "A neuron’s weights can be represented as a small image the size of the receptive field.\n",
      "For example, Figure 14-5  shows two possible sets of weights, called filters  (or convolu‐\n",
      "tion kernels ). The first one is represented as a black square with a vertical white line in\n",
      "the middle (it is a 7 × 7 matrix full of 0s except for the central column, which is full of\n",
      "1s); neurons using these weights will ignore everything in their receptive field except\n",
      "for the central vertical line (since all inputs will get multiplied by 0, except for the\n",
      "ones located in the central vertical line). The second filter is a black square with a\n",
      "horizontal white line in the middle. Once again, neurons using these weights will\n",
      "ignore everything in their receptive field except for the central horizontal line.\n",
      "Now if all neurons in a layer use the same vertical line filter (and the same bias term),\n",
      "and you feed the network the input image shown in Figure 14-5  (bottom image), the\n",
      "layer will output the top-left image. Notice that the vertical white lines get enhanced\n",
      "while the rest gets blurred. Similarly, the upper-right image is what you get if all neu‐\n",
      "rons use the same horizontal line filter; notice that the horizontal white lines get\n",
      "enhanced while the rest is blurred out. Thus, a layer full of neurons using the same\n",
      "filter outputs a feature map , which highlights the areas in an image that activate the\n",
      "filter the most. Of course you do not have to define the filters manually: instead, dur‐\n",
      "ing training the convolutional layer will automatically learn the most useful filters for\n",
      "its task, and the layers above will learn to combine them into more complex patterns.\n",
      "436 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "Figure 14-5. Applying two different  filters  to get two feature maps\n",
      "Stacking Multiple Feature Maps\n",
      "Up to now, for simplicity, I have represented the output of each convolutional layer as\n",
      "a thin 2D layer, but in reality a convolutional layer has multiple filters (you decide\n",
      "how many), and it outputs one feature map per filter, so it is more accurately repre‐\n",
      "sented in 3D (see Figure 14-6 ). To do so, it has one neuron per pixel in each feature\n",
      "map, and all neurons within a given feature map share the same parameters (i.e., the\n",
      "same weights and bias term). However, neurons in different feature maps use differ‐\n",
      "ent parameters. A neuron’s receptive field is the same as described earlier, but it\n",
      "extends across all the previous layers’ feature maps. In short, a convolutional layer\n",
      "simultaneously applies multiple trainable filters to its inputs, making it capable of\n",
      "detecting multiple features anywhere in its inputs.\n",
      "The fact that all neurons in a feature map share the same parame‐\n",
      "ters dramatically reduces the number of parameters in the model.\n",
      "Moreover, once the CNN has learned to recognize a pattern in one\n",
      "location, it can recognize it in any other location. In contrast, once\n",
      "a regular DNN has learned to recognize a pattern in one location, it\n",
      "can recognize it only in that particular location.\n",
      "Moreover, input images are also composed of multiple sublayers: one per color chan‐\n",
      "nel. There are typically three: red, green, and blue (RGB). Grayscale images have just\n",
      "Convolutional Layer | 437\n",
      "one channel, but some images may have much more—for example, satellite images\n",
      "that capture extra light frequencies (such as infrared).\n",
      "Figure 14-6. Convolution layers with multiple feature maps, and images with three color\n",
      "channels\n",
      "Specifically, a neuron located in row i, column j of the feature map k in a given convo‐\n",
      "lutional layer l is connected to the outputs of the neurons in the previous layer l – 1,\n",
      "located in rows i × sh to i × sh + fh – 1 and columns j × sw to j × sw + fw – 1, across all\n",
      "feature maps (in layer l – 1). Note that all neurons located in the same row i and col‐\n",
      "umn j but in different feature maps are connected to the outputs of the exact same\n",
      "neurons in the previous layer.\n",
      "Equation 14-1  summarizes the preceding explanations in one big mathematical equa‐\n",
      "tion: it shows how to compute the output of a given neuron in a convolutional layer.\n",
      "438 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "It is a bit ugly due to all the different indices, but all it does is calculate the weighted\n",
      "sum of all the inputs, plus the bias term.\n",
      "Equation 14-1. Computing the output of a neuron in a convolutional layer\n",
      "zi,j,k=bk+∑\n",
      "u= 0fh− 1\n",
      "∑\n",
      "v= 0fw− 1\n",
      "∑\n",
      "k′= 0fn′− 1\n",
      "xi′,j′,k′.wu,v,k′,kwithi′=i×sh+u\n",
      "j′=j×sw+v\n",
      "•zi, j, k is the output of the neuron located in row i, column j in feature map k of the\n",
      "convolutional layer (layer l).\n",
      "•As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\n",
      "the height and width of the receptive field, and fn′ is the number of feature maps\n",
      "in the previous layer (layer l – 1).\n",
      "•xi′, j′, k′ is the output of the neuron located in layer l – 1, row i′, column j′, feature\n",
      "map k′ (or channel k′ if the previous layer is the input layer).\n",
      "•bk is the bias term for feature map k (in layer l). Y ou can think of it as a knob that\n",
      "tweaks the overall brightness of the feature map k.\n",
      "•wu, v, k′ ,k is the connection weight between any neuron in feature map k of the layer\n",
      "l and its input located at row u, column v (relative to the neuron’s receptive field),\n",
      "and feature map k′.\n",
      "TensorFlow Implementation\n",
      "In TensorFlow, each input image is typically represented as a 3D tensor of shape\n",
      "[height, width, channels] . A mini-batch is represented as a 4D tensor of shape\n",
      "[mini-batch size, height, width, channels] . The weights of a convolutional\n",
      "layer are represented as a 4D tensor of shape [ fh, fw, fn′, fn]. The bias terms of a convo‐\n",
      "lutional layer are simply represented as a 1D tensor of shape [ fn].\n",
      "Let’s look at a simple example. The following code loads two sample images, using\n",
      "Scikit-Learn’s load_sample_images()  (which loads two color images, one of a Chi‐\n",
      "nese temple, and the other of a flower). The pixel intensities (for each color channel)\n",
      "is represented as a byte from 0 to 255, so we scale these features simply by dividing by\n",
      "255, to get floats ranging from 0 to 1. Then we create two 7 × 7 filters (one with a\n",
      "vertical white line in the middle, and the other with a horizontal white line in the\n",
      "middle), and we apply them to both images using the tf.nn.conv2d()  function,\n",
      "which is part of TensorFlow’s low-level Deep Learning API. In this example, we use\n",
      "zero padding ( padding=\"SAME\" ) and a stride of 2. Finally, we plot one of the resulting\n",
      "feature maps (similar to the top-right image in Figure 14-5 ).\n",
      "Convolutional Layer | 439\n",
      "from sklearn.datasets  import load_sample_image\n",
      "# Load sample images\n",
      "china = load_sample_image (\"china.jpg\" ) / 255\n",
      "flower = load_sample_image (\"flower.jpg\" ) / 255\n",
      "images = np.array([china, flower])\n",
      "batch_size , height, width, channels  = images.shape\n",
      "# Create 2 filters\n",
      "filters = np.zeros(shape=(7, 7, channels , 2), dtype=np.float32)\n",
      "filters[:, 3, :, 0] = 1  # vertical line\n",
      "filters[3, :, :, 1] = 1  # horizontal line\n",
      "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n",
      "plt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image's 2nd feature map\n",
      "plt.show()\n",
      "Most of this code is self-explanatory, but the tf.nn.conv2d()  line deserves a bit of\n",
      "explanation:\n",
      "•images  is the input mini-batch (a 4D tensor, as explained earlier).\n",
      "•filters  is the set of filters to apply (also a 4D tensor, as explained earlier).\n",
      "•strides  is equal to 1, but it could also be a 1D array with 4 elements, where the\n",
      "two central elements are the vertical and horizontal strides ( sh and sw). The first\n",
      "and last elements must currently be equal to 1. They may one day be used to\n",
      "specify a batch stride (to skip some instances) and a channel stride (to skip some\n",
      "of the previous layer’s feature maps or channels).\n",
      "•padding  must be either \"VALID\"  or \"SAME\" :\n",
      "—If set to \"VALID\" , the convolutional layer does not use zero padding, and may\n",
      "ignore some rows and columns at the bottom and right of the input image,\n",
      "depending on the stride, as shown in Figure 14-7  (for simplicity, only the hor‐\n",
      "izontal dimension is shown here, but of course the same logic applies to the\n",
      "vertical dimension).\n",
      "—If set to \"SAME\" , the convolutional layer uses zero padding if necessary. In this\n",
      "case, the number of output neurons is equal to the number of input neurons\n",
      "divided by the stride, rounded up (in this example, 13 / 5 = 2.6, rounded up to\n",
      "3). Then zeros are added as evenly as possible around the inputs.\n",
      "440 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "Figure 14-7. Padding options—input width: 13, filter  width: 6, stride: 5\n",
      "In this example, we manually defined the filters, but in a real CNN you would nor‐\n",
      "mally define filters as trainable variables, so the neural net can learn which filters\n",
      "work best, as explained earlier. Instead of manually creating the variables, however,\n",
      "you can simply use the keras.layers.Conv2D  layer:\n",
      "conv = keras.layers.Conv2D(filters=32, kernel_size =3, strides=1,\n",
      "                           padding=\"SAME\", activation =\"relu\")\n",
      "This code creates a Conv2D  layer with 32 filters, each 3 × 3, using a stride of 1 (both\n",
      "horizontally and vertically), SAME padding, and applying the ReLU activation func‐\n",
      "tion to its outputs. As you can see, convolutional layers have quite a few hyperpara‐\n",
      "meters: you must choose the number of filters, their height and width, the strides, and\n",
      "the padding type. As always, you can use cross-validation to find the right hyperpara‐\n",
      "meter values, but this is very time-consuming. We will discuss common CNN archi‐\n",
      "tectures later, to give you some idea of what hyperparameter values work best in \n",
      "practice.\n",
      "Memory Requirements\n",
      "Another problem with CNNs is that the convolutional layers require a huge amount\n",
      "of RAM. This is especially true during training, because the reverse pass of backpro‐\n",
      "pagation requires all the intermediate values computed during the forward pass.\n",
      "For example, consider a convolutional layer with 5 × 5 filters, outputting 200 feature\n",
      "maps of size 150 × 100, with stride 1 and SAME padding. If the input is a 150 × 100\n",
      "Convolutional Layer | 441\n",
      "7A fully connected layer with 150 × 100 neurons, each connected to all 150 × 100 × 3 inputs, would have 1502\n",
      "× 1002 × 3 = 675 million parameters!\n",
      "8In the international system of units (SI), 1 MB = 1,000 kB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits.\n",
      "RGB image (three channels), then the number of parameters is (5 × 5 × 3 + 1) × 200\n",
      "= 15,200 (the +1 corresponds to the bias terms), which is fairly small compared to a\n",
      "fully connected layer.7 However, each of the 200 feature maps contains 150 × 100 neu‐\n",
      "rons, and each of these neurons needs to compute a weighted sum of its 5 × 5 × 3 =\n",
      "75 inputs: that’s a total of 225 million float multiplications. Not as bad as a fully con‐\n",
      "nected layer, but still quite computationally intensive. Moreover, if the feature maps\n",
      "are represented using 32-bit floats, then the convolutional layer’s output will occupy\n",
      "200 × 150 × 100 × 32 = 96 million bits (12 MB) of RAM.8 And that’s just for one\n",
      "instance! If a training batch contains 100 instances, then this layer will use up 1.2 GB\n",
      "of RAM!\n",
      "During inference (i.e., when making a prediction for a new instance) the RAM occu‐\n",
      "pied by one layer can be released as soon as the next layer has been computed, so you\n",
      "only need as much RAM as required by two consecutive layers. But during training\n",
      "everything computed during the forward pass needs to be preserved for the reverse\n",
      "pass, so the amount of RAM needed is (at least) the total amount of RAM required by\n",
      "all layers.\n",
      "If training crashes because of an out-of-memory error, you can try\n",
      "reducing the mini-batch size. Alternatively, you can try reducing\n",
      "dimensionality using a stride, or removing a few layers. Or you can\n",
      "try using 16-bit floats instead of 32-bit floats. Or you could distrib‐\n",
      "ute the CNN across multiple devices.\n",
      "Now let’s look at the second common building block of CNNs: the pooling layer .\n",
      "Pooling Layer\n",
      "Once you understand how convolutional layers work, the pooling layers are quite\n",
      "easy to grasp. Their goal is to subsample  (i.e., shrink) the input image in order to\n",
      "reduce the computational load, the memory usage, and the number of parameters\n",
      "(thereby limiting the risk of overfitting).\n",
      "Just like in convolutional layers, each neuron in a pooling layer is connected to the\n",
      "outputs of a limited number of neurons in the previous layer, located within a small\n",
      "rectangular receptive field. Y ou must define its size, the stride, and the padding type,\n",
      "just like before. However, a pooling neuron has no weights; all it does is aggregate the\n",
      "inputs using an aggregation function such as the max or mean. Figure 14-8  shows a\n",
      "max pooling layer , which is the most common type of pooling layer. In this example,\n",
      "442 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "9Other kernels we discussed so far had weights, but pooling kernels do not: they are just stateless sliding win‐\n",
      "dows.\n",
      "we use a 2 × 2 _pooling kernel_9, with a stride of 2, and no padding. Only the max\n",
      "input value in each receptive field makes it to the next layer, while the other inputs\n",
      "are dropped. For example, in the lower left receptive field in Figure 14-8 , the input\n",
      "values are 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because\n",
      "of the stride of 2, the output image has half the height and half the width of the input\n",
      "image (rounded down since we use no padding).\n",
      "Figure 14-8. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)\n",
      "A pooling layer typically works on every input channel independ‐\n",
      "ently, so the output depth is the same as the input depth.\n",
      "Other than reducing computations, memory usage and the number of parameters, a\n",
      "max pooling layer also introduces some level of invariance  to small translations, as\n",
      "shown in Figure 14-9 . Here we assume that the bright pixels have a lower value than\n",
      "dark pixels, and we consider 3 images (A, B, C) going through a max pooling layer\n",
      "with a 2 × 2 kernel and stride 2. Images B and C are the same as image A, but shifted\n",
      "by one and two pixels to the right. As you can see, the outputs of the max pooling\n",
      "layer for images A and B are identical. This is what translation invariance means.\n",
      "However, for image C, the output is different: it is shifted by one pixel to the right\n",
      "(but there is still 75% invariance). By inserting a max pooling layer every few layers in\n",
      "a CNN, it is possible to get some level of translation invariance at a larger scale.\n",
      "Moreover, max pooling also offers a small amount of rotational invariance and a\n",
      "slight scale invariance. Such invariance (even if it is limited) can be useful in cases\n",
      "where the prediction should not depend on these details, such as in classification\n",
      "tasks.\n",
      "Pooling Layer | 443\n",
      "Figure 14-9. Invariance to small translations\n",
      "But max pooling has some downsides: firstly, it is obviously very destructive: even\n",
      "with a tiny 2 × 2 kernel and a stride of 2, the output will be two times smaller in both\n",
      "directions (so its area will be four times smaller), simply dropping 75% of the input\n",
      "values. And in some applications, invariance is not desirable, for example for seman‐\n",
      "tic segmentation : this is the task of classifying each pixel in an image depending on the\n",
      "object that pixel belongs to: obviously, if the input image is translated by 1 pixel to the\n",
      "right, the output should also be translated by 1 pixel to the right. The goal in this case\n",
      "is equivariance , not invariance: a small change to the inputs should lead to a corre‐\n",
      "sponding small change in the output.\n",
      "TensorFlow Implementation\n",
      "Implementing a max pooling layer in TensorFlow is quite easy. The following code\n",
      "creates a max pooling layer using a 2 × 2 kernel. The strides default to the kernel size,\n",
      "so this layer will use a stride of 2 (both horizontally and vertically). By default, it uses\n",
      "V ALID padding (i.e., no padding at all):\n",
      "max_pool  = keras.layers.MaxPool2D (pool_size =2)\n",
      "To create an average pooling layer , just use AvgPool2D  instead of MaxPool2D . As you\n",
      "might expect, it works exactly like a max pooling layer, except it computes the mean\n",
      "rather than the max. Average pooling layers used to be very popular, but people\n",
      "444 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "mostly use max pooling layers now, as they generally perform better. This may seem\n",
      "surprising, since computing the mean generally loses less information than comput‐\n",
      "ing the max. But on the other hand, max pooling preserves only the strongest feature,\n",
      "getting rid of all the meaningless ones, so the next layers get a cleaner signal to work\n",
      "with. Moreover, max pooling offers stronger translation invariance than average\n",
      "pooling.\n",
      "Note that max pooling and average pooling can be performed along the depth dimen‐\n",
      "sion rather than the spatial dimensions, although this is not as common. This can\n",
      "allow the CNN to learn to be invariant to various features. For example, it could learn\n",
      "multiple filters, each detecting a different rotation of the same pattern, such as hand-\n",
      "written digits (see Figure 14-10 ), and the depth-wise max pooling layer would ensure\n",
      "that the output is the same regardless of the rotation. The CNN could similarly learn\n",
      "to be invariant to anything else: thickness, brightness, skew, color, and so on.\n",
      "Figure 14-10. Depth-wise max pooling can help the CNN learn any invariance\n",
      "Pooling Layer | 445\n",
      "Keras does not include a depth-wise max pooling layer, but TensorFlow’s low-level\n",
      "Deep Learning API does: just use the tf.nn.max_pool()  function, and specify the\n",
      "kernel size and strides as 4-tuples. The first three values of each should be 1: this indi‐\n",
      "cates that the kernel size and stride along the batch, height and width dimensions\n",
      "shoud be 1. The last value should be whatever kernel size and stride you want along\n",
      "the depth dimension, for example 3 (this must be a divisor of the input depth; for\n",
      "example, it will not work if the previous layer outputs 20 feature maps, since 20 is not\n",
      "a multiple of 3):\n",
      "output = tf.nn.max_pool (images,\n",
      "                        ksize=(1, 1, 1, 3),\n",
      "                        strides=(1, 1, 1, 3),\n",
      "                        padding=\"VALID\")\n",
      "If you want to include this as a layer in your Keras models, you can simply wrap it in\n",
      "a Lambda  layer (or create a custom Keras layer):\n",
      "depth_pool  = keras.layers.Lambda(\n",
      "    lambda X: tf.nn.max_pool (X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3),\n",
      "                             padding=\"VALID\"))\n",
      "One last type of pooling layer that you will often see in modern architectures is the\n",
      "global average pooling  layer. It works very differently: all it does is compute the mean\n",
      "of each entire feature map (it’s like an average pooling layer using a pooling kernel\n",
      "with the same spatial dimensions as the inputs). This means that it just outputs a sin‐\n",
      "gle number per feature map and per instance. Although this is of course extremely\n",
      "destructive (most of the information in the feature map is lost), it can be useful as the\n",
      "output layer, as we will see later in this chapter. To create such a layer, simply use the\n",
      "keras.layers.GlobalAvgPool2D  class:\n",
      "global_avg_pool  = keras.layers.GlobalAvgPool2D ()\n",
      "It is actually equivalent to this simple Lamba  layer, which computes the mean over the\n",
      "spatial dimensions (height and width):\n",
      "global_avg_pool  = keras.layers.Lambda(lambda X: tf.reduce_mean (X, axis=[1, 2]))\n",
      "Now you know all the building blocks to create a convolutional neural network. Let’s\n",
      "see how to assemble them.\n",
      "CNN Architectures\n",
      "Typical CNN architectures stack a few convolutional layers (each one generally fol‐\n",
      "lowed by a ReLU layer), then a pooling layer, then another few convolutional layers\n",
      "(+ReLU), then another pooling layer, and so on. The image gets smaller and smaller\n",
      "as it progresses through the network, but it also typically gets deeper and deeper (i.e.,\n",
      "with more feature maps) thanks to the convolutional layers (see Figure 14-11 ). At the\n",
      "top of the stack, a regular feedforward neural network is added, composed of a few\n",
      "446 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a\n",
      "softmax layer that outputs estimated class probabilities).\n",
      "Figure 14-11. Typical CNN architecture\n",
      "A common mistake is to use convolution kernels that are too large.\n",
      "For example, instead of using a convolutional layer with a 5 × 5\n",
      "kernel, it is generally preferable to stack two layers with 3 × 3 ker‐\n",
      "nels: it will use less parameters and require less computations, and\n",
      "it will usually perform better. One exception to this recommenda‐\n",
      "tion is for the first convolutional layer: it can typically have a large\n",
      "kernel (e.g., 5 × 5), usually with stride of 2 or more: this will reduce\n",
      "the spatial dimension of the image without losing too much infor‐\n",
      "mation, and since the input image only has 3 channels in general, it\n",
      "will not be too costly.\n",
      "Here is how you can implement a simple CNN to tackle the fashion MNIST dataset\n",
      "(introduced in Chapter 10 ):\n",
      "from functools  import partial\n",
      "DefaultConv2D  = partial(keras.layers.Conv2D,\n",
      "                        kernel_size =3, activation ='relu', padding=\"SAME\")\n",
      "model = keras.models.Sequential ([\n",
      "    DefaultConv2D (filters=64, kernel_size =7, input_shape =[28, 28, 1]),\n",
      "    keras.layers.MaxPooling2D (pool_size =2),\n",
      "    DefaultConv2D (filters=128),\n",
      "    DefaultConv2D (filters=128),\n",
      "    keras.layers.MaxPooling2D (pool_size =2),\n",
      "    DefaultConv2D (filters=256),\n",
      "    DefaultConv2D (filters=256),\n",
      "    keras.layers.MaxPooling2D (pool_size =2),\n",
      "    keras.layers.Flatten(),\n",
      "    keras.layers.Dense(units=128, activation ='relu'),\n",
      "    keras.layers.Dropout(0.5),\n",
      "    keras.layers.Dense(units=64, activation ='relu'),\n",
      "    keras.layers.Dropout(0.5),\n",
      "    keras.layers.Dense(units=10, activation ='softmax' ),\n",
      "])\n",
      "CNN Architectures | 447\n",
      "•In this code, we start by using the partial()  function to define a thin wrapper\n",
      "around the Conv2D  class, called DefaultConv2D : it simply avoids having to repeat\n",
      "the same hyperparameter values over and over again.\n",
      "•The first layer uses a large kernel size, but no stride because the input images are\n",
      "not very large. It also sets input_shape=[28, 28, 1] , which means the images\n",
      "are 28 × 28 pixels, with a single color channel (i.e., grayscale).\n",
      "•Next, we have a max pooling layer, which divides each spatial dimension by a fac‐\n",
      "tor of two (since pool_size=2 ).\n",
      "•Then we repeat the same structure twice: two convolutional layers followed by a\n",
      "max pooling layer. For larger images, we could repeat this structure several times\n",
      "(the number of repetitions is a hyperparameter you can tune).\n",
      "•Note that the number of filters grows as we climb up the CNN towards the out‐\n",
      "put layer (it is initially 64, then 128, then 256): it makes sense for it to grow, since\n",
      "the number of low level features is often fairly low (e.g., small circles, horizontal\n",
      "lines, etc.), but there are many different ways to combine them into higher level\n",
      "features. It is a common practice to double the number of filters after each pool‐\n",
      "ing layer: since a pooling layer divides each spatial dimension by a factor of 2, we\n",
      "can afford doubling the number of feature maps in the next layer, without fear of\n",
      "exploding the number of parameters, memory usage, or computational load.\n",
      "•Next is the fully connected network, composed of 2 hidden dense layers and a\n",
      "dense output layer. Note that we must flatten its inputs, since a dense network\n",
      "expects a 1D array of features for each instance. We also add two dropout layers,\n",
      "with a dropout rate of 50% each, to reduce overfitting.\n",
      "This CNN reaches over 92% accuracy on the test set. It’s not the state of the art, but it\n",
      "is pretty good, and clearly much better than what we achieved with dense networks in\n",
      "Chapter 10 .\n",
      "Over the years, variants of this fundamental architecture have been developed, lead‐\n",
      "ing to amazing advances in the field. A good measure of this progress is the error rate\n",
      "in competitions such as the ILSVRC ImageNet challenge . In this competition the\n",
      "top-5 error rate for image classification fell from over 26% to less than 2.3% in just six\n",
      "years. The top-five error rate is the number of test images for which the system’s top 5\n",
      "predictions did not include the correct answer. The images are large (256 pixels high)\n",
      "and there are 1,000 classes, some of which are really subtle (try distinguishing 120\n",
      "dog breeds). Looking at the evolution of the winning entries is a good way to under‐\n",
      "stand how CNNs work.\n",
      "We will first look at the classical LeNet-5 architecture (1998), then three of the win‐\n",
      "ners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), and ResNet\n",
      "(2015).\n",
      "448 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "10“Gradient-Based Learning Applied to Document Recognition” , Y . LeCun, L. Bottou, Y . Bengio and P . Haffner\n",
      "(1998).LeNet-5\n",
      "The LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\n",
      "mentioned earlier, it was created by Y ann LeCun in 1998 and widely used for hand‐\n",
      "written digit recognition (MNIST). It is composed of the layers shown in Table 14-1 .\n",
      "Table 14-1. LeNet-5 architecture\n",
      "Layer Type Maps Size Kernel size Stride Activation\n",
      "Out Fully Connected – 10 – – RBF\n",
      "F6 Fully Connected – 84 – – tanh\n",
      "C5 Convolution 120 1 × 1 5 × 5 1 tanh\n",
      "S4 Avg Pooling 16 5 × 5 2 × 2 2 tanh\n",
      "C3 Convolution 16 10 × 10 5 × 5 1 tanh\n",
      "S2 Avg Pooling 6 14 × 14 2 × 2 2 tanh\n",
      "C1 Convolution 6 28 × 28 5 × 5 1 tanh\n",
      "In Input 1 32 × 32 – – –\n",
      "There are a few extra details to be noted:\n",
      "•MNIST images are 28 × 28 pixels, but they are zero-padded to 32 × 32 pixels and\n",
      "normalized before being fed to the network. The rest of the network does not use\n",
      "any padding, which is why the size keeps shrinking as the image progresses\n",
      "through the network.\n",
      "•The average pooling layers are slightly more complex than usual: each neuron\n",
      "computes the mean of its inputs, then multiplies the result by a learnable coeffi‐\n",
      "cient (one per map) and adds a learnable bias term (again, one per map), then\n",
      "finally applies the activation function.\n",
      "•Most neurons in C3 maps are connected to neurons in only three or four S2\n",
      "maps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for\n",
      "details.\n",
      "•The output layer is a bit special: instead of computing the matrix multiplication\n",
      "of the inputs and the weight vector, each neuron outputs the square of the Eucli‐\n",
      "dian distance between its input vector and its weight vector. Each output meas‐\n",
      "ures how much the image belongs to a particular digit class. The cross entropy \n",
      "cost function is now preferred, as it penalizes bad predictions much more, pro‐\n",
      "ducing larger gradients and converging faster.\n",
      "CNN Architectures | 449\n",
      "11“ImageNet Classification with Deep Convolutional Neural Networks, ” A. Krizhevsky et al. (2012).Y ann LeCun’s website  (“LENET” section) features great demos of LeNet-5 classifying \n",
      "digits.\n",
      "AlexNet\n",
      "The AlexNet  CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a\n",
      "large margin: it achieved 17% top-5 error rate while the second best achieved only\n",
      "26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and\n",
      "Geoffrey Hinton. It is quite similar to LeNet-5, only much larger and deeper, and it\n",
      "was the first to stack convolutional layers directly on top of each other, instead of\n",
      "stacking a pooling layer on top of each convolutional layer. Table 14-2  presents this\n",
      "architecture.\n",
      "Table 14-2. AlexNet architecture\n",
      "Layer Type Maps Size Kernel size Stride Padding Activation\n",
      "Out Fully Connected – 1,000 – – – Softmax\n",
      "F9 Fully Connected – 4,096 – – – ReLU\n",
      "F8 Fully Connected – 4,096 – – – ReLU\n",
      "C7 Convolution 256 13 × 13 3 × 3 1 SAME ReLU\n",
      "C6 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\n",
      "C5 Convolution 384 13 × 13 3 × 3 1 SAME ReLU\n",
      "S4 Max Pooling 256 13 × 13 3 × 3 2 VALID –\n",
      "C3 Convolution 256 27 × 27 5 × 5 1 SAME ReLU\n",
      "S2 Max Pooling 96 27 × 27 3 × 3 2 VALID –\n",
      "C1 Convolution 96 55 × 55 11 × 11 4 VALID ReLU\n",
      "In Input 3 (RGB) 227 × 227 – – – –\n",
      "To reduce overfitting, the authors used two regularization techniques: first they\n",
      "applied dropout (introduced in Chapter 11 ) with a 50% dropout rate during training\n",
      "to the outputs of layers F8 and F9. Second, they performed data augmentation  by ran‐\n",
      "domly shifting the training images by various offsets, flipping them horizontally, and\n",
      "changing the lighting conditions.\n",
      "Data Augmentation\n",
      "Data augmentation artificially increases the size of the training set by generating\n",
      "many realistic variants of each training instance. This reduces overfitting, making this\n",
      "a regularization technique. The generated instances should be as realistic as possible:\n",
      "450 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "ideally, given an image from the augmented training set, a human should not be able\n",
      "to tell whether it was augmented or not. Moreover, simply adding white noise will not\n",
      "help; the modifications should be learnable (white noise is not).\n",
      "For example, you can slightly shift, rotate, and resize every picture in the training set\n",
      "by various amounts and add the resulting pictures to the training set (see\n",
      "Figure 14-12 ). This forces the model to be more tolerant to variations in the position,\n",
      "orientation, and size of the objects in the pictures. If you want the model to be more\n",
      "tolerant to different lighting conditions, you can similarly generate many images with\n",
      "various contrasts. In general, you can also flip the pictures horizontally (except for\n",
      "text, and other non-symmetrical objects). By combining these transformations you\n",
      "can greatly increase the size of your training set.\n",
      "Figure 14-12. Generating new training instances from existing ones\n",
      "AlexNet also uses a competitive normalization step immediately after the ReLU step\n",
      "of layers C1 and C3, called local response normalization . The most strongly activated\n",
      "neurons inhibit other neurons located at the same position in neighboring feature\n",
      "maps (such competitive activation has been observed in biological neurons). This\n",
      "encourages different feature maps to specialize, pushing them apart and forcing them\n",
      "CNN Architectures | 451\n",
      "12“Going Deeper with Convolutions, ” C. Szegedy et al. (2015).\n",
      "13In the 2010 movie Inception , the characters keep going deeper and deeper into multiple layers of dreams,\n",
      "hence the name of these modules.to explore a wider range of features, ultimately improving generalization. Equation\n",
      "14-2  shows how to apply LRN.\n",
      "Equation 14-2. Local response normalization\n",
      "bi=aik+α∑\n",
      "j=jlowjhigh\n",
      "aj2−β\n",
      "withjhigh= min i+r\n",
      "2,fn− 1\n",
      "jlow= max 0,i−r\n",
      "2\n",
      "•bi is the normalized output of the neuron located in feature map i, at some row u\n",
      "and column v (note that in this equation we consider only neurons located at this\n",
      "row and column, so u and v are not shown).\n",
      "•ai is the activation of that neuron after the ReLU step, but before normalization.\n",
      "•k, α, β, and r are hyperparameters. k is called the bias, and r is called the depth\n",
      "radius .\n",
      "•fn is the number of feature maps.\n",
      "For example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\n",
      "of the neurons located in the feature maps immediately above and below its own.\n",
      "In AlexNet, the hyperparameters are set as follows: r = 2, α = 0.00002, β = 0.75, and k\n",
      "= 1. This step can be implemented using the tf.nn.local_response_normaliza\n",
      "tion()  function (which you can wrap in a Lambda  layer if you want to use it in a\n",
      "Keras model).\n",
      "A variant of AlexNet called ZF Net  was developed by Matthew Zeiler and Rob Fergus\n",
      "and won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked \n",
      "hyperparameters (number of feature maps, kernel size, stride, etc.).\n",
      "GoogLeNet\n",
      "The GoogLeNet architecture  was developed by Christian Szegedy et al. from Google\n",
      "Research,12 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate\n",
      "below 7%. This great performance came in large part from the fact that the network\n",
      "was much deeper than previous CNNs (see Figure 14-14 ). This was made possible by\n",
      "sub-networks called inception modules ,13 which allow GoogLeNet to use parameters\n",
      "452 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "much more efficiently than previous architectures: GoogLeNet actually has 10 times\n",
      "fewer parameters than AlexNet (roughly 6 million instead of 60 million).\n",
      "Figure 14-13  shows the architecture of an inception module. The notation “3 × 3 +\n",
      "1(S)” means that the layer uses a 3 × 3 kernel, stride 1, and SAME padding. The input\n",
      "signal is first copied and fed to four different layers. All convolutional layers use the\n",
      "ReLU activation function. Note that the second set of convolutional layers uses differ‐\n",
      "ent kernel sizes (1 × 1, 3 × 3, and 5 × 5), allowing them to capture patterns at different\n",
      "scales. Also note that every single layer uses a stride of 1 and SAME padding (even\n",
      "the max pooling layer), so their outputs all have the same height and width as their\n",
      "inputs. This makes it possible to concatenate all the outputs along the depth dimen‐\n",
      "sion in the final depth concat layer  (i.e., stack the feature maps from all four top con‐\n",
      "volutional layers). This concatenation layer can be implemented in TensorFlow using\n",
      "the tf.concat()  operation, with axis=3  (axis 3 is the depth).\n",
      "Figure 14-13. Inception module\n",
      "Y ou may wonder why inception modules have convolutional layers with 1 × 1 ker‐\n",
      "nels. Surely these layers cannot capture any features since they look at only one pixel\n",
      "at a time? In fact, these layers serve three purposes:\n",
      "•First, although they cannot capture spatial patterns, they can capture patterns\n",
      "along the depth dimension.\n",
      "•Second, they are configured to output fewer feature maps than their inputs, so\n",
      "they serve as bottleneck layers , meaning they reduce dimensionality. This cuts the\n",
      "computational cost and the number of parameters, speeding up training and\n",
      "improving generalization.\n",
      "•Lastly, each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like\n",
      "a single, powerful convolutional layer, capable of capturing more complex pat‐\n",
      "terns. Indeed, instead of sweeping a simple linear classifier across the image (as a\n",
      "CNN Architectures | 453\n",
      "single convolutional layer does), this pair of convolutional layers sweeps a two-\n",
      "layer neural network across the image.\n",
      "In short, you can think of the whole inception module as a convolutional layer on\n",
      "steroids, able to output feature maps that capture complex patterns at various scales.\n",
      "The number of convolutional kernels for each convolutional layer\n",
      "is a hyperparameter. Unfortunately, this means that you have six\n",
      "more hyperparameters to tweak for every inception layer you add.\n",
      "Now let’s look at the architecture of the GoogLeNet CNN (see Figure 14-14 ). The\n",
      "number of feature maps output by each convolutional layer and each pooling layer is\n",
      "shown before the kernel size. The architecture is so deep that it has to be represented\n",
      "in three columns, but GoogLeNet is actually one tall stack, including nine inception\n",
      "modules (the boxes with the spinning tops). The six numbers in the inception mod‐\n",
      "ules represent the number of feature maps output by each convolutional layer in the\n",
      "module (in the same order as in Figure 14-13 ). Note that all the convolutional layers\n",
      "use the ReLU activation function.\n",
      "454 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "Figure 14-14. GoogLeNet architecture\n",
      "Let’s go through this network:\n",
      "•The first two layers divide the image’s height and width by 4 (so its area is divided\n",
      "by 16), to reduce the computational load. The first layer uses a large kernel size,\n",
      "so that much of the information is still preserved.\n",
      "•Then the local response normalization layer ensures that the previous layers learn\n",
      "a wide variety of features (as discussed earlier).\n",
      "•Two convolutional layers follow, where the first acts like a bottleneck layer . As\n",
      "explained earlier, you can think of this pair as a single smarter convolutional\n",
      "layer.\n",
      "•Again, a local response normalization layer ensures that the previous layers cap‐\n",
      "ture a wide variety of patterns.\n",
      "CNN Architectures | 455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14“Very Deep Convolutional Networks for Large-Scale Image Recognition, ” K. Simonyan and A. Zisserman\n",
      "(2015).•Next a max pooling layer reduces the image height and width by 2, again to speed\n",
      "up computations.\n",
      "•Then comes the tall stack of nine inception modules, interleaved with a couple\n",
      "max pooling layers to reduce dimensionality and speed up the net.\n",
      "•Next, the global average pooling layer simply outputs the mean of each feature\n",
      "map: this drops any remaining spatial information, which is fine since there was\n",
      "not much spatial information left at that point. Indeed, GoogLeNet input images\n",
      "are typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each\n",
      "dividing the height and width by 2, the feature maps are down to 7 × 7. More‐\n",
      "over, it is a classification task, not localization, so it does not matter where the\n",
      "object is. Thanks to the dimensionality reduction brought by this layer, there is\n",
      "no need to have several fully connected layers at the top of the CNN (like in\n",
      "AlexNet), and this considerably reduces the number of parameters in the net‐\n",
      "work and limits the risk of overfitting.\n",
      "•The last layers are self-explanatory: dropout for regularization, then a fully con‐\n",
      "nected layer with 1,000 units, since there are a 1,000 classes, and a softmax acti‐\n",
      "vation function to output estimated class probabilities.\n",
      "This diagram is slightly simplified: the original GoogLeNet architecture also included\n",
      "two auxiliary classifiers plugged on top of the third and sixth inception modules.\n",
      "They were both composed of one average pooling layer, one convolutional layer, two\n",
      "fully connected layers, and a softmax activation layer. During training, their loss\n",
      "(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish‐\n",
      "ing gradients problem and regularize the network. However, it was later shown that\n",
      "their effect was relatively minor.\n",
      "Several variants of the GoogLeNet architecture were later proposed by Google\n",
      "researchers, including Inception-v3 and Inception-v4, using slightly different incep‐\n",
      "tion modules, and reaching even better performance.\n",
      "VGGNet\n",
      "The runner up in the ILSVRC 2014 challenge was  VGGNet14, developed by K. Simon‐\n",
      "yan and A. Zisserman. It had a very simple and classical architecture, with 2 or 3 con‐\n",
      "volutional layers, a pooling layer, then again 2 or 3 convolutional layers, a pooling\n",
      "layer, and so on (with a total of just 16 convolutional layers), plus a final dense net‐\n",
      "work with 2 hidden layers and the output layer. It used only 3 × 3 filters, but many\n",
      "filters.\n",
      "456 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "15“Deep Residual Learning for Image Recognition, ” K. He (2015).ResNet\n",
      "The ILSVRC 2015 challenge was won using a Residual Network  (or ResNet ), devel‐\n",
      "oped by Kaiming He et al.,15 which delivered an astounding top-5 error rate under\n",
      "3.6%, using an extremely deep CNN composed of 152 layers. It confirmed the general\n",
      "trend: models are getting deeper and deeper, with fewer and fewer parameters. The\n",
      "key to being able to train such a deep network is to use skip connections  (also called\n",
      "shortcut connections ): the signal feeding into a layer is also added to the output of a\n",
      "layer located a bit higher up the stack. Let’s see why this is useful.\n",
      "When training a neural network, the goal is to make it model a target function h(x).\n",
      "If you add the input x to the output of the network (i.e., you add a skip connection),\n",
      "then the network will be forced to model f(x) = h(x) – x rather than h(x). This is\n",
      "called residual learning  (see Figure 14-15 ).\n",
      "Figure 14-15. Residual learning\n",
      "When you initialize a regular neural network, its weights are close to zero, so the net‐\n",
      "work just outputs values close to zero. If you add a skip connection, the resulting net‐\n",
      "work just outputs a copy of its inputs; in other words, it initially models the identity\n",
      "function. If the target function is fairly close to the identity function (which is often\n",
      "the case), this will speed up training considerably.\n",
      "Moreover, if you add many skip connections, the network can start making progress\n",
      "even if several layers have not started learning yet (see Figure 14-16 ). Thanks to skip\n",
      "connections, the signal can easily make its way across the whole network. The deep\n",
      "residual network can be seen as a stack of residual units , where each residual unit is a\n",
      "small neural network with a skip connection.\n",
      "CNN Architectures | 457\n",
      "Figure 14-16. Regular deep neural network (left)  and deep residual network (right)\n",
      "Now let’s look at ResNet’s architecture (see Figure 14-17 ). It is actually surprisingly\n",
      "simple. It starts and ends exactly like GoogLeNet (except without a dropout layer),\n",
      "and in between is just a very deep stack of simple residual units. Each residual unit is\n",
      "composed of two convolutional layers (and no pooling layer!), with Batch Normaliza‐\n",
      "tion (BN) and ReLU activation, using 3 × 3 kernels and preserving spatial dimensions\n",
      "(stride 1, SAME padding).\n",
      "Figure 14-17. ResNet architecture\n",
      "Note that the number of feature maps is doubled every few residual units, at the same\n",
      "time as their height and width are halved (using a convolutional layer with stride 2).\n",
      "When this happens the inputs cannot be added directly to the outputs of the residual\n",
      "unit since they don’t have the same shape (for example, this problem affects the skip\n",
      "458 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "16“Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning, ” C. Szegedy et al.\n",
      "(2016).\n",
      "17“Xception: Deep Learning with Depthwise Separable Convolutions, ” François Chollet (2016)\n",
      "connection represented by the dashed arrow in Figure 14-17 ). To solve this problem,\n",
      "the inputs are passed through a 1 × 1 convolutional layer with stride 2 and the right\n",
      "number of output feature maps (see Figure 14-18 ).\n",
      "Figure 14-18. Skip connection when changing feature map size and depth\n",
      "ResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and\n",
      "the fully connected layer) containing three residual units that output 64 feature maps,\n",
      "4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple‐\n",
      "ment this architecture later in this chapter.\n",
      "ResNets deeper than that, such as ResNet-152, use slightly different residual units.\n",
      "Instead of two 3 × 3 convolutional layers with (say) 256 feature maps, they use three\n",
      "convolutional layers: first a 1 × 1 convolutional layer with just 64 feature maps (4\n",
      "times less), which acts as a bottleneck layer (as discussed already), then a 3 × 3 layer\n",
      "with 64 feature maps, and finally another 1 × 1 convolutional layer with 256 feature\n",
      "maps (4 times 64) that restores the original depth. ResNet-152 contains three such\n",
      "RUs that output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\n",
      "maps, and finally 3 RUs with 2,048 maps.\n",
      "Google’s Inception-v416 architecture merged the ideas of GoogLe‐\n",
      "Net and ResNet and achieved close to 3% top-5 error rate on\n",
      "ImageNet classification.\n",
      "Xception\n",
      "Another variant of the GoogLeNet architecture is also worth noting: Xception17\n",
      "(which stands for Extreme Inception ) was proposed in 2016 by François Chollet (the\n",
      "CNN Architectures | 459\n",
      "18This name can sometimes be ambiguous, since spatially separable convolutions are often called “separable\n",
      "convolutions” as well.author of Keras), and it significantly outperformed Inception-v3 on a huge vision task\n",
      "(350 million images and 17,000 classes). Just like Inception-v4, it also merges the\n",
      "ideas of GoogLeNet and ResNet, but it replaces the inception modules with a special\n",
      "type of layer called a depthwise separable convolution  (or separable convolution  for\n",
      "short18). These layers had been used before in some CNN architectures, but they were\n",
      "not as central as in the Xception architecture. While a regular convolutional layer\n",
      "uses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-\n",
      "channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer\n",
      "makes the strong assumption that spatial patterns and cross-channel patterns can be\n",
      "modeled separately (see Figure 14-19 ). Thus, it is composed of two parts: the first part\n",
      "applies a single spatial filter for each input feature map, then the second part looks\n",
      "exclusively for cross-channel patterns—it is just a regular convolutional layer with 1 ×\n",
      "1 filters.\n",
      "Figure 14-19. Depthwise Separable Convolutional Layer\n",
      "Since separable convolutional layers only have one spatial filter per input channel,\n",
      "you should avoid using them after layers that have too few channels, such as the input\n",
      "layer (granted, that’s what Figure 14-19  represents, but it is just for illustration pur‐\n",
      "poses). For this reason, the Xception architecture starts with 2 regular convolutional\n",
      "layers, but then the rest of the architecture uses only separable convolutions (34 in\n",
      "460 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "19“Crafting GBD-Net for Object Detection, ” X. Zeng et al. (2016).\n",
      "20“Squeeze-and-Excitation Networks, ” Jie Hu et al. (2017)\n",
      "all), plus a few max pooling layers and the usual final layers (a global average pooling\n",
      "layer, and a dense output layer).\n",
      "Y ou might wonder why Xception is considered a variant of GoogLeNet, since it con‐\n",
      "tains no inception module at all? Well, as we discussed earlier, an Inception module\n",
      "contains convolutional layers with 1 × 1 filters: these look exclusively for cross-\n",
      "channel patterns. However, the convolution layers that sit on top of them are regular\n",
      "convolutional layers that look both for spatial and cross-channel patterns. So you can\n",
      "think of an Inception module as an intermediate between a regular convolutional\n",
      "layer (which considers spatial patterns and cross-channel patterns jointly) and a sepa‐\n",
      "rable convolutional layer (which considers them separately). In practice, it seems that\n",
      "separable convolutions generally perform better.\n",
      "Separable convolutions use less parameters, less memory and less\n",
      "computations than regular convolutional layers, and in general\n",
      "they even perform better, so you should consider using them by\n",
      "default (except after layers with few channels).\n",
      "The ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni‐\n",
      "versity of Hong Kong. They used an ensemble of many different techniques, includ‐\n",
      "ing a sophisticated object-detection system called GBD-Net19, to achieve a top-5 error\n",
      "rate below 3%. Although this result is unquestionably impressive, the complexity of\n",
      "the solution contrasted with the simplicity of ResNets. Moreover, one year later\n",
      "another fairly simple architecture performed even better, as we will see now.\n",
      "SENet\n",
      "The winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-\n",
      "Excitation Network  (SENet)20. This architecture extends existing architectures such as\n",
      "inception networks or ResNets, and boosts their performance. This allowed SENet to\n",
      "win the competition with an astonishing 2.25% top-5 error rate! The extended ver‐\n",
      "sions of inception networks and ResNet are called SE-Inception  and SE-ResNet  respec‐\n",
      "tively. The boost comes from the fact that a SENet adds a small neural network, called\n",
      "a SE Block , to every unit in the original architecture (i.e., every inception module or\n",
      "every residual unit), as shown in Figure 14-20 .\n",
      "CNN Architectures | 461\n",
      "Figure 14-20. SE-Inception Module (left)  and SE-ResNet Unit (right)\n",
      "A SE Block analyzes the output of the unit it is attached to, focusing exclusively on\n",
      "the depth dimension (it does not look for any spatial pattern), and it learns which fea‐\n",
      "tures are usually most active together. It then uses this information to recalibrate the\n",
      "feature maps, as shown in Figure 14-21 . For example, a SE Block may learn that\n",
      "mouths, noses and eyes usually appear together in pictures: if you see a mouth and a\n",
      "nose, you should expect to see eyes as well. So if a SE Block sees a strong activation in\n",
      "the mouth and nose feature maps, but only mild activation in the eye feature map, it\n",
      "will boost the eye feature map (more accurately, it will reduce irrelevant feature\n",
      "maps). If the eyes were somewhat confused with something else, this feature map\n",
      "recalibration will help resolve the ambiguity.\n",
      "462 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "Figure 14-21. An SE Block Performs Feature Map Recalibration\n",
      "A SE Block is composed of just 3 layers: a global average pooling layer, a hidden dense\n",
      "layer using the ReLU activation function, and a dense output layer using the sigmoid\n",
      "activation function (see Figure 14-22 ):\n",
      "Figure 14-22. SE Block Architecture\n",
      "CNN Architectures | 463\n",
      "As earlier, the global average pooling layer computes the mean activation for each fea‐\n",
      "ture map: for example, if its input contains 256 feature maps, it will output 256 num‐\n",
      "bers representing the overall level of response for each filter. The next layer is where\n",
      "the “squeeze” happens: this layer has much less than 256 neurons, typically 16 times\n",
      "less than the number of feature maps (e.g., 16 neurons), so the 256 numbers get com‐\n",
      "pressed into a small vector (e.g., 16 dimensional). This is a low-dimensional vector\n",
      "representation (i.e., an embedding) of the distribution of feature responses. This bot‐\n",
      "tleneck step forces the SE Block to learn a general representation of the feature com‐\n",
      "binations (we will see this principle in action again when we discuss autoencoders\n",
      "in ???). Finally, the output layer takes the embedding and outputs a recalibration vec‐\n",
      "tor containing one number per feature map (e.g., 256), each between 0 and 1. The\n",
      "feature maps are then multiplied by this recalibration vector, so irrelevant features\n",
      "(with a low recalibration score) get scaled down while relevant features (with a recali‐\n",
      "bration score close to 1) are left alone.\n",
      "Implementing a ResNet-34 CNN Using Keras\n",
      "Most CNN architectures described so far are fairly straightforward to implement\n",
      "(although generally you would load a pretrained network instead, as we will see). To\n",
      "illustrate the process, let’s implement a ResNet-34 from scratch using Keras. First, let’s\n",
      "create a ResidualUnit  layer:\n",
      "DefaultConv2D  = partial(keras.layers.Conv2D, kernel_size =3, strides=1,\n",
      "                        padding=\"SAME\", use_bias =False)\n",
      "class ResidualUnit (keras.layers.Layer):\n",
      "    def __init__ (self, filters, strides=1, activation =\"relu\", **kwargs):\n",
      "        super().__init__ (**kwargs)\n",
      "        self.activation  = keras.activations .get(activation )\n",
      "        self.main_layers  = [\n",
      "            DefaultConv2D (filters, strides=strides),\n",
      "            keras.layers.BatchNormalization (),\n",
      "            self.activation ,\n",
      "            DefaultConv2D (filters),\n",
      "            keras.layers.BatchNormalization ()]\n",
      "        self.skip_layers  = []\n",
      "        if strides > 1:\n",
      "            self.skip_layers  = [\n",
      "                DefaultConv2D (filters, kernel_size =1, strides=strides),\n",
      "                keras.layers.BatchNormalization ()]\n",
      "    def call(self, inputs):\n",
      "        Z = inputs\n",
      "        for layer in self.main_layers :\n",
      "            Z = layer(Z)\n",
      "        skip_Z = inputs\n",
      "        for layer in self.skip_layers :\n",
      "464 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "            skip_Z = layer(skip_Z)\n",
      "        return self.activation (Z + skip_Z)\n",
      "As you can see, this code matches Figure 14-18  pretty closely. In the constructor, we\n",
      "create all the layers we will need: the main layers are the ones on the right side of the\n",
      "diagram, and the skip layers are the ones on the left (only needed if the stride is\n",
      "greater than 1). Then in the call()  method, we simply make the inputs go through\n",
      "the main layers, and the skip layers (if any), then we add both outputs and we apply\n",
      "the activation function.\n",
      "Next, we can build the ResNet-34 simply using a Sequential  model, since it is really\n",
      "just a long sequence of layers (we can treat each residual unit as a single layer now\n",
      "that we have the ResidualUnit  class):\n",
      "model = keras.models.Sequential ()\n",
      "model.add(DefaultConv2D (64, kernel_size =7, strides=2,\n",
      "                        input_shape =[224, 224, 3]))\n",
      "model.add(keras.layers.BatchNormalization ())\n",
      "model.add(keras.layers.Activation (\"relu\"))\n",
      "model.add(keras.layers.MaxPool2D (pool_size =3, strides=2, padding=\"SAME\"))\n",
      "prev_filters  = 64\n",
      "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
      "    strides = 1 if filters == prev_filters  else 2\n",
      "    model.add(ResidualUnit (filters, strides=strides))\n",
      "    prev_filters  = filters\n",
      "model.add(keras.layers.GlobalAvgPool2D ())\n",
      "model.add(keras.layers.Flatten())\n",
      "model.add(keras.layers.Dense(10, activation =\"softmax\" ))\n",
      "The only slightly tricky part in this code is the loop that adds the ResidualUnit  layers\n",
      "to the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\n",
      "have 128 filters, and so on. We then set the strides to 1 when the number of filters is\n",
      "the same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit ,\n",
      "and finally we update prev_filters .\n",
      "It is quite amazing that in less than 40 lines of code, we can build the model that won\n",
      "the ILSVRC 2015 challenge! It demonstrates both the elegance of the ResNet model,\n",
      "and the expressiveness of the Keras API. Implementing the other CNN architectures\n",
      "is not much harder. However, Keras comes with several of these architectures built in,\n",
      "so why not use them instead?\n",
      "Using Pretrained Models From Keras\n",
      "In general, you won’t have to implement standard models like GoogLeNet or ResNet\n",
      "manually, since pretrained networks are readily available with a single line of code, in\n",
      "the keras.applications  package. For example:\n",
      "model = keras.applications .resnet50 .ResNet50 (weights=\"imagenet\" )\n",
      "Using Pretrained Models From Keras | 465\n",
      "21In the ImageNet dataset, each image is associated to a word in the WordNet dataset : the class ID is just a\n",
      "WordNet ID.\n",
      "That’s all! This will create a ResNet-50 model and download weights pretrained on\n",
      "the ImageNet dataset. To use it, you first need to ensure that the images have the right\n",
      "size. A ResNet-50 model expects 224 × 224 images (other models may expect other\n",
      "sizes, such as 299 × 299), so let’s use TensorFlow’s tf.image.resize()  function to\n",
      "resize the images we loaded earlier:\n",
      "images_resized  = tf.image.resize(images, [224, 224])\n",
      "The tf.image.resize()  will not preserve the aspect ratio. If this is\n",
      "a problem, you can try cropping the images to the appropriate\n",
      "aspect ratio before resizing. Both operations can be done in one\n",
      "shot with tf.image.crop_and_resize() .\n",
      "The pretrained models assume that the images are preprocessed in a specific way. In\n",
      "some cases they may expect the inputs to be scaled from 0 to 1, or -1 to 1, and so on.\n",
      "Each model provides a preprocess_input()  function that you can use to preprocess\n",
      "your images. These functions assume that the pixel values range from 0 to 255, so we\n",
      "must multiply them by 255 (since earlier we scaled them to the 0–1 range):\n",
      "inputs = keras.applications .resnet50 .preprocess_input (images_resized  * 255)\n",
      "Now we can use the pretrained model to make predictions:\n",
      "Y_proba = model.predict(inputs)\n",
      "As usual, the output Y_proba  is a matrix with one row per image and one column per\n",
      "class (in this case, there are 1,000 classes). If you want to display the top K predic‐\n",
      "tions, including the class name and the estimated probability of each predicted class,\n",
      "you can use the decode_predictions()  function. For each image, it returns an array\n",
      "containing the top K predictions, where each prediction is represented as an array\n",
      "containing the class identifier21, its name and the corresponding confidence score:\n",
      "top_K = keras.applications .resnet50 .decode_predictions (Y_proba, top=3)\n",
      "for image_index  in range(len(images)):\n",
      "    print(\"Image #{}\" .format(image_index ))\n",
      "    for class_id , name, y_proba in top_K[image_index ]:\n",
      "        print(\"  {} - {:12s} {:.2f}%\" .format(class_id , name, y_proba * 100))\n",
      "    print()\n",
      "The output looks like this:\n",
      "Image #0\n",
      "  n03877845 - palace       42.87%\n",
      "  n02825657 - bell_cote    40.57%\n",
      "  n03781244 - monastery    14.56%\n",
      "466 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "Image #1\n",
      "  n04522168 - vase         46.83%\n",
      "  n07930864 - cup          7.78%\n",
      "  n11939491 - daisy        4.87%\n",
      "The correct classes (monastery and daisy) appear in the top 3 results for both images.\n",
      "That’s pretty good considering that the model had to choose among 1,000 classes.\n",
      "As you can see, it is very easy to create a pretty good image classifier using a pre‐\n",
      "trained model. Other vision models are available in keras.applications , including\n",
      "several ResNet variants, GoogLeNet variants like InceptionV3 and Xception,\n",
      "VGGNet variants, MobileNet and MobileNetV2 (lightweight models for use in\n",
      "mobile applications), and more.\n",
      "But what if you want to use an image classifier for classes of images that are not part\n",
      "of ImageNet? In that case, you may still benefit from the pretrained models to per‐\n",
      "form transfer learning.\n",
      "Pretrained Models for Transfer Learning\n",
      "If you want to build an image classifier, but you do not have enough training data,\n",
      "then it is often a good idea to reuse the lower layers of a pretrained model, as we dis‐\n",
      "cussed in Chapter 11 . For example, let’s train a model to classify pictures of flowers,\n",
      "reusing a pretrained Xception model. First, let’s load the dataset using TensorFlow\n",
      "Datasets (see Chapter 13 ):\n",
      "import tensorflow_datasets  as tfds\n",
      "dataset, info = tfds.load(\"tf_flowers\" , as_supervised =True, with_info =True)\n",
      "dataset_size  = info.splits[\"train\"].num_examples  # 3670\n",
      "class_names  = info.features [\"label\"].names # [\"dandelion\", \"daisy\", ...]\n",
      "n_classes  = info.features [\"label\"].num_classes  # 5\n",
      "Note that you can get information about the dataset by setting with_info=True . Here,\n",
      "we get the dataset size and the names of the classes. Unfortunately, there is only a\n",
      "\"train\"  dataset, no test set or validation set, so we need to split the training set. The\n",
      "TF Datasets project provides an API for this. For example, let’s take the first 10% of\n",
      "the dataset for testing, the next 15% for validation, and the remaining 75% for train‐\n",
      "ing:\n",
      "test_split , valid_split , train_split  = tfds.Split.TRAIN.subsplit ([10, 15, 75])\n",
      "test_set  = tfds.load(\"tf_flowers\" , split=test_split , as_supervised =True)\n",
      "valid_set  = tfds.load(\"tf_flowers\" , split=valid_split , as_supervised =True)\n",
      "train_set  = tfds.load(\"tf_flowers\" , split=train_split , as_supervised =True)\n",
      "Pretrained Models for Transfer Learning | 467\n",
      "Next we must preprocess the images. The CNN expects 224 × 224 images, so we need\n",
      "to resize them. We also need to run the image through Xception’s prepro\n",
      "cess_input()  function:\n",
      "def preprocess (image, label):\n",
      "    resized_image  = tf.image.resize(image, [224, 224])\n",
      "    final_image  = keras.applications .xception .preprocess_input (resized_image )\n",
      "    return final_image , label\n",
      "Let’s apply this preprocessing function to all 3 datasets, and let’s also shuffle & repeat\n",
      "the training set, and add batching & prefetching to all datasets:\n",
      "batch_size  = 32\n",
      "train_set  = train_set .shuffle(1000).repeat()\n",
      "train_set  = train_set .map(preprocess ).batch(batch_size ).prefetch (1)\n",
      "valid_set  = valid_set .map(preprocess ).batch(batch_size ).prefetch (1)\n",
      "test_set  = test_set .map(preprocess ).batch(batch_size ).prefetch (1)\n",
      "If you want to perform some data augmentation, you can just change the preprocess‐\n",
      "ing function for the training set, adding some random transformations to the training\n",
      "images. For example, use tf.image.random_crop()  to randomly crop the images, use\n",
      "tf.image.random_flip_left_right()  to randomly flip the images horizontally, and\n",
      "so on (see the notebook for an example).\n",
      "Next let’s load an Xception model, pretrained on ImageNet. We exclude the top of the\n",
      "network (by setting include_top=False ): this excludes the global average pooling\n",
      "layer and the dense output layer. We then add our own global average pooling layer,\n",
      "based on the output of the base model, followed by a dense output layer with 1 unit\n",
      "per class, using the softmax activation function. Finally, we create the Keras Model :\n",
      "base_model  = keras.applications .xception .Xception (weights=\"imagenet\" ,\n",
      "                                                  include_top =False)\n",
      "avg = keras.layers.GlobalAveragePooling2D ()(base_model .output)\n",
      "output = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg)\n",
      "model = keras.models.Model(inputs=base_model .input, outputs=output)\n",
      "As explained in Chapter 11 , it’s usually a good idea to freeze the weights of the pre‐\n",
      "trained layers, at least at the beginning of training:\n",
      "for layer in base_model .layers:\n",
      "    layer.trainable  = False\n",
      "Since our model uses the base model’s layers directly, rather than\n",
      "the base_model  object itself, setting base_model.trainable=False\n",
      "would have no effect.\n",
      "Finally, we can compile the model and start training:\n",
      "468 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "optimizer  = keras.optimizers .SGD(lr=0.2, momentum =0.9, decay=0.01)\n",
      "model.compile(loss=\"sparse_categorical_crossentropy\" , optimizer =optimizer ,\n",
      "              metrics=[\"accuracy\" ])\n",
      "history = model.fit(train_set ,\n",
      "                    steps_per_epoch =int(0.75 * dataset_size  / batch_size ),\n",
      "                    validation_data =valid_set ,\n",
      "                    validation_steps =int(0.15 * dataset_size  / batch_size ),\n",
      "                    epochs=5)\n",
      "This will be very slow, unless you have a GPU. If you do not, then\n",
      "you should run this chapter’s notebook in Colab, using a GPU run‐\n",
      "time (it’s free!). See the instructions at https://github.com/ageron/\n",
      "handson-ml2 .\n",
      "After training the model for a few epochs, its validation accuracy should reach about\n",
      "75-80%, and stop making much progress. This means that the top layers are now\n",
      "pretty well trained, so we are ready to unfreeze all layers (or you could try unfreezing\n",
      "just the top ones), and continue training (don’t forget to compile the model when you\n",
      "freeze or unfreeze layers). This time we use a much lower learning rate to avoid dam‐\n",
      "aging the pretrained weights:\n",
      "for layer in base_model .layers:\n",
      "    layer.trainable  = True\n",
      "optimizer  = keras.optimizers .SGD(lr=0.01, momentum =0.9, decay=0.001)\n",
      "model.compile(...)\n",
      "history = model.fit(...)\n",
      "It will take a while, but this model should reach around 95% accuracy on the test set.\n",
      "With that, you can start training amazing image classifiers! But there’s more to com‐\n",
      "puter vision than just classification. For example, what if you also want to know where\n",
      "the flower is in the picture? Let’s look at this now.\n",
      "Classification  and Localization\n",
      "Localizing an object in a picture can be expressed as a regression task, as discussed in\n",
      "Chapter 10 : to predict a bounding box around the object, a common approach is to\n",
      "predict the horizontal and vertical coordinates of the object’s center, as well as its\n",
      "height and width. This means we have 4 numbers to predict. It does not require much\n",
      "change to the model, we just need to add a second dense output layer with 4 units\n",
      "(typically on top of the global average pooling layer), and it can be trained using the\n",
      "MSE loss:\n",
      "base_model  = keras.applications .xception .Xception (weights=\"imagenet\" ,\n",
      "                                                  include_top =False)\n",
      "avg = keras.layers.GlobalAveragePooling2D ()(base_model .output)\n",
      "class_output  = keras.layers.Dense(n_classes , activation =\"softmax\" )(avg)\n",
      "Classification  and Localization | 469\n",
      "22“Crowdsourcing in Computer Vision, ” A. Kovashka et al. (2016).\n",
      "loc_output  = keras.layers.Dense(4)(avg)\n",
      "model = keras.models.Model(inputs=base_model .input,\n",
      "                           outputs=[class_output , loc_output ])\n",
      "model.compile(loss=[\"sparse_categorical_crossentropy\" , \"mse\"],\n",
      "              loss_weights =[0.8, 0.2], # depends on what you care most about\n",
      "              optimizer =optimizer , metrics=[\"accuracy\" ])\n",
      "But now we have a problem: the flowers dataset does not have bounding boxes\n",
      "around the flowers. So we need to add them ourselves. This is often one of the hard‐\n",
      "est and most costly part of a Machine Learning project: getting the labels. It’s a good\n",
      "idea to spend time looking for the right tools. To annotate images with bounding\n",
      "boxes, you may want to use an open source image labeling tool like VGG Image\n",
      "Annotator, LabelImg, OpenLabeler or ImgLab, or perhaps a commercial tool like\n",
      "LabelBox or Supervisely. Y ou may also want to consider crowdsourcing platforms\n",
      "such as Amazon Mechanical Turk or CrowdFlower if you have a very large number of\n",
      "images to annotate. However, it is quite a lot of work to setup a crowdsourcing plat‐\n",
      "form, prepare the form to be sent to the workers, to supervise them and ensure the\n",
      "quality of the bounding boxes they produce is good, so make sure it is worth the\n",
      "effort: if there are just a few thousand images to label, and you don’t plan to do this\n",
      "frequently, it may be preferable to do it yourself. Adriana Kovashka et al. wrote a very\n",
      "practical paper22 about crowdsourcing in Computer Vision, I recommend you check\n",
      "it out, even if you do not plan to use crowdsourcing.\n",
      "So let’s suppose you obtained the bounding boxes for every image in the flowers data‐\n",
      "set (for now we will assume there is a single bounding box per image), you then need\n",
      "to create a dataset whose items will be batches of preprocessed images along with\n",
      "their class labels and their bounding boxes. Each item should be a tuple of the form:\n",
      "(images, (class_labels, bounding_boxes)) . Then you are ready to train your\n",
      "model!\n",
      "The bounding boxes should be normalized so that the horizontal\n",
      "and vertical coordinates, as well as the height and width all range\n",
      "from 0 to 1. Also, it is common to predict the square root of the\n",
      "height and width rather than the height and width directly: this\n",
      "way, a 10 pixel error for a large bounding box will not be penalized\n",
      "as much as a 10 pixel error for a small bounding box.\n",
      "The MSE often works fairly well as a cost function to train the model, but it is not a\n",
      "great metric to evaluate how well the model can predict bounding boxes. The most\n",
      "common metric for this is the Intersection over Union (IoU): it is the area of overlap\n",
      "between the predicted bounding box and the target bounding box, divided by the\n",
      "470 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area of their union (see Figure 14-23 ). In tf.keras, it is implemented by the\n",
      "tf.keras.metrics.MeanIoU  class.\n",
      "Figure 14-23. Intersection over Union (IoU) Metric for Bounding Boxes\n",
      "Classifying and localizing a single object is nice, but what if the images contain multi‐\n",
      "ple objects (as is often the case in the flowers dataset)?\n",
      "Object Detection\n",
      "The task of classifying and localizing multiple objects in an image is called object\n",
      "detection . Until a few years ago, a common approach was to take a CNN that was\n",
      "trained to classify and locate a single object, then slide it across the image, as shown\n",
      "in Figure 14-24 . In this example, the image was chopped into a 6 × 8 grid, and we\n",
      "show a CNN (the thick black rectangle) sliding across all 3 × 3 regions. When the\n",
      "CNN was looking at the top left of the image, it detected part of the left-most rose,\n",
      "and then it detected that same rose again when it was first shifted one step to the\n",
      "right. At the next step, it started detecting part of the top-most rose, and then it detec‐\n",
      "ted it again once it was shifted one more step to the right. Y ou would then continue to\n",
      "slide the CNN through the whole image, looking at all 3 × 3 regions. Moreover, since\n",
      "objects can have varying sizes, you would also slide the CNN across regions of differ‐\n",
      "ent sizes. For example, once you are done with the 3 × 3 regions, you might want to\n",
      "slide the CNN across all 4 × 4 regions as well.\n",
      "Object Detection | 471\n",
      "Figure 14-24. Detecting Multiple Objects by Sliding a CNN Across the Image\n",
      "This technique is fairly straightforward, but as you can see it will detect the same\n",
      "object multiple times, at slightly different positions. Some post-processing will then\n",
      "be needed to get rid of all the unnecessary bounding boxes. A common approach for\n",
      "this is called non-max suppression :\n",
      "•First, you need to add an extra objectness  output to your CNN, to estimate the\n",
      "probability that a flower is indeed present in the image (alternatively, you could\n",
      "add a “no-flower” class, but this usually does not work as well). It must use the\n",
      "sigmoid activation function and you can train it using the \"binary_crossen\n",
      "tropy\"  loss. Then just get rid of all the bounding boxes for which the objectness\n",
      "score is below some threshold: this will drop all the bounding boxes that don’t\n",
      "actually contain a flower.\n",
      "•Second, find the bounding box with the highest objectness score, and get rid of\n",
      "all the other bounding boxes that overlap a lot with it (e.g., with an IoU greater\n",
      "than 60%). For example, in Figure 14-24 , the bounding box with the max object‐\n",
      "ness score is the thick bounding box over the top-most rose (the objectness score\n",
      "is represented by the thickness of the bounding boxes). The other bounding box\n",
      "over that same rose overlaps a lot with the max bounding box, so we will get rid\n",
      "of it.\n",
      "472 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "23“Fully Convolutional Networks for Semantic Segmentation, ” J. Long, E. Shelhamer, T. Darrell (2015).\n",
      "24There is one small exception: a convolutional layer using V ALID padding will complain if the input size is\n",
      "smaller than the kernel size.\n",
      "•Third, repeat step two until there are no more bounding boxes to get rid of.\n",
      "This simple approach to object detection works pretty well, but it requires running\n",
      "the CNN many times, so it is quite slow. Fortunately, there is a much faster way to\n",
      "slide a CNN across an image: using a Fully Convolutional Network .\n",
      "Fully Convolutional Networks (FCNs)\n",
      "The idea of FCNs was first introduced in a 2015 paper23 by Jonathan Long et al., for\n",
      "semantic segmentation (the task of classifying every pixel in an image according to\n",
      "the class of the object it belongs to). They pointed out that you could replace the\n",
      "dense layers at the top of a CNN by convolutional layers. To understand this, let’s look\n",
      "at an example: suppose a dense layer with 200 neurons sits on top of a convolutional\n",
      "layer that outputs 100 feature maps, each of size 7 × 7 (this is the feature map size, not\n",
      "the kernel size). Each neuron will compute a weighted sum of all 100 × 7 × 7 activa‐\n",
      "tions from the convolutional layer (plus a bias term). Now let’s see what happens if we\n",
      "replace the dense layer with a convolution layer using 200 filters, each 7 × 7, and with\n",
      "V ALID padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel\n",
      "is exactly the size of the input feature maps and we are using V ALID padding). In\n",
      "other words, it will output 200 numbers, just like the dense layer did, and if you look\n",
      "closely at the computations performed by a convolutional layer, you will notice that\n",
      "these numbers will be precisely the same as the dense layer produced. The only differ‐\n",
      "ence is that the dense layer’s output was a tensor of shape [batch size, 200] while the\n",
      "convolutional layer will output a tensor of shape [batch size, 1, 1, 200].\n",
      "To convert a dense layer to a convolutional layer, the number of fil‐\n",
      "ters in the convolutional layer must be equal to the number of units\n",
      "in the dense layer, the filter size must be equal to the size of the\n",
      "input feature maps, and you must use V ALID padding. The stride\n",
      "may be set to 1 or more, as we will see shortly.\n",
      "Why is this important? Well, while a dense layer expects a specific input size (since it\n",
      "has one weight per input feature), a convolutional layer will happily process images of\n",
      "any size24 (however, it does expect its inputs to have a specific number of channels,\n",
      "since each kernel contains a different set of weights for each input channel). Since an\n",
      "FCN contains only convolutional layers (and pooling layers, which have the same\n",
      "property), it can be trained and executed on images of any size!\n",
      "Object Detection | 473\n",
      "25This assumes we used only SAME padding in the network: indeed, V ALID padding would reduce the size of\n",
      "the feature maps. Moreover, 448 can be neatly divided by 2 several times until we reach 7, without any round‐\n",
      "ing error. If any layer uses a different stride than 1 or 2, then there may be some rounding error, so again the\n",
      "feature maps may end up being smaller.For example, suppose we already trained a CNN for flower classification and localiza‐\n",
      "tion. It was trained on 224 × 224 images and it outputs 10 numbers: outputs 0 to 4 are\n",
      "sent through the softmax activation function, and this gives the class probabilities\n",
      "(one per class); output 5 is sent through the logistic activation function, and this gives\n",
      "the objectness score; outputs 6 to 9 do not use any activation function, and they rep‐\n",
      "resent the bounding box’s center coordinates, and its height and width. We can now\n",
      "convert its dense layers to convolutional layers. In fact, we don’t even need to retrain\n",
      "it, we can just copy the weights from the dense layers to the convolutional layers!\n",
      "Alternatively, we could have converted the CNN into an FCN before training.\n",
      "Now suppose the last convolutional layer before the output layer (also called the bot‐\n",
      "tleneck layer) outputs 7 × 7 feature maps when the network is fed a 224 × 224 image\n",
      "(see the left side of Figure 14-25 ). If we feed the FCN a 448 × 448 image (see the right\n",
      "side of Figure 14-25 ), the bottleneck layer will now output 14 × 14 feature maps.25\n",
      "Since the dense output layer was replaced by a convolutional layer using 10 filters of\n",
      "size 7 × 7, V ALID padding and stride 1, the output will be composed of 10 features\n",
      "maps, each of size 8 × 8 (since 14 - 7 + 1 = 8). In other words, the FCN will process\n",
      "the whole image only once and it will output an 8 × 8 grid where each cell contains 10\n",
      "numbers (5 class probabilities, 1 objectness score and 4 bounding box coordinates).\n",
      "It’s exactly like taking the original CNN and sliding it across the image using 8 steps\n",
      "per row and 8 steps per column: to visualize this, imagine chopping the original\n",
      "image into a 14 × 14 grid, then sliding a 7 × 7 window across this grid: there will be 8\n",
      "× 8 = 64 possible locations for the window, hence 8 × 8 predictions. However, the\n",
      "FCN approach is much  more efficient, since the network only looks at the image\n",
      "once. In fact, You Only Look Once  (YOLO) is the name of a very popular object detec‐\n",
      "tion architecture!\n",
      "474 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "26“Y ou Only Look Once: Unified, Real-Time Object Detection, ” J. Redmon, S. Divvala, R. Girshick, A. Farhadi\n",
      "(2015).\n",
      "27“YOLO9000: Better, Faster, Stronger, ” J. Redmon, A. Farhadi (2016).\n",
      "28“YOLOv3: An Incremental Improvement, ” J. Redmon, A. Farhadi (2018).\n",
      "Figure 14-25. A Fully Convolutional Network Processing a Small Image (left)  and a\n",
      "Large One (right)\n",
      "You Only Look Once (YOLO)\n",
      "YOLO is an extremely fast and accurate object detection architecture proposed by\n",
      "Joseph Redmon et al. in a 2015 paper26, and subsequently improved in 201627\n",
      "(YOLOv2) and in 201828 (YOLOv3). It is so fast that it can run in realtime on a video\n",
      "(check out this nice demo ).\n",
      "YOLOv3’s architecture is quite similar to the one we just discussed, but with a few\n",
      "important differences:\n",
      "Object Detection | 475\n",
      "•First, it outputs 5 bounding boxes for each grid cell (instead of just 1), and each\n",
      "bounding box comes with an objectness score. It also outputs 20 class probabili‐\n",
      "ties per grid cell, as it was trained on the PASCAL VOC dataset, which contains\n",
      "20 classes. That’s a total of 45 numbers per grid cell (5 * 4 bounding box coordi‐\n",
      "nates, plus 5 objectness scores, plus 20 class probabilities).\n",
      "•Second, instead of predicting the absolute coordinates of the bounding box cen‐\n",
      "ters, YOLOv3 predicts an offset relative to the coordinates of the grid cell, where\n",
      "(0, 0) means the top left of that cell, and (1, 1) means the bottom right. For each\n",
      "grid cell, YOLOv3 is trained to predict only bounding boxes whose center lies in\n",
      "that cell (but the bounding box itself generally extends well beyond the grid cell).\n",
      "YOLOv3 applies the logistic activation function to the bounding box coordinates\n",
      "to ensure they remain in the 0 to 1 range.\n",
      "•Third, before training the neural net, YOLOv3 finds 5 representative bounding\n",
      "box dimensions, called anchor boxes  (or bounding box priors ): it does this by\n",
      "applying the K-Means algorithm (see ???) to the height and width of the training\n",
      "set bounding boxes. For example, if the training images contain many pedes‐\n",
      "trians, then one of the anchor boxes will likely have the dimensions of a typical\n",
      "pedestrian. Then when the neural net predicts 5 bounding boxes per grid cell, it\n",
      "actually predicts how much to rescale each of the anchor boxes. For example,\n",
      "suppose one anchor box is 100 pixels tall and 50 pixels wide, and the network\n",
      "predicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling of 0.9 (for\n",
      "one of the grid cells), this will result in a predicted bounding box of size 150 × 45\n",
      "pixels. To be more precise, for each grid cell and each anchor box, the network\n",
      "predicts the log of the vertical and horizontal rescaling factors. Having these pri‐\n",
      "ors makes the network more likely to predict bounding boxes of the appropriate\n",
      "dimensions, and it also speeds up training since it will more quickly learn what\n",
      "reasonable bounding boxes look like.\n",
      "•Fourth, the network is trained using images of different scales: every few batches\n",
      "during training, the network randomly chooses a new image dimension (from\n",
      "330 × 330 to 608 × 608 pixels). This allows the network to learn to detect objects\n",
      "at different scales. Moreover, it makes it possible to use YOLOv3 at different\n",
      "scales: the smaller scale will be less accurate but faster than the larger scale, so\n",
      "you can choose the right tradeoff for your use case.\n",
      "There are a few more innovations you might be interested in, such as the use of skip\n",
      "connections to recover some of the spatial resolution that is lost in the CNN (we will\n",
      "discuss this shortly when we look at semantic segmentation). Moreover, in the 2016\n",
      "paper, the authors introduce the YOLO9000 model that uses hierarchical classifica‐\n",
      "tion: the model predicts a probability for each node in a visual hierarchy called Word‐\n",
      "Tree. This makes it possible for the network to predict with high confidence that an\n",
      "image represents, say, a dog, even though it is unsure what specific type of dog it is.\n",
      "476 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "So I encourage you to go ahead and read all three papers: they are quite pleasant to\n",
      "read, and it is an excellent example of how Deep Learning systems can be incremen‐\n",
      "tally improved.\n",
      "Mean Average Precision (mAP)\n",
      "A very common metric used in object detection tasks is the mean Average Precision\n",
      "(mAP). “Mean Average” sounds a bit redundant, doesn’t it? To understand this met‐\n",
      "ric, let’s go back to two classification metrics we discussed in Chapter 3 : precision and\n",
      "recall. Remember the tradeoff: the higher the recall, the lower the precision. Y ou can\n",
      "visualize this in a Precision/Recall curve (see Figure 3-5 ). To summarize this curve\n",
      "into a single number, we could compute its Area Under the Curve (AUC). But note\n",
      "that the Precision/Recall curve may contain a few sections where precision actually\n",
      "goes up when recall increases, especially at low recall values (you can see this at the\n",
      "top left of Figure 3-5 ). This is one of the motivations for the mAP metric.\n",
      "Suppose the classifier has a 90% precision at 10% recall, but a 96% precision at 20%\n",
      "recall: there’s really no tradeoff here: it simply makes more sense to use the classifier\n",
      "at 20% recall rather than at 10% recall, as you will get both higher recall and higher\n",
      "precision. So instead of looking at the precision at 10% recall, we should really be\n",
      "looking at the maximum  precision that the classifier can offer with at least  10% recall.\n",
      "It would be 96%, not 90%. So one way to get a fair idea of the model’s performance is\n",
      "to compute the maximum precision you can get with at least 0% recall, then 10%\n",
      "recall, 20%, and so on up to 100%, and then calculate the mean of these maximum\n",
      "precisions. This is called the Average Precision  (AP) metric. Now when there are more\n",
      "than 2 classes, we can compute the AP for each class, and then compute the mean AP\n",
      "(mAP). That’s it!\n",
      "However, in an object detection systems, there is an additional level of complexity:\n",
      "what if the system detected the correct class, but at the wrong location (i.e., the\n",
      "bounding box is completely off)? Surely we should not count this as a positive predic‐\n",
      "tion. So one approach is to define an IOU threshold: for example, we may consider\n",
      "that a prediction is correct only if the IOU is greater than, say, 0.5, and the predicted\n",
      "class is correct. The corresponding mAP is generally noted mAP@0.5 (or mAP@50%,\n",
      "or sometimes just AP50). In some competitions (such as the Pascal VOC challenge),\n",
      "this is what is done. In others (such as the COCO competition), the mAP is computed\n",
      "for different IOU thresholds (0.50, 0.55, 0.60, …, 0.95), and the final metric is the\n",
      "mean of all these mAPs (noted AP@[.50:.95] or AP@[.50:0.05:.95]). Y es, that’s a mean\n",
      "mean average.\n",
      "Several YOLO implementations built using TensorFlow are available on github, some\n",
      "with pretrained weights. At the time of writing, they are based on TensorFlow 1, but\n",
      "by the time you read this, TF 2 implementations will certainly be available. Moreover,\n",
      "other object detection models are available in the TensorFlow Models project, many\n",
      "Object Detection | 477\n",
      "29“SSD: Single Shot MultiBox Detector, ” Wei Liu et al. (2015).\n",
      "30“Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks, ” Shaoqing Ren et al.\n",
      "(2015).with pretrained weights, and some have even been ported to TF Hub, making them\n",
      "extremely easy to use, such as SSD29 and Faster-RCNN .30, which are both quite popu‐\n",
      "lar. SSD is also a “single shot” detection model, quite similar to YOLO, while Faster R-\n",
      "CNN is more complex: the image first goes through a CNN, and the output is passed\n",
      "to a Region Proposal Network (RPN) which proposes bounding boxes that are most\n",
      "likely to contain an object, and a classifier is run for each bounding box, based on the\n",
      "cropped output of the CNN.\n",
      "The choice of detection system depends on many factors: speed, accuracy, available\n",
      "pretrained models, training time, complexity, etc. The papers contain tables of met‐\n",
      "rics, but there is quite a lot of variability in the testing environments, and the technol‐\n",
      "ogies evolve so fast that it is difficulty to make a fair comparison that will be useful for\n",
      "most people and remain valid for more than a few months.\n",
      "Great! So we can locate objects by drawing bounding boxes around them. But per‐\n",
      "haps you might want to be a bit more precise. Let’s see how to go down to the pixel\n",
      "level.\n",
      "Semantic Segmentation\n",
      "In semantic segmentation , each pixel is classified according to the class of the object it\n",
      "belongs to (e.g., road, car, pedestrian, building, etc.), as shown in Figure 14-26 . Note\n",
      "that different objects of the same class are not distinguished. For example, all the bicy‐\n",
      "cles on the right side of the segmented image end up as one big lump of pixels. The\n",
      "main difficulty in this task is that when images go through a regular CNN, they grad‐\n",
      "ually lose their spatial resolution (due to the layers with strides greater than 1): so a\n",
      "regular CNN may end up knowing that there’s a person in the image, somewhere in\n",
      "the bottom left of the image, but it will not be much more precise than that.\n",
      "478 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "31This type of layer is sometimes referred to as a deconvolution layer , but it does not perform what mathemati‐\n",
      "cians call a deconvolution, so this name should be avoided.\n",
      "Figure 14-26. Semantic segmentation\n",
      "Just like for object detection, there are many different approaches to tackle this prob‐\n",
      "lem, some quite complex. However, a fairly simple solution was proposed in the 2015\n",
      "paper by Jonathan Long et al. we discussed earlier. They start by taking a pretrained\n",
      "CNN and turning into an FCN, as discussed earlier. The CNN applies a stride of 32 to\n",
      "the input image overall (i.e., if you add up all the strides greater than 1), meaning the\n",
      "last layer outputs feature maps that are 32 times smaller than the input image. This is\n",
      "clearly too coarse, so they add a single upsampling layer  that multiplies the resolution\n",
      "by 32. There are several solutions available for upsampling (increasing the size of an\n",
      "image), such as bilinear interpolation, but it only works reasonably well up to ×4 or\n",
      "×8. Instead, they used a transposed convolutional layer :31 it is equivalent to first\n",
      "stretching the image by inserting empty rows and columns (full of zeros), then per‐\n",
      "forming a regular convolution (see Figure 14-27 ). Alternatively, some people prefer to\n",
      "think of it as a regular convolutional layer that uses fractional strides (e.g., 1/2 in\n",
      "Figure 14-27 ). The transposed convolutional layer  can be initialized to perform some‐\n",
      "thing close to linear interpolation, but since it is a trainable layer, it will learn to do\n",
      "better during training.\n",
      "Semantic Segmentation | 479\n",
      "Figure 14-27. Upsampling Using a Transpose Convolutional Layer\n",
      "In a transposed convolution layer, the stride defines how much the\n",
      "input will be stretched, not the size of the filter steps, so the larger\n",
      "the stride, the larger the output (unlike for convolutional layers or\n",
      "pooling layers).\n",
      "TensorFlow Convolution Operations\n",
      "TensorFlow also offers a few other kinds of convolutional layers:\n",
      "•keras.layers.Conv1D  creates a convolutional layer for 1D inputs, such as time\n",
      "series or text (sequences of letters or words), as we will see in ???.\n",
      "•keras.layers.Conv3D  creates a convolutional layer for 3D inputs, such as 3D\n",
      "PET scan.\n",
      "•Setting the dilation_rate  hyperparameter of any convolutional layer to a value\n",
      "of 2 or more creates an à-trous convolutional layer  (“à trous” is French for “with\n",
      "holes”). This is equivalent to using a regular convolutional layer with a filter dila‐\n",
      "ted by inserting rows and columns of zeros (i.e., holes). For example, a 1 × 3 filter\n",
      "equal to [[1,2,3]]  may be dilated with a dilation rate  of 4, resulting in a dilated\n",
      "filter  [[1, 0, 0, 0, 2, 0, 0, 0, 3]] . This allows the convolutional layer to\n",
      "480 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "have a larger receptive field at no computational price and using no extra param‐\n",
      "eters.\n",
      "•tf.nn.depthwise_conv2d()  can be used to create a depthwise convolutional layer\n",
      "(but you need to create the variables yourself). It applies every filter to every\n",
      "individual input channel independently. Thus, if there are fn filters and fn′ input\n",
      "channels, then this will output fn × fn′ feature maps.\n",
      "This solution is okay, but still too imprecise. To do better, the authors added skip con‐\n",
      "nections from lower layers: for example, they upsampled the output image by a factor\n",
      "of 2 (instead of 32), and they added the output of a lower layer that had this double\n",
      "resolution. Then they upsampled the result by a factor of 16, leading to a total upsam‐\n",
      "pling factor of 32 (see Figure 14-28 ). This recovered some of the spatial resolution\n",
      "that was lost in earlier pooling layers. In their best architecture, they used a second\n",
      "similar skip connection to recover even finer details from an even lower layer: in\n",
      "short, the output of the original CNN goes through the following extra steps: upscale\n",
      "×2, add the output of a lower layer (of the appropriate scale), upscale ×2, add the out‐\n",
      "put of an even lower layer, and finally upscale ×8. It is even possible to scale up\n",
      "beyond the size of the original image: this can be used to increase the resolution of an\n",
      "image, which is a technique called super-resolution .\n",
      "Figure 14-28. Skip layers recover some spatial resolution from lower layers\n",
      "Once again, many github repositories provide TensorFlow implementations of\n",
      "semantic segmentation (TensorFlow 1 for now), and you will even find a pretrained\n",
      "instance segmentation  model in the TensorFlow Models project. Instance segmenta‐\n",
      "tion is similar to semantic segmentation, but instead of merging all objects of the\n",
      "same class into one big lump, each object is distinguished from the others (e.g., it\n",
      "identifies each individual bicycle). At the present, they provide multiple implementa‐\n",
      "tions of the Mask R-CNN  architecture, which was proposed in a 2017 paper : it\n",
      "extends the Faster R-CNN model by additionally producing a pixel-mask for each\n",
      "bounding box. So not only do you get a bounding box around each object, with a set\n",
      "of estimated class probabilities, you also get a pixel mask that locates pixels in the\n",
      "bounding box that belong to the object.\n",
      "Semantic Segmentation | 481\n",
      "32“Matrix Capsules with EM Routing, ” G. Hinton, S. Sabour, N. Frosst (2018).As you can see, the field of Deep Computer Vision is vast and moving fast, with all\n",
      "sorts of architectures popping out every year, all based on Convolutional Neural Net‐\n",
      "works. The progress made in just a few years has been astounding, and researchers\n",
      "are now focusing on harder and harder problems, such as adversarial learning  (which\n",
      "attempts to make the network more resistant to images designed to fool it), explaina‐\n",
      "bility (understanding why the network makes a specific classification), realistic image\n",
      "generation  (which we will come back to in ???), single-shot learning  (a system that can\n",
      "recognize an object after it has seen it just once), and much more. Some even explore\n",
      "completely novel architectures, such as Geoffrey Hinton’s capsule networks32 (I pre‐\n",
      "sented them in a couple videos , with the corresponding code in a notebook). Now on\n",
      "to the next chapter, where we will look at how to process sequential data such as time\n",
      "series using Recurrent Neural Networks and Convolutional Neural Networks.\n",
      "Exercises\n",
      "1.What are the advantages of a CNN over a fully connected DNN for image classi‐\n",
      "fication?\n",
      "2.Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,\n",
      "a stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the\n",
      "middle one outputs 200, and the top one outputs 400. The input images are RGB\n",
      "images of 200 × 300 pixels. What is the total number of parameters in the CNN?\n",
      "If we are using 32-bit floats, at least how much RAM will this network require\n",
      "when making a prediction for a single instance? What about when training on a\n",
      "mini-batch of 50 images?\n",
      "3.If your GPU runs out of memory while training a CNN, what are five things you\n",
      "could try to solve the problem?\n",
      "4.Why would you want to add a max pooling layer rather than a convolutional\n",
      "layer with the same stride?\n",
      "5.When would you want to add a local response normalization  layer?\n",
      "6.Can you name the main innovations in AlexNet, compared to LeNet-5? What\n",
      "about the main innovations in GoogLeNet, ResNet, SENet and Xception?\n",
      "7.What is a Fully Convolutional Network? How can you convert a dense layer into\n",
      "a convolutional layer?\n",
      "8.What is the main technical difficulty of semantic segmentation?\n",
      "9.Build your own CNN from scratch and try to achieve the highest possible accu‐\n",
      "racy on MNIST.\n",
      "482 | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n",
      "10.Use transfer learning for large image classification.\n",
      "a.Create a training set containing at least 100 images per class. For example, you\n",
      "could classify your own pictures based on the location (beach, mountain, city,\n",
      "etc.), or alternatively you can just use an existing dataset (e.g., from Tensor‐\n",
      "Flow Datasets).\n",
      "b.Split it into a training set, a validation set and a test set.\n",
      "c.Build the input pipeline, including the appropriate preprocessing operations,\n",
      "and optionally add data augmentation.\n",
      "d.Fine-tune a pretrained model on this dataset.\n",
      "11.Go through TensorFlow’s DeepDream tutorial . It is a fun way to familiarize your‐\n",
      "self with various ways of visualizing the patterns learned by a CNN, and to gener‐\n",
      "ate art using Deep Learning.\n",
      "Solutions to these exercises are available in ???.\n",
      "Exercises | 483\n",
      "About the Author\n",
      "Aurélien Géron  is a Machine Learning consultant. A former Googler, he led the Y ou‐\n",
      "Tube video classification team from 2013 to 2016. He was also a founder and CTO of\n",
      "Wifirst from 2002 to 2012, a leading Wireless ISP in France; and a founder and CTO\n",
      "of Polyconseil in 2001, the firm that now manages the electric car sharing service\n",
      "Autolib’ .\n",
      "Before this he worked as an engineer in a variety of domains: finance (JP Morgan and\n",
      "Société Générale), defense (Canada’s DOD), and healthcare (blood transfusion). He\n",
      "published a few technical books (on C++, WiFi, and internet architectures), and was\n",
      "a Computer Science lecturer in a French engineering school.\n",
      "A few fun facts: he taught his three children to count in binary with their fingers (up\n",
      "to 1023), he studied microbiology and evolutionary genetics before going into soft‐\n",
      "ware engineering, and his parachute didn’t open on the second jump.\n",
      "Colophon\n",
      "The animal on the cover of Hands-On Machine Learning with Scikit-Learn and Ten‐\n",
      "sorFlow  is the fire salamander ( Salamandra salamandra ), an amphibian found across\n",
      "most of Europe. Its black, glossy skin features large yellow spots on the head and\n",
      "back, signaling the presence of alkaloid toxins. This is a possible source of this\n",
      "amphibian’s common name: contact with these toxins (which they can also spray\n",
      "short distances) causes convulsions and hyperventilation. Either the painful poisons\n",
      "or the moistness of the salamander’s skin (or both) led to a misguided belief that these\n",
      "creatures not only could survive being placed in fire but could extinguish it as well.\n",
      "Fire salamanders live in shaded forests, hiding in moist crevices and under logs near\n",
      "the pools or other freshwater bodies that facilitate their breeding. Though they spend\n",
      "most of their life on land, they give birth to their young in water. They subsist mostly\n",
      "on a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up to a foot\n",
      "in length, and in captivity, may live as long as 50 years.\n",
      "The fire salamander’s numbers have been reduced by destruction of their forest habi‐\n",
      "tat and capture for the pet trade, but the greatest threat is the susceptibility of their\n",
      "moisture-permeable skin to pollutants and microbes. Since 2014, they have become\n",
      "extinct in parts of the Netherlands and Belgium due to an introduced fungus.\n",
      "Many of the animals on O’Reilly covers are endangered; all of them are important to\n",
      "the world. To learn more about how you can help, go to animals.oreilly.com .\n",
      "The cover image is from Wood’s Illustrated Natural History . The cover fonts are URW\n",
      "Typewriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font\n",
      "is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.\n"
     ]
    }
   ],
   "source": [
    "#Example Code:\n",
    "from PyPDF2 import PdfReader\n",
    "pdf_reader = PdfReader(\"C:\\\\Users\\\\rajde\\\\ineuron_ai\\\\2-Aurélien-Géron-Hands-On-Machine-Learning-with-Scikit-Learn-Keras-and-Tensorflow_-Concepts-Tools-and-Techniques-to-Build-Intelligent-Systems-O’Reilly-Media-2019.pdf\")\n",
    "if pdf_reader.is_encrypted: # to check whether the pdf is encrypted or not\n",
    "    pdf_reader.decrypt(\"swordfish\")\n",
    "for page in pdf_reader.pages:\n",
    "    print(page.extract_text()) # to print the text data of a page from pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3f74fd",
   "metadata": {},
   "source": [
    "# 5. What methods do you use to rotate a page?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans: PyPDF2 Package provides 2 methods to rotate a page:\n",
    "\n",
    "1. rotateClockWise() -> For Clockwise rotation\n",
    "2. rotateCounterClockWise() -> For Counter Clockwise rotation\n",
    "The PyPDF2 package only allows you to rotate a page in increments of 90 degrees. You will receive an AssertionError otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f12da27",
   "metadata": {},
   "source": [
    "# 6. What is the difference between a Run object and a Paragraph object?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24808d8",
   "metadata": {},
   "source": [
    "Ans: The structure of a document is represented by three different data types in python-Docx. At the highest level, a Document object represents the entire document. The Document object contains a list of Paragraph objects for the paragraphs in the document. (A new paragraph begins whenever the user presses ENTER or RETURN while typing in a Word document.) Each of these Paragraph objects contains a list of one or more Run objects.\n",
    "\n",
    "The text in a Word document is more than just a string. It has font, size, color, and other styling information associated with it. A style in Word is a collection of these attributes. A Run object is a contiguous run of text with the same style. A new Run object is needed whenever the text style changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c62c6",
   "metadata": {},
   "source": [
    "# 7. How do you obtain a list of Paragraph objects for a Document object that’s stored in a variable named doc?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Program\n",
    "from docx import Document\n",
    "doc = Document(\"sample_file.docx\") # Path of the Docx file\n",
    "print(doc.paragraphs) # Prints the list of Paragraph objects for a Document\n",
    "for paragraph in doc.paragraphs:\n",
    "    print(paragraph.text) # Prints the text in the paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d1572",
   "metadata": {},
   "source": [
    "# 8. What type of object has bold, underline, italic, strike, and outline variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e07ee1",
   "metadata": {},
   "source": [
    "Ans: Run object has bold, underline, italic, strike, and outline variables. The text in a Word document is more than just a string. It has font, size, color, and other styling information associated with it.\n",
    "\n",
    "A style in Word is a collection of these attributes. A Run object is a contiguous run of text with the same style. A new Run object is needed whenever the text style changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37334d6e",
   "metadata": {},
   "source": [
    "# 9. What is the difference between False, True, and None for the bold variable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a779e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "bold = True  # Style Set to Bold\n",
    "bold = False # Style Not Set to Bold\n",
    "bold = None # Style is Not Applicable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88966f",
   "metadata": {},
   "source": [
    "# 10. How do you create a Document object for a new Word document?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029a7e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Program\n",
    "from docx import Document\n",
    "document = Document()\n",
    "document.add_paragraph(\"iNeuron Full Stack DataScience Course\")\n",
    "document.save('mydocument.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a65312",
   "metadata": {},
   "source": [
    "# 11. How do you add a paragraph with the text 'Hello, there!' to a Document object stored in a variable named doc?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7eee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Program\n",
    "from docx import Document\n",
    "doc = Document()\n",
    "doc.add_paragraph('Hello, there!')\n",
    "doc.save('hello.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de2a763",
   "metadata": {},
   "source": [
    "# 12. What integers represent the levels of headings available in Word documents?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd75dc",
   "metadata": {},
   "source": [
    "Ans: The levels for a heading in a word document can be specified by using the level attribute inside the add_heading method. There are a total of 5 levels statring for 0 t0 4. where level 0 makes a headline with the horizontal line below the text, whereas the heading level 1 is the main heading. Similarly, the other headings are sub-heading with their's font-sizes in decreasing order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5eeeab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0477e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790e2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56f1607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5962dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de128c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774de643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ccf2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08fe91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2668ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb331d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974c088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae725c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1492b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297fe645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9859462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d91ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a63fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c2dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c238022e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdddc565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a39389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f22cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
